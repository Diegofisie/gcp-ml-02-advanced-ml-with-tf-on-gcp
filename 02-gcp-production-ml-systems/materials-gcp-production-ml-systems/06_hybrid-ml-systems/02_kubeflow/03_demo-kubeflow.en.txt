Hi, I'm Amy, and I'm part of Google Cloud Platform developer relations. I have a background in AI and I focus on machine learning. I'm excited to demo KubeFlow. What you see here is the machine learning workflow that we're going to run shortly. We're going to train, analyze, and serve a TensorFlow model that predicts whether a taxi ride will result in a tip. Our input data comes from a BigQuery public dataset of Chicago taxi rides. We're first doing some pre-processing of the raw data and we're using this workflow to do a feature engineering experiment. We'll asynchronously pre-process and then train against two different feature sets and we'll see which model produces the best results. We'll be using the TensorFlow Transform library to do feature processing. The output of TensorFlow Transform is exported as a TensorFlow graph to use for training and serving. So, in addition to supporting feature generation, this ensures that the data that we use for prediction is processed in the same way as the training data was. So, this prevents training serving skew. So, to train our models, we'll use KubeFlow support for running distributed TensorFlow training jobs. For demo purposes, we'll actually use distributed training for just one of the training jobs and single-note training for the second to show how easy it is to configure a KubeFlow to scale out training when you need to. We'll then deploy the trained models, so, that they're available for serving, that is prediction. We'll deploy the models to TensorFlow serving using KubeFlow and also to Google Cloud ML engines online prediction service. In this demo, we're focusing on TensorFlow but KubeFlow also supports other machine learning frameworks and deployment platforms. We'll also analyze the train TensorFlow models and compare them to each other. To do this, we'll use the TensorFlow model analysis library. It lets us easily visualize model metrics and compare models. We can look at the results using KubeFlow's Jupyter hub notebook installation. Okay, let's move to the demo. So, the first thing we're going to do is deploy KubeFlow to our Kubernetes cluster. So, this is super straightforward. What I've done is I've changed to the scripts directory of the KubeFlow repo and I'm back to set some environment variables. I'm telling it what KubeFlow version to use. I'm giving a name to the case on app directory that I'm about to build, and I'm telling it to not actually deploy because, in fact, I've already got it deployed to the demo cluster that I'm going to use. Now, I just run the deploy script, and this uses the case on a tool to write a bunch of configuration information into a directory that we're going to use for the deploy. If I had set the flag to deploy, the last step of the script would have been simply to deploy it to the cluster, and that is it. It's very straight forward. So, now, we'll run our machine learning workflow as we take a look at the pods belong to that workflow as they come online. So, before I launch the workflow, we can take a look at what's already deployed. Many of these Kubernetes pods are associated with KubeFlow. A few of them are not. So, TF hub runs the Jupyter hub installation for KubeFlow. The TF job pods are related to the KubeFlow support for doing distributed TensorFlow training jobs. There are a number of TensorFlow serving, endpoints running, and there's some other stuff that I won't go into detail on. There are a few things that are not KubeFlow related, and, in fact, I'm going to use a framework called Argo, which is a Kubernetes native workflow framework to actually run the workflow and you can see if a few pods related to Argo here as well. All right. So, let's launch our pipeline. That's just for ease of demoing. I've set it up with a bunch of reasonable defaults that I'm going to use for this demo. One thing I have it set it up to do yet is specify at the size of the TensorFlow distributed training cluster that we're going to use. I'll tell it to use two workers and one perimeter server. As you might remember, only one of our two training jobs is going to be a distributed training job. The other one is hard wire- to be a single no training job, just so you can see how easy it is to switch between the two of them. All right. So, we'll deploy our machine learning workflow here. We see that it started to run. This is the start of the workflow graph, and just to be clear, what we're talking about workflow graph here, not TensorFlow model graph as you may have heard about in other modules. This is the workflow graph. Okay. So, now we can take a look in our list of pods. There are a few containers that are started up. These are doing the pre-processing using TensorFlow Transform. Now, I'm going to pause for just a couple of minutes until they complete and the TensorFlow training starts up. Okay. Now we have finished our pre-processing and both training jobs are running, and I particularly wanted to show you this because it highlights the use of KubeFlow's, TF job resource to easily do distributed training. Here's our first training job. It has one master, one parameter server, and two workers. Then here's our single node training job, and it's only got a master. It's not using a distributed training cluster. You can see that their worker is running. If sometimes they error out and then they're able to recover and restart themselves because they're checkpointing all their work. So, that is not of concern to us. Okay. So, I'm going to take another pause until the entire workflow finishes running, and then let's look at the results in our deployed models. Okay. Our machine learning workflow has completed, and you might notice that we have a few new pods running. These are the backends for new TensorFlow serving endpoints, and we'll talk about those in just a second. But first, let's take a look at the results of the model analysis that we did. So, if we look at our workflow graph here again, this is the workflow we just ran, you'll remember that once our two models were trained, we're doing a bit of feature engineering experimentation. They're using two different feature sets. Now that they're trained, let's compare them and see which one worked better. That's these analyze steps right here. We're using TensorFlow model analysis to do that. This is the library with a lot of nice visualization support and we're going to run it in a Jupyter Notebook. So, KubeFlow includes a Jupyter hub install, and when you start it up, you can make user accounts as part of that installation. I have already done that. This is my user account, and the KubeFlow install uses a persistent disk for all of these notebooks, so if the pod goes down or even if the cluster goes down, you won't lose your work. So, I've created a notebook, and again, this is running as part of the KubeFlow install. I've created a notebook to poke around a little with some of the modal analysis that we did as part of this workflow. I don't have time to do it full justice but I'll show a few highlights. So, the first thing we're going to do is do some imports, and you can see we're using TensorFlow 1.6 here. So, now, we're going to look at the model results that were derived using a set of specs that told TFMA, TensorFlow model analysis to, do slice so-called slicing across various dimensions. So, we told it we're interested in some slices that have to do with comparative accuracy across the hour of the day that a taxi trip starts. Remember this is taxi trip data, the day of a week that a taxi trip starts and ends, and some other interesting things. So, we've generated all this ahead of time, it's part of the workflow, and now we can just load it in really quickly, easily. I'm actually using- to save a little typing in my notebook, and I'm using the results of the same workflow run a little bit earlier and pulling in the data for that workflow that was stored in Google Cloud Storage and now we can render it. So, let's slice across trip start hour to start with. Remember, we've already done the hard work to do the analysis as part of our workflow. Now, we just need to look at the results and visualize them. So, there's lot of nice information here. We can see for the different hours of the day when we have the most data. Unsurprisingly, you can see that most taxi trips seemed to happen at noon. If you think about that, that's probably makes sense. We can look at the common sets of machine learning metrics for each of those hours, things like accuracy, area under the curve, average loss, etc., and see which hour were doing the- our models are doing the best job of modeling on. We do the same for the second result. I'll skip that. It shows the same columns in much the same results across trip hours. Now, I'm going to do something interesting. I'm going to show a cross value spec. This is- if we look at the definition of this here, we're crossing by trip start day, day of the week, where trip start hour is noon. So, now, we can see the same information calculated for each day of the week for the noon hour. So, this is really cool stuff. Again, I don't have time to do it justice, but there's much more of that that you can show. So now, we're interested in seeing which of our two feature experiments seem to give the best results. So, we can look at overall metrics for each model. It turns out in this case, the second one is slightly more accurate. Of course, percentage points can count when you're serving a machine learning model. They're fairly close to each other, but the second one turned out to be a little more accurate. We can also do runs where we compare them directly with each other like I'm going to do here for a given slice. The slice here is feature value spec which is that if we go back and look at our definitions is the noon hour. So, I'm analyzing. Here, the respective model accuracy comparing the two models for predictions made for the noon hour. Here, the scale is actually only showing a few percentage point differences but, again, model two ended up working slightly better. There's much more we can do with TensorFlow model analysis. Let's say we've convinced ourselves that that model two is the best one. So now, we can use it for serving, for making predictions, and we can do this in two ways because remember our workflow deployed the models to both Cloud ML Engine online prediction and to TensorFlow Serving. So, first, let's take a look at making a prediction with this model using Cloud ML Engine. The result, the probabilities here are whether or not there's going to be a tip and, in fact, for this data instance, it predicts that there is going to be a tip that's the second value. In fact, in this case, it was correct that there was indeed a tip. So, say we're happy with this model. We've decided it's the best across all our future experiments. We want to make it the default. We can do that as well. The model name is taxi fare and we're setting the second model to be the taxi fare default. So, let's look at one more thing now. We just showed Cloud ML Engine online prediction. Let's look at doing much the same thing with TensorFlow Serving instead. So, to do this, we'll go back to our Kubernetes cluster. So, let's take a look at the services running on our cluster. The way I have this set up for demo purposes, there's actually- each TensorFlow Serving endpoint has its own external IP. There's other ways you could set it up as well, including fronting all of your TensorFlow Serving endpoints with something like Google Cloud Platform's Identity Aware Proxy. But here, just to make it easy to demo, they've each got their own IP, and you can see the ones that we just deployed, these newest ones, one for each model that we learned. So, behind the service, there are deployments, and we can see that currently, each Kubernetes deployment is running only one pod. If we wanted to, if we are happy with our model and we wanted to serve it out and put it in an app, we could scale up the underlying deployments to let our TensorFlow Serving service scale-up with the larger amount of traffic that we expected to get. So, what if we want to use one of these new models for prediction? Let's look at how that would look. For that, I need the model name, which turns out to be the same as the service name, and the external IP. I'm going to use a little client script to talk to the server and get the information back. So, I'm just editing this little client script with the information about the new model name and IP address; sending off the request. Got back a lot of stuff. That's just the way this client library is written. But you can see amongst the output we got, we got the probabilities, the predictions. It's much the same as we saw with Cloud ML Engine online prediction, which you'd expect because it's the same training model that were serving in two different ways, and again, we predicted that we would get a tip out of this taxi ride, which indeed turns out to be correct. That concludes the demo. Let's recap some of the benefits that you saw in the demo. First up is portability. We can run our workflows on any Kubernetes cluster, whether on-premises or in the Cloud. The case sonnet utility makes it easy to set up different environment configs, for example, running locally versus in the Cloud, Dev cluster versus production cluster. KubeFlow also makes it easy to build hybrid machine-learning workflows, where, say, training is run On-prem but then model serving is done using a Cloud service like Cloud ML Engine. Next is composability and reproducibility. We can reuse our building blocks across multiple workflows. We can run the same machine learning workflow multiple times or run a set of controlled experiments by varying the workflow parameters. Then we have scalability. We can do single node training during initial development than. By just changing a config setting, do distributed training on larger datasets. You saw on the demo how easy it was to do either one. Once our models are trained, we can decide whether to use KubeFlow-based TensorFlow serving or Cloud ML Engine online prediction or both by changing a config setting. We can use Kubernetes to scale out the TensorFlow Serving resources. Lastly, visualization and collaboration. KubeFlows, Jupyter hub installation lets us visualize workflow results and share notebooks. All of the common libraries and extensions we need come pre-installed with KubeFlow. KubeFlow component definitions and workflow building blocks are also easy to share. KubeFlow as a platform has a lot of momentum with a growing list of contributors and participating companies. Even though we have been focused in this specialization on Google Cloud Machine Learning Engine, we've recognized that there are situations where a training or deploying on the Cloud is not an option. In such situations, we recommend that you minimize your infrastructure cognitive load as much as possible. You can use KubeFlow to abstract way the machine-learning infrastructure, plus you get the benefits of portability, composability, and scalability.