In order to build hybrid machine learning systems that work well both on-premises and in the cloud, your machine learning framework has to support three things, composability, portability, and scalability. So, let's take composability first. When people think about machine learning, they think about building a model, training a model, TensorFlow, PyTorch, NumPy, et cetera. But the reality is, 95 percent of the time is spent not building a model, it's all the other stuff. Each machine learning stage, data analysis, training, model validation, monitoring, these are all independent systems. Everyone has a different way to handle all these boxes. So when we say composability, it's about the ability to compose a bunch of microservices together and the option to use what makes sense for your problem. But now that you've built your specific framework, you want to move it around and that's where we get into portability. The stack that you use is likely made up of all these components and probably lots more. All those microservices I detailed earlier only touch a small number of them. But you do it, you can figure every stage in the stack and it's finally running. What's this good for? What happens next? Think about the machine learning workflow. Remember that you did all of this just so that you could develop the model, we'll call that experimentation. But once you have the code running, what do you need to do? That's right, you need to train the model on the full dataset. You probably can't do it on the small setup on which you did all your initial development. So, you start up a training cluster and you have to do it all over again. All the configuration, all the libraries, all the testing, you've got to repeat it for the new environment. Then chances are you've got to do it once again to move it from on-premises to the cloud. Because remember, we said we want a hybrid environment and machine learning model that maybe it helps you train on the cloud and predicted the edge are trained on the cloud that predict on-premises. The point is that you have to configure the stack over and over again for each environment that you need to support. Maybe at this point you're thinking, "That doesn't matter to me. I never have to change environments. I'll only use one environment." Wrong. Joe Beda is the CTO of Heptio, a startup that's focused on bringing Kubernetes to everyone. Before that, he was at Google, he co-founded Kubernetes, started Google Compute Engine. So, vast experience building production systems and that vast experience building production systems shows in this code which is not even about machine learning. So, Joe says, "The way I think about it, every difference between development staging and production will eventually result in outage." But notice a first environment that he mentions, it's dev, it's development. Your developing environment is an environment. So portability, it's essential and then of course you've got to do it again when your inputs change, or your boss calls you and tells you to train faster by training on more machines. You inevitably find that you have to change environments over and over again. Also, your laptop, it counts as environment number one and you don't do production services in your laptop, so you need portability. So composability, portability, finally, scalability. You always hear about Kubernetes being able to scale and that's true but scalability in machine learning means so many more things, accelerators, GPUs, TPUs, et cetera. Disks, skillsets, software engineers, researchers, data engineers, data analysts, data scientists, different skillsets. Teams across the org because there are teams that are going to be building the experiments, teams that are going to be using the experiments, teams that are going to be monitoring the machine learning models. So, accelerators, disks, skillsets, teams, experiments. So, that's what we think of when we think of machine learning in a hybrid cloud environment, composability, portability, scalability.