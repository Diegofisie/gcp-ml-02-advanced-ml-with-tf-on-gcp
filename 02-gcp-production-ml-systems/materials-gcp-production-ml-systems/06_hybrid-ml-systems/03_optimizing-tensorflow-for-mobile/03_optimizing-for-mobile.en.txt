Let's take a look at the Inception v3 model. The model takes about 91 megabytes in storage, with about 25 million parameters. That would fit into a server or a desktop machine, but it's a bit too large for mobile. So, there are multiple techniques that you can use to optimize a graph like this. So, you can freeze the graph, you can use a graph transform tool, you can do quantization, you can do memory mapping et cetera. So, let's talk about each of them. Freezing a graph is a low time optimization. So, what this does, is that it goes through the graph, looks at all the variables, and converts those variable nodes into constant nodes. The reason you want to do this is that in TensorFlow, variable nodes are stored in different files, whereas constant nodes are embedded in the graph itself in the same file. So, by converting the variable nodes into constant nodes, you get a slight performance win on mobile and it's easier to handle too. So, you can use a Python script to do this. So, why don't you do this all the time? If you do this, you cannot do continuous training, you cannot do federated learning, because they're no longer variables to train, just constants. So, you lose a little bit of flexibility here. So, you don't want to always convert variables to constants, but, this gives you a little bit of a performance improvement. Another thing that you can do is to use a graph transform tool. The graph transform tool is part of the TensorFlow distribution, and the tool supports various optimization task like stripping nodes that are not used during inference, but that were used during the learning phase. What kind of nodes? Nodes like gradient computation, batch norm, et cetera. These nodes can be removed during inference. The tool supports a removal of such training on the operations. Obviously, debug nodes can also be removed. What fold_batch_norms does, is that it converts convolution 2D or matrix multiplication operations, that are followed by column-wise multiplications into an equivalent app, where the multiplication, the column-wise multiplication, is baked into the convolution weights. So that, instead of two apps, you have only one app. This saves some computation during inference. Finally, if you want, the weights themselves can be quantized to make the model more compressible. But if you quantize the weights, you're reducing the accuracy. So, you're trading off accuracy for model size. The question is, how much accuracy are you trading off? There is no immediate answer to that, you have to measure it because this varies from model to model. When modern neural networks were first developed, accuracy and speed were the prime concerns, and as a result neural networks focused on 32-bit floating point arithmetic. But nowadays, researchers are deploying a lot of different models especially in commercial applications. When we consider the number of cycles needed, then the number of cycles that you need for inference, actually grows in proportion to the number of users, because you do an inference for each user. So, you can see why the focus of neural networks, now has shifted from the efficiency of training to the efficiencies of inference. So, to combat these inefficiencies, inefficiencies during inference, you basically have different techniques for storing numbers and performing calculations. So, these techniques together are often called quantization, and what quantization does, is that it takes a floating point value and compresses it to an eight bit integer. It reduces the size of the files, it reduces computational resources that you need to handle the data, and that's what you see on the slide. The graph on the left, it shows a typical Relu, a rectified linear unit operation, with the internal conversion from float to eight-bit values. The min and max values are from the input flow tensor. Once the Relu operation is performed, the values are dequantized and the output becomes floats. The second graph, the middle graph here, shows the next stage in quantization, removing the unnecessary convergence to and from the float. This stage identifies any patterns in the conversions that are performed in stage number one, and removes those redundancies. The final stage, shows a graph where all the tensor calculations are done in eight bits, and there are no conversions that are needed to floating point. With these optimizations, with all these three stages, the optimized graph of Inception v3 now becomes just 23 megs, which is about 75 percent smaller. So, while you can do the freezing quantization et cetera yourself, we recommend that you check out TensorFlow Lite which is a different TensorFlow runtime. But what TensorFlow Lite lets you do, is that it allows you to run TensorFlow models right on the device, leverages the Android Neural Network API, and it's optimized for mobile apps.