TensorFlow supports multiple mobile platforms including Android, iOS and Raspberry Pi. So, here we're going to focus on mobile devices. Mobile TensorFlow makes sense when there's a poor or missing network connection or we're sending continuous data to a server would be too expensive. The purpose is to help developers make the lean mobile apps using TensorFlow, both by continuing to reduce a code footprint and by supporting quantization and lower precision arithmetic that make the models smaller. You can build a TensorFlow shared object on Android using Android Studio. Using a continuous integration tool called basil. For iOS, there is Cocoapod integration as well and it's all relatively simple. So, let's take a look at how you can use the TensorFlow API. The Android Inference Library integrates with TensorFlow for job applications. So, these libraries are very thin wrapper from Java to the native implementation So, that the performance impact is not very high. So, at first, you create TensorFlow inference interface opening the model file from the asset in the APK, and then you set up an input feed using the Feed API and on mobile, the input data tends to be retrieved from various sensors like the camera etcetera.Then you run the inference and you fetch the result using the fetch method. So, all of these are blocking calls so you typically run them in a worker thread instead of the main thread because an API call takes time. Even though we've talked primarily about prediction on the mobile a new frontier is called federated learning, the idea is you continuously trained the model on the device and then you combine the model updates from a federation of user devices to update the overall model, the goal is for each user to get their customized experience because there's bottle training happening on the device but still retain privacy because it's the overall model update that goes back to the cloud.