Let's look at a second scenario, where hybrid models are necessary. Remember this example from earlier in our first course? We saw that applications are increasingly combining machine learning with mobile applications. Google Translate, for example, is a combination of several models: one model to find the sign, another model to read the sign and do OCR on it, a third model to translate the sign, a fourth model to superimpose translated text. The idea is that ML allows you to add intelligence to your mobile applications such as image and voice recognition, translation and natural language processing. Also, you can apply the technology for smarter analytics on mobile specific data. For example, you can use machine learning for detecting certain patterns from motion or GPS tracking data. Why is that? This is because machine learning can extract the meaning from the raw data. For example, if you want to do image recognition with your mobile app, the easiest way to send the raw image to the Cloud and let the Cloud service recognize the objects in the image. But, if you have a neural network algorithm running on your mobile app, you can get the labels of the objects and then send them to the Cloud. It's tens or hundreds of times more efficient to collect the object labels on the Cloud service. So, as an example, if you do motion detection with your mobile app and you run a neural network to extract a feature vector from that sensor data, the bunch of numbers in the feature vector represents a signature of each motion. So, you don't have to send the raw video sequence or raw motion data to the Cloud service. You can do the inference on the mobile and send just the results to the Cloud service, and that involves less network transfer, it's a lot more efficient. Also, by applying machine learning technology to mobile apps, you can reduce network bandwidth and you can get faster response when you're communicating with the Cloud services. We can delegate to a Microservice, like we were running everything in the Cloud. So now, we want a library, a library that we can invoke directly on the mobile device and not a Web service that we have to call out over the network. So, in situations like this, we would like to train on the Cloud, but do the predictions on the device. So, we don't want to call out to a Microservice, instead we want to embed the model within the device itself. So, how do we do that?