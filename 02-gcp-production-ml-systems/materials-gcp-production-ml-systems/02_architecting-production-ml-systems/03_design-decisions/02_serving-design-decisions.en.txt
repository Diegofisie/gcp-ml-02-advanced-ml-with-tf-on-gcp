Just as the use case determines the appropriate training architecture, it also determines the appropriate serving architecture. In designing our serving architecture, one of our goals is to minimize average latency. Just like an operating systems where we don't want to be bottlenecked by slow disk IO, when serving models, we don't want to be bottleneck by slow to decide models. Remarkably, the solution for serving models is very similar to what we do to optimize IO performance, we use a cache. In this case, rather than faster memory, we simply use a table. Static serving then computes the label ahead of time and serves by looking it up in the table. Dynamic serving in contrast, computes the label on-demand. In short, there's a space-time trade-off. Static serving is space-intensive because we store our pre-computed predictions, with a low fixed latency and lower maintenance costs. While dynamic is compute-intensive with lower storage costs, higher maintenance, and variable latency. The choice of which to use, static or dynamic serving, is determined by considering how important latency, storage, and CPU costs are. Sometimes it can be hard to express the relative importance of these three areas. Which is why I find it helpful to consider static and dynamic serving through another lens, peakedness and cardinality. Peakedness refers to the extent to which the distribution of the prediction workload is concentrated. You can also think of it as inverse entropy. For example, a model that predicts the next word given the current word, which you might find in your mobile phone keyboard app, would be highly peaked. Because a small number of words account for the majority of words used. In contrast, a model that predicted quarterly revenue for all our sales verticals, in order to populate a report, will be run on the same verticals every time and with the same frequency for each. So, it would have very low peakedness. Cardinality refers to the number of values in a set. In this case, the set is the set of all possible things we might have to make predictions for. A model predicting sales revenue given organization division number, would be fairly low cardinality. A model predicting lifetime value given a user for an e-commerce platform would be very high cardinality, because the number of users and the number of characteristics of each user, are likely to be quite large. Taken together, these two criteria create a space. When the cardinality is sufficiently low, we can store the entire expected prediction workload. For example, the predicted sales revenue for all divisions in a table and use static serving. When the cardinality is high because the size of the input space is large, and the workload is not very peaked, you probably want to use dynamic training. In practice though, you often choose a hybrid of static and dynamic, where you statically cache some of the predictions, or responding on demand for the long tail. This works best when the distribution is sufficiently peaked. The area above the curve and not inside the green rectangle, is ripe for a hybrid solution, with the most frequently requested predictions cached and the tail computed on-demand. Let's take a moment to fill out this table. The first use case will consider a spam detection. You should ask yourself what latency is needed. It depends on the system architecture, but you would probably run your model after every email is received and thus, insulate your users from the latency of the model. Then, you could ask yourself, how peaked is the distribution? In this case, not at all. Most emails are likely to be different, although, they may be very similar if generated programmatically. Then you can ask about cardinality. Depending on the choice of representation, the cardinality might be enormous. In this case, dynamic serving is best. What about Android voice to text? This is again subtle. Inference is almost certainly online, since there's such a long tail of possible voice clips. But maybe with sufficient signal processing, some key phrases like, "Okay, Google may have pre-computered answers." So in this case, the best use case is dynamic or hybrid. What about add conversion rate? The set of all ads, doesn't change much from day to day. Assuming all users are comfortable waiting a little bit of time after uploading their ads to get their predicted conversion rate, you could do this statically and run a batch script at regular intervals throughout the day. In practice though, you'll often use a hybrid. You might not have realized it, but dynamic serving is what we've learned so far. Think back to the architecture of the systems we've used to make predictions. A model that lived in Cloud ML Engine, was sent one or more instances and returned predictions for each. If you wanted to build a static serving system, you'd need to make three design changes. Firstly, you need to change your called Cloud ML Engine from an online prediction job to a batch prediction job. Secondly, you need to make sure that your model accepted and passed through keys as input. These keys are what will allow you to join your requests to predictions at serving time. Thirdly, you'd need to write the predictions to a data warehouse, like BigQuery, and create an API to read from it. The details for each of these, is beyond the scope of this module. However, I'll provide links to these in the course resources.