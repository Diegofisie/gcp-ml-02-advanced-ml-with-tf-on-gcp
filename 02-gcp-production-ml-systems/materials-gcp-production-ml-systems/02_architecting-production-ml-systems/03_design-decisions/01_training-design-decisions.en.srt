1
00:00:00,540 --> 00:00:03,840
One of the key decisions you'll need
to make about your production ML system

2
00:00:03,840 --> 00:00:04,670
concerns training.

3
00:00:05,930 --> 00:00:07,120
Here's a riddle.

4
00:00:07,120 --> 00:00:09,280
How is physics unlike fashion?

5
00:00:10,530 --> 00:00:14,507
If we assume that science is about
discovering relationships that already

6
00:00:14,507 --> 00:00:15,580
exist in the world,

7
00:00:15,580 --> 00:00:19,254
then the answer is that physics is
constant whereas fashion isn't.

8
00:00:19,254 --> 00:00:21,830
To see some proof, just look at
some old pictures of yourself.

9
00:00:22,910 --> 00:00:25,620
Now, you might be asking,
why is this relevant?

10
00:00:25,620 --> 00:00:28,850
The reason it's relevant is that when
making decisions about training,

11
00:00:28,850 --> 00:00:32,660
you have to decide whether the phenomenon
you're modelling is more like physics or

12
00:00:32,660 --> 00:00:33,270
like fashion.

13
00:00:34,930 --> 00:00:38,290
When training your model, there
are two paradigms, static training and

14
00:00:38,290 --> 00:00:39,090
dynamic training.

15
00:00:40,390 --> 00:00:43,690
In static training, we do what we
did in the last specialization.

16
00:00:43,690 --> 00:00:48,280
We gather our data, we partition it,
we train our model, and then we deploy it.

17
00:00:48,280 --> 00:00:52,320
In dynamic training,
we do this repeatedly as more data arrive.

18
00:00:52,320 --> 00:00:56,380
This leads to the fundamental
trade-off between static and dynamic.

19
00:00:56,380 --> 00:00:59,857
Static is simpler to build and
test, but likely to become stale.

20
00:00:59,857 --> 00:01:04,360
Whereas dynamic is harder to build and
test, but will adapt to changes.

21
00:01:04,360 --> 00:01:08,470
And the tendency to become or not become
stale is what I was alluding to earlier

22
00:01:08,470 --> 00:01:10,130
when I contrasted physics and fashion.

23
00:01:11,130 --> 00:01:15,330
If the relationship you're trying to model
is one that is constant, like physics,

24
00:01:15,330 --> 00:01:18,550
then a statically trained
model may be sufficient.

25
00:01:18,550 --> 00:01:21,900
If on the other hand the relationship
your trying to model is like fashion,

26
00:01:21,900 --> 00:01:24,340
then the dynamically trained
model might be more appropriate.

27
00:01:25,530 --> 00:01:28,313
Part of the reason the dynamic
is harder to build and

28
00:01:28,313 --> 00:01:31,365
test is our that new data may
have all sorts of bugs in it.

29
00:01:31,365 --> 00:01:35,075
And that's something we'll talk about more
deeply in a later course on designing

30
00:01:35,075 --> 00:01:36,840
adoptable ML systems.

31
00:01:36,840 --> 00:01:40,510
Can you think of other reasons why
the engineering might be harder?

32
00:01:40,510 --> 00:01:44,120
The reason is that we need more
monitoring, model rollback, and

33
00:01:44,120 --> 00:01:46,290
data quarantine capabilities.

34
00:01:46,290 --> 00:01:49,570
Let's test our understanding
with some new use cases.

35
00:01:49,570 --> 00:01:51,290
Take a moment to read
the problem column and

36
00:01:51,290 --> 00:01:53,600
think about which sort of training
style would be appropriate.

37
00:01:54,730 --> 00:01:56,980
First use case concerns spam detection.

38
00:01:56,980 --> 00:01:58,820
And the question you
should ask yourself is,

39
00:01:58,820 --> 00:02:00,820
how fresh does spam detection need to be?

40
00:02:02,220 --> 00:02:05,953
You could do this as static, but spammers
are a crafty and determined bunch.

41
00:02:05,953 --> 00:02:09,593
They will likely figure out ways of
passing whatever filter you impose within

42
00:02:09,593 --> 00:02:10,710
short order.

43
00:02:10,710 --> 00:02:13,230
So dynamic is likely to be
more effective over time.

44
00:02:14,810 --> 00:02:17,560
What about Android Voice-to-Text?

45
00:02:17,560 --> 00:02:20,000
Note that this question has some subtlety.

46
00:02:20,000 --> 00:02:23,410
For a global model,
training offline is probably fine.

47
00:02:23,410 --> 00:02:26,170
But if you want to personalize
the voice recognition,

48
00:02:26,170 --> 00:02:30,260
you may need to do something online,
or at least different, on the phone.

49
00:02:30,260 --> 00:02:33,790
So this could be static or dynamic
depending on whether you want global or

50
00:02:33,790 --> 00:02:34,970
personalized transcription.

51
00:02:36,510 --> 00:02:37,870
What about ad conversion rate?

52
00:02:39,360 --> 00:02:43,340
The interesting subtlety here is that
conversion may come in very late.

53
00:02:43,340 --> 00:02:47,030
For example, if I'm shopping for
a car online, I'm unlikely to buy for

54
00:02:47,030 --> 00:02:48,740
a very long time.

55
00:02:48,740 --> 00:02:51,320
What does dynamic training
look like in this setting?

56
00:02:52,680 --> 00:02:56,870
One system I know uses dynamic training,
but then regularly goes back at different

57
00:02:56,870 --> 00:03:00,130
intervals to catch up on new conversion
data that has arrived for the past.

58
00:03:01,410 --> 00:03:04,180
In practice, most of the time,
you'll need to do dynamic, but

59
00:03:04,180 --> 00:03:05,970
you might start with static
because it's simpler.

60
00:03:07,890 --> 00:03:10,410
This is a reference architecture for
static training.

61
00:03:10,410 --> 00:03:13,600
And it's the same thing we've done so far,
where the models are trained once and

62
00:03:13,600 --> 00:03:15,440
then pushed to ML engine.

63
00:03:15,440 --> 00:03:19,300
Now, for dynamic training, there are three
potential architectures to explore,

64
00:03:19,300 --> 00:03:22,870
Cloud functions, App Engine,
or Cloud Dataflow.

65
00:03:22,870 --> 00:03:26,270
Here's the general architecture for
dynamic training using Cloud functions.

66
00:03:26,270 --> 00:03:30,010
A new data file appears in Cloud storage
and then the Cloud function is launched.

67
00:03:31,070 --> 00:03:34,520
The Cloud function starts
Cloud MLE training job, and

68
00:03:34,520 --> 00:03:36,379
then the Cloud ML engine
writes out a new model.

69
00:03:38,060 --> 00:03:40,870
Now, let's look at a specific
architecture that uses Cloud Composer.

70
00:03:42,740 --> 00:03:45,550
Here is a more sophisticated
dynamic training architecture that

71
00:03:45,550 --> 00:03:48,310
you'll see a demo of in the next
module on Data Ingestion.

72
00:03:49,910 --> 00:03:53,470
Here we have an orchestration layer with
Cloud Composer which has cloud functions

73
00:03:53,470 --> 00:03:56,670
that trigger cloud data flow
processing jobs on new data

74
00:03:56,670 --> 00:03:59,899
based on new data files being
dropped into Google Cloud Storage.

75
00:04:01,110 --> 00:04:04,520
That data is then processed and
stored and then fed into Cloud ML Engine.

76
00:04:05,660 --> 00:04:08,440
This diagram illustrates
a user making a web request,

77
00:04:08,440 --> 00:04:10,110
perhaps from a dashboard to AppEngine.

78
00:04:11,110 --> 00:04:13,900
Then a Cloud ML Engine
training job is launched, and

79
00:04:13,900 --> 00:04:18,140
the Cloud ML Engine job writes
a new model to Cloud storage.

80
00:04:18,140 --> 00:04:21,610
And then finally, the statistics of
the training job are displayed to the user

81
00:04:21,610 --> 00:04:22,620
when the job is complete.

82
00:04:24,090 --> 00:04:27,370
It's also possible that the dataflow
pipeline is invoking the model for

83
00:04:27,370 --> 00:04:28,220
predictions.

84
00:04:28,220 --> 00:04:30,430
And we'll build a pipeline in module 3,

85
00:04:30,430 --> 00:04:32,760
Designing Adaptable ML Systems,
that does just that.