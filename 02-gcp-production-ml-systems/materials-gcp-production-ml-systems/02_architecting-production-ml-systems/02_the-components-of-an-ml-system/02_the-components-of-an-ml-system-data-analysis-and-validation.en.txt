The next component we'll review concerns the quality of the data. Machine learning models are only as good as their training data. And unlike catastrophic bugs which are easy to find, small bugs in the data can be really hard to find even though they can still significantly degrade model quality over time. We'll talk more about the many ways that data can introduce bugs in designing adaptable ML systems. Because bugs can be disastrous and hard to find, we need data analysis and data validation components. The data analysis component is all about understanding the distribution of your data, which is the first step in small bug detection. It may not be apparent to you how changes in the distribution of your data could affect your model, so consider what would happen if an upstream data source encoded a categorical feature using a number, like a product number. Only one day, they changed their product numbering convention and now use a totally different mapping using some old numbers and some new numbers. How would you know that this had happened? How would you debug your ML model? The output of your ML model would tell you if there's a drop them for performance, but it won't tell you why. The raw inputs themselves would appear valid because we're still getting numbers. In order to recognize this change, you would need to look at changes in the distribution of your inputs. If you did, this is what you might find. Whereas earlier, the most commonly occurring value might have been a four. In the new distribution, four might not even occur, and the most commonly occurring value might be a 10. Depending on how you implemented your feature columns, these new values might be mapped to one component of a one-hot encoded vector or many components. If, for example, you used a categorical column with a hash bucket, the new values will be distributed according to the hash function. So, one hash bucket might now get more and different values than before. If you used the vocabulary, then the new values would map to out-of-vocabulary buckets. But what's important is that for a given tensor, its relationship to the label before and now are likely to be very different. After analyzing the data, the next step is to ask, is the data healthy or not? There are a number of questions that relate to health. Is the new distribution similar enough to the old one? There are many ways of comparing distributions. You can look at the five-number summaries to compare the center and spread of the data, you can count the number of modes comparing symmetry and skewness, you can also compute the likelihood of observing the new distribution given the original distribution. Some other questions you can ask are, are all expected features present? Are any unexpected features present? Does the feature have the expected type? Does an expected proportion of the examples contain the feature? Did the examples have the expected number of values for features? Let's practice applying these questions in context. For each scenario, I'd like you to consider which diagnostic question would have caught this issue. For our first scenario, assume that your ML model accepts the prices of goods from all over the world in US dollar in order to make predictions about their future price. In order to accept prices in US dollars for all goods all over the world, the data need to be transformed from their original currency into dollars. One day, a system outside of your ML system changes the format of its data stream and your parser silently starts returning 1.0 for the conversion rate between Japanese Yen and the USD. Because your model uses this quantity to convert Yen to dollar those items prices are now unnaturally high. Instead of 100 yen equals $1, those items show up as $100. Which question would have caught that? In this case, question one. What if the parser throws an error when this happens and so the price for such items is null? Then question two would have caught it. What if the parser throws an error for such items and your converter returns the error string? So, instead of $1.05, you get the price as currency rate not available. Then question four would've caught it. What if the ML model uses several prizes? For example, list price and discount price and all the prices exhibit the same error. Then question five would have caught it. What if the error is only on Yen and all other currency conversions are fine? Then question one would have caught it. Each of the diagnostic questions in data validation could be part of a dashboard like Data Studio, or a monitoring system like Cloud Reliability, or you could even write a script in Datalab.