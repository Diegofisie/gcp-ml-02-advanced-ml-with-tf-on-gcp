The data transformation component allows for feature wrangling. It can do things like generate feature to integer mappings. Critically, whatever mappings that are generated must be saved and reused at serving time. Failure to do this consistently results in a problem we'll be talking more about in a later module, called Training Serving Skill. We've seen examples in the previous specialization of how to do data transformation using dataflow and dataprep. We also saw how to create dataflow pipelines that are part of the model graph using TF transform, and how you can export a transform function for use at serving time. In a later module, we'll talk about how to get the most performance out of your input pipelines. The trainer is responsible for training your model. It should be able to support data parallelism and model parallelism, and scale to large numbers of workers. We'll talk about data and model parallelism in a later module. It should also automatically monitor and log everything, and support the use of experimentation. Finally, the trainer should also support hyperparameter tuning. There are two products that aligned with this component in GCP, ML Engine which provides the managed service for TensorFlow and GKE which provides a managed environment for hybrid ML models in Kubeflow. We'll discuss hybrid ML models in a later module. ML Engine is a managed execution environment for TensorFlow, that allows you to instantly scale up to hundreds of workers, and it's automatically integrated with the three other core components: The tuner, logging and serving. It also has a built-in concept of models inversions, allowing for easy AB testing and there's no lock-in, you can take your train model and run it anywhere.