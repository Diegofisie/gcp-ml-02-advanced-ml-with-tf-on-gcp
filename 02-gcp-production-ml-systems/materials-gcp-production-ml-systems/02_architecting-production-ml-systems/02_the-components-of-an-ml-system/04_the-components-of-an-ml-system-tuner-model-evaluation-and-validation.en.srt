1
00:00:00,680 --> 00:00:04,760
As we saw on the last specialization,
there are no globally optimal values for

2
00:00:04,760 --> 00:00:08,260
hyperparameters, only
problem-specific optima.

3
00:00:08,260 --> 00:00:12,120
Because we expect to do hyperparameter
tuning, our system needs to support it.

4
00:00:12,120 --> 00:00:15,850
By which I mean, operate multiple
experiments in parallel and ideally,

5
00:00:15,850 --> 00:00:19,927
using early experiments to
guide later ones automatically.

6
00:00:19,927 --> 00:00:24,070
Cloud ML Engines supports hyperparameter
tuning using a variety of algorithms, and

7
00:00:24,070 --> 00:00:27,680
you can read more about the specific
algorithms in this paper.

8
00:00:27,680 --> 00:00:31,450
The Model Evaluation and Validation
components have one responsibility.

9
00:00:31,450 --> 00:00:33,230
To ensure that the models are good,

10
00:00:33,230 --> 00:00:35,760
before moving them into
the production environment.

11
00:00:35,760 --> 00:00:39,430
The goal is to ensure that users'
experiences aren't degraded.

12
00:00:39,430 --> 00:00:42,710
There are two main things that we care
about with respect to model quality.

13
00:00:42,710 --> 00:00:45,881
How safe the model is to serve, and
the model's prediction quality.

14
00:00:47,243 --> 00:00:51,239
A safe to serve model won't crash or cause
errors in the serving system when being

15
00:00:51,239 --> 00:00:54,062
loaded or when sent on expected inputs.

16
00:00:54,062 --> 00:00:57,735
It also shouldn't use more than the
expected amount of resources, like memory.

17
00:00:59,065 --> 00:01:02,402
Model evaluation is part of the iterative
process where teams try and

18
00:01:02,402 --> 00:01:03,860
improve their models.

19
00:01:03,860 --> 00:01:06,720
However, because it's expensive
to test on live data,

20
00:01:06,720 --> 00:01:09,410
experiments are generally
run offline first.

21
00:01:09,410 --> 00:01:13,177
And it's in this setting where
model evaluation takes place.

22
00:01:13,177 --> 00:01:17,644
Model evaluation consists of a person or
a group of people assessing the model with

23
00:01:17,644 --> 00:01:22,865
respect to some business-relevant metric,
like AUC or cost-weighted error.

24
00:01:22,865 --> 00:01:26,533
If the model meets their criteria,
then it can be pushed into production for

25
00:01:26,533 --> 00:01:27,515
a live experiment.

26
00:01:28,771 --> 00:01:32,634
In contrast to the model evaluation
component, which is human facing,

27
00:01:32,634 --> 00:01:35,630
the model validation component is not.

28
00:01:35,630 --> 00:01:38,630
Instead, it evaluates the model
against fixed thresholds and

29
00:01:38,630 --> 00:01:41,489
alerts engineers when things go awry.

30
00:01:41,489 --> 00:01:45,080
One common test is to look at
performance by slice of the input.

31
00:01:45,080 --> 00:01:45,970
For example,

32
00:01:45,970 --> 00:01:50,360
business stakeholders may care strongly
about a particular geographic market.

33
00:01:50,360 --> 00:01:53,551
This is also another junction
where ML fairness comes up.

34
00:01:53,551 --> 00:01:58,561
Both of these modules are a part of TFX,
Google's internal production ML system.

35
00:01:58,561 --> 00:02:01,041
We've already open sourced
some of these TFX libraries,

36
00:02:01,041 --> 00:02:04,481
including TF Transform,
TF Model Analysis, and TF Serving.