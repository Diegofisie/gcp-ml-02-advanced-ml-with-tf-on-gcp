The next components will talk about help the system function as a whole, rather than as a bunch of cobbled together parts. If I make change to the trainer, what component or components might also need to change? The answer is potentially all of them. The trainer uses meta information like vocabularies, collected by the data transformation component on data that had been validated and adjusted. The tuner conducts experiments, each of which involves the trainer yielding models that must then be evaluated and validated. The survey component, host models that have been evaluated, validated, and trained, and then runs them in parallel. Of course, everything logs everything. Because everything needs to talk to everything else, it's imperative that these components share resources, and a configuration framework. Failure to do so, can result in large amounts of glue code to tie the system together. Glue code, is an example of an anti-pattern. Something that slows development down. It accumulates because often research and engineering are very distinct organizationally. Within research, ML models can be developed as self-contained black boxes. Engineering on the other hand, needs to tie whatever is produced into a single production environment. Glue code arises from their attempts to run code that was never intended to be run in production in production. The best remedies for this problem are establish a common architecture for both R and D, and production deployment, and embed the teams together, so that engineering can influence the design of code from its inception. Orchestration is the name for the component responsible for gluing the other components together. In a system where pieces are designed thoughtfully, the orchestration component will be simple and elegant. In GCP, orchestration can be done with Cloud composer, which is managed Apache airflow. There are airflow operators for all the GCP components that we've considered so far, including Cloud Storage, BitQuery, Dataflow, and ML Engine. So, you can orchestrate all these tasks from composer. Another option for orchestration is to use Argo on Google Kubernetes engine. Argo is a container management tool. If each of your tasks, data ingest, data transformation, or model training or running containers, then Argo is a good way to orchestrate the ML pipeline consisting of such containers. Because GCP products are designed to work together, the code to integrate them as simple and elegant. Here are the steps to compose a workflow and Cloud composer. First, define the Ops. Second, arrange them into a directed a cyclic graph, or Dag. The workflow engine uses a Dag to run the apps in the appropriate order, and to explore opportunities for parallelism. But it can't figure out the dependencies on its own, you need to specify them. Then you upload the Dag to the environment. Finally, explore the Dag run in the web UI. We won't go into detail on Cloud composer in this specialization. However, here are some sample code, but only some minor bits abstracted. Each line of code is an independent node in our Dag. The first node runs the SQL query and dumps that data into a table, the second node exports the data from the table into a CSV file in Cloud storage, the third node trains the model using the CSV file of event data as input. The training task writes new model files to the Cloud storage model directory that is then read by the App Engine endpoint. The final node deploys a new version of the app engine endpoint, so that the new model files are loaded and it migrates traffic to the new app version. After defining these nodes, we arrange them in the graph in the order that they should execute. Stay tuned for the next module where we'll demo another architecture, and where we use Cloud composer for data ingestion.