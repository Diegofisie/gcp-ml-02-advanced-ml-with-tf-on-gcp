The first component we'll talk about is data ingestion. Before we can begin working with the data, it needs to be ingested into the system, and what's important to think of at the outset is where you'll be ingesting it from. There are many use cases that require streaming data ingestion. For example, perhaps you'd like to deliver a personalized gaining experience, or do preventative maintenance for manufacturing equipment, or react to changes in patients medical device data. Sometimes, the data you want to train your model on or get predictions for is structured in which case it might live in a data warehouse. BigQuery is a structured data warehouse in GCP that we've used extensively in the first specialization. It uses sharding to achieve incredible IO parallelism. There are many ways of reading data from BigQuery. You can do it from within the model graph using the BigQuery reader up, or you can make use of Apache Beams IO module. In this example, we're loading data from BigQuery calling predict on every record and then writing the results back into BigQuery. In the last course, we created a pipeline that ref from a structured data source and created CSV files in Cloud Storage. You can think of that architecture of read, process, and write as general purpose. It works just as well for unstructured data too. When you're ready to reach for performance, instead of CSV files, you can use TFRrecord files, and we'll talk more about this in a later module designing for high-performance ML Systems. On GCP, those three types of data ingestion mapped to three different products. If you're ingesting streaming data, you do use Pub/Sub, if you're ingesting structured data directly into your null model, you might use BigQuery, and if you're transforming data from trainings that you can train on it later, you'd read it from Cloud Storage.