In this section, we'll take everything we've learned so far and practice diagnosing a production model, given the signals that we might actually get to observe. Let's pretend you've architected an ML system to predict the demand for widgets, and it looks like this diagram. I realized the diagram is a bit complicated but that's also realistic. You're streaming purchase orders arrive in pub-sub and are fulfilled asynchronously, but let's ignore fulfillment and focus on demand prediction. Dataflow processes the stream, and using windowing functions, aggregates purchase orders over time. It then uses the output of these windowing functions and passes that to an ML model that lives in Cloud ML Engine, where the data are joined against historical purchase data that lives in a data warehouse like BigQuery. The model returns back a predicted demand for a particular time. The predictions returned by the model are both written to storage and send to the purchasing system. The purchasing system determines what's in the inventory. The inventory is also logged in the data warehouse. The models retrained daily based upon actual inventories and sales and other signals. One day, one of your product managers has a great idea, let's add credit rating to each purchase order, and you're the head of machine learning engineering, what do you think? Should it be added? Why or why not. One day, you get an email from the head of the business unit saying that he's just noticed that sales are down significantly. The warehouse manager who sees it, says that inventory storage costs are also down significantly. The room is suddenly getting quite warm. What might have happened? This is a great example of a feedback loop. What might have happened was that the model started under a predicting demand, perhaps because of some corrupted historical data or an error in the pipeline. Once demand started to go down, product turnover started to creep up. If this problem had escaped notice indefinitely the model might have learned to keep zero inventory all the time. In addition to being a great reminder that humans need to stay in the loop, it's also a reminder that we're often optimizing for something other than what we ultimately care about. In this case, we're optimizing from matching predicted demand, when what we cared about was minimizing carrying costs in order to maximize profits. Here's another scenario. One of your salespeople just shared some amazing news. By leveraging their contacts at one of MegaCorps many regional divisions, they signed MegaCorp to a five-year deal, and it's the biggest contract yet. Great you think, not realizing that this could actually have implications for your model's performance. How can these be related? It all depends on how the sales orders come in, and how independent the divisions actually are. If the divisions are entirely dependent, because there's actually just one purchasing decision split up by division and these orders come in separately, your model may still treat these orders as independent. In which case, it would look much more compelling as evidence of an uptick in demand. The solution here would be to add some aggregation by company ID to your pipeline before computing other statistics. Here's another scenario. Your warehouse manager emails you and tells you that the warehouse flooded, and then they've had to scrap a large portion of the inventory. They've ordered replacements from purchasing but it will be four days before those arrive, and unfulfilled orders in the meantime wi'll have to wait. You realize that you have a particular set of skills to address this problem. So, what do you do? You stop your automatic model deployment process. The reason you do so is because data collected during this period will be contaminated because the number of customer orders will be so low. Because the products will show up as out of stock on the webpage.