We've talked about training-serving skew a number of times in this specialization and the one that preceded it, but always in a high level. In this module, we'll provide richer examples of what it is and then demonstrate a solution. Training-serving skew is a related idea to what we've discussed so far today. It refers to differences in performance that occur as a function of differences in environment. Specifically, training-serving skew refers to differences caused by one of three things, a discrepancy between how you handle data in the training and serving pipelines, a change in the data between when you training and when you serve, or a feedback loop between your model and your algorithm. Up until now, we focused on the data aspect of training-serving skew, but it's also possible to have inconsistencies that arise after the data have been introduced. Say for example, that in your development environment you have version two of a library, but in production you have version one. The libraries may be functionally equivalent, but version two is highly optimized and version one isn't. Consequently, predictions might be significantly slower or consume more memory in production than they did in development. Ultimately, it's possible that version one and version two are functionally different perhaps because of a bug. Finally, it's also possible that different code is used in production versus development, perhaps because of recognition of one of the other issues, but though the intent was to create equivalent code, results were imperfect.