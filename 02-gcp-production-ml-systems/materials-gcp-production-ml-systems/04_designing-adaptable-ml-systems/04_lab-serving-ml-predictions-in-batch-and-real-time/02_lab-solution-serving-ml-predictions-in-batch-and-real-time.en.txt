Let's go through the solution to the lab line-by-line. We'll start our code tour in AddPrediction.java, and what I hope you'll take note of is how elegant and simple the code ends up being despite the fact that we're going to have completely different instructions, depending on whether we're running a batch pipeline or a real-time pipeline. Here, we're conditioning the code based upon some command line arguments. You'll notice on line 72 depending on whether we've passed the isRealtime flag either we're going to create a new PubSubBigQuery pipeline or a new text InputOutput pipeline. But then the subsequent steps are the same regardless of which pipeline we actually do. The first thing that we do, is we do readInstances to ingest data, then we do an application of our ML model and then finally, we write the predictions to either to disc or to BigQuery. The next file we'll look at is InputOutput.java. InputOutput.java is an abstract class in which we'll define some of the functions that we intend to share across the child classes and we'll also specify some abstract functions for functions that we intend to implement within our child classes and which we'll vary between them. The functions that are implemented abstractly are for input and for output. The first of these is called readInstances and readInstances generates a PCollection of babies. PCollections as you may recall are collections of infinite size. The other abstract function is the writePredictions function and writePredictions accepts a PCollection of babies and then writes the predictions to disc. For our batch pipeline, our input will be from CSV files and so our output will also be to CSV files. For our streaming pipeline, our input will be from Pub/Sub and our output will be to BigQuery. We'll review how those are implemented after. The functions that are shared between our two child classes concern the calling of our prediction service. We have two different inference styles. The first is addPredictionInBatches, which simulates calling the prediction service in batch and the other one is online Inference which we've represented here with addPredictionOneByOne. In addPredictionInBatches, the first thing we do is we have to divide our input into a finite number of batches, which we do with the CreateBatch function and all that CreateBatch does is it takes the System HashCode, mods it with the number of batches which yields an integer between zero and NUM-BATCHES minus one. We can then use this batch key to divide the input into different buckets and perform Inference on those buckets. So that's what we do in the BatchByKey operation. We group based upon our batch key and then finally we have to call Inference. An inference is a ParDo or we're going to call a function and parallel on a PCollection. In this case, each function that we call is the Babyweight mock_ batchPredict service where we pass in the instances and we get an array of results, and then finally, we yield each BabyPredictions separately. When we're doing predictions online, the process is a little simpler because we don't actually have to batch our Input. Instead, we simply call our ParDo, which accepts a single baby and returns a BabyPred object and for each baby we call the BabyweightMLService to get the prediction. The next thing that we'll do is peak inside the BabyweightMLService file. The BabyweightMLService file is responsible for communicating with our model that's deployed in production, and to do this, it needs to have some notion of certain classes. We need for instance to find an instance which represents a baby that we're going to make an inference on. It needs to define a request which is what we're going to communicate to the server which consists of a list of instances. It needs to define a prediction, which consists of a list of predictions, and it needs to have a response, which consists of a list of predictions. To send the request, we're going have to make an HTTP request and this should look very similar to the Python code we've already worked with in class. You'll notice we specify an endpoint and this endpoint is a function of our product, our project, our model and our version. We have to establish credentials with the service and then we make an HTTP request. Note too that we have used exponential back-off here to ensure that even if the service is unresponsive initially, we will have more opportunities to try. After we make the request, we parse it, then turn into a string and then finally because at this point it's still JSON, we convert that to the response object, as response class using the JSON parser. The next file we'll look at is PubSubBigQuery.Java. PubSubBigQuery.java is one of the two children of the InputOutput class and because it's one of the two children that extends InputOutput.Java, we need to implement readInstances and writePedictions. But remember that this code is responsible for just the IO. We're going to read in this case from Pub/Sub and we're going to write to BigQuery. But the actual calling of the prediction service is something that will still be done with an InputOutput.java. The readInstance function as for the contract specified InputOutput.Java always much yield a PCollection of babies. In this case, because we're reading from pub/Sub, all we need to do is make use of the PubsubIO library and specify a particular topic that we're reading from. In this case, each message consists of a comma delimited string representing a single baby, and so to parse that, we need to split the message by new line which we do here and then iterate over those lines and on each one using the baby.fromCsv function to parse it and turn it into a baby. The writePredictions function is where we will start. We will invoke the functionality in InputOutput.java to talk to our prediction service, and we'll do that on line 50 where we call addPredictionInBatches. AddPredictionInBatches is a function that we already looked at within InputOutput.java. Once we have gotten the PCollection of baby predictions, the next thing that we need to do is write those to BigQuery and so in order to do that just like we use the PubsubIO, we're now going to use BigQueryIO and BigQueryIO expects a few more parameters. We need to supply the output table as well as whether we intend to append our data or overwrite it and instructions for whether we need to create the table. The next file that we'll look at is the second child class of InputOutput.java, and in this file, we also extend InputOutput which means that we also need to implement readInstances and writePredictions, just as before readInstances returns a PCollection of babies. Although in this case, instead of reading from Pub/Sub, we'll read from Google Cloud Storage. We read from Google Cloud Storage on line twenty 21 where we use the TextIO.read function and we pass that in an input file. Each input file consists of new line delimited lines within which each line represents a baby and the fields are delimited by commas. So once again we can make good use of our baby.fromCsv function to parse individual lines. After ingesting the data, we can then call the writePredictions, and as before, writePredictions makes a call to functionality that's defined with an InputOutput.java via the addPredictionInBatches function. Once we've collected a PCollection of baby predictions, we can then write those to disk. In this case, we're going to convert those to CSVs first and then call textIO.write, passing in the name of the file we want and the suffix that we want. So there you have it. Our solution for mitigating training serving skew through code. We mitigated this problem by using polymorphism. Implementing the parts of the pipeline that don't change within a single parent class called InputOutput.java and then implementing the parts of the code that do need to change in separate child classes, which were enforced via a contract in the parent class.