1
00:00:00,000 --> 00:00:03,525
In this section, we're going to learn how to deploy

2
00:00:03,525 --> 00:00:07,545
a few common automatic ETL pipelines into Google Cloud Platform.

3
00:00:07,545 --> 00:00:10,855
A link to the sample code will be provided in the course.

4
00:00:10,855 --> 00:00:13,050
The patterns using these pipelines represent

5
00:00:13,050 --> 00:00:17,270
the two primary methods of loading data in any system, push or pull.

6
00:00:17,270 --> 00:00:21,865
Both patterns take advantage of Google's wide array of services from orchestration,

7
00:00:21,865 --> 00:00:26,234
storage, transformation and processing, monitoring and warehousing.

8
00:00:26,234 --> 00:00:31,270
Let's look at each of them now and then see a demo of data loading and action.

9
00:00:31,590 --> 00:00:34,685
In our first pattern, the push solution,

10
00:00:34,685 --> 00:00:39,085
we'll look at how easy it is to load data on-demand with Google Cloud Platform.

11
00:00:39,085 --> 00:00:40,685
This solution takes advantage of

12
00:00:40,685 --> 00:00:43,715
Google Cloud Functions to invoke the data loading process.

13
00:00:43,715 --> 00:00:48,205
This architecture is best for those wanting ad hoc or invent based loading.

14
00:00:48,205 --> 00:00:54,390
An ETL pipeline begins with a setup of google-cloud composer to orchestrate the pipeline.

15
00:00:54,430 --> 00:00:57,470
After the composer graph has been set up,

16
00:00:57,470 --> 00:01:00,310
it waits for an event from Cloud Functions to start.

17
00:01:00,310 --> 00:01:03,230
The event that the Cloud Functions is triggered on is

18
00:01:03,230 --> 00:01:06,200
dropping a file into Google Cloud Storage bucket.

19
00:01:06,200 --> 00:01:09,500
Once a new file is dropped into the Cloud Storage bucket,

20
00:01:09,500 --> 00:01:14,740
it is picked up and sent to transformation and processing in Google Cloud Dataflow.

21
00:01:15,490 --> 00:01:20,600
Dataflow also checks for any errors in the file based on business rules.

22
00:01:20,600 --> 00:01:24,645
Once the dataflow job is complete or a file is found to be invalid,

23
00:01:24,645 --> 00:01:29,010
the source files transferred into an output bucket back into Cloud Storage,

24
00:01:29,010 --> 00:01:35,220
and the process data is then loaded into a separate bucket for loading into BigQuery.

25
00:01:37,160 --> 00:01:41,180
From there, users can query the data interactively or

26
00:01:41,180 --> 00:01:43,025
leverage visualization tools like

27
00:01:43,025 --> 00:01:47,850
Google Data Studio or machine learning engine for advanced analytics.

28
00:01:48,830 --> 00:01:53,760
Now, let's look at the other ETL pattern, the pull model.

29
00:01:54,280 --> 00:01:57,800
The architecture for the pull model is very similar to

30
00:01:57,800 --> 00:02:02,695
the push model with the only difference being how the entire pipeline is invoked.

31
00:02:02,695 --> 00:02:05,750
This pattern expects that the file will be waiting

32
00:02:05,750 --> 00:02:09,065
there to set schedule instead of starting upon an event.

33
00:02:09,065 --> 00:02:12,110
As you could have guessed, pull models are better for when

34
00:02:12,110 --> 00:02:14,910
there is a repeatable process and scheduled interval,

35
00:02:14,910 --> 00:02:17,560
instead of firing on-demand.

36
00:02:20,120 --> 00:02:24,100
Now, let's see the pipelines in action.

37
00:02:25,510 --> 00:02:29,320
Now that the file has been sent and job has kicked off,

38
00:02:29,320 --> 00:02:31,900
I want to show you what's happening behind the scenes.

39
00:02:31,900 --> 00:02:35,840
Google-cloud composer, a managed version of Airflow is

40
00:02:35,840 --> 00:02:40,370
handling all of the orchestration for our job as you seen in the ETL diagrams.

41
00:02:40,370 --> 00:02:44,260
Let's take a look and see what's happening within this particular job.

42
00:02:44,260 --> 00:02:47,095
When I looked at the graph view,

43
00:02:47,095 --> 00:02:51,500
we see that there are three potential parts of this job, right?

44
00:02:51,500 --> 00:02:55,460
Now that data is running through the processing and transformation stage.

45
00:02:55,460 --> 00:02:58,800
Upon successful completion of transformation,

46
00:02:58,800 --> 00:03:01,160
it will then be loaded into BigQuery and pushed

47
00:03:01,160 --> 00:03:04,145
into the appropriate bucket in Google Cloud Storage.

48
00:03:04,145 --> 00:03:07,850
If dataflow finds any errors with the file,

49
00:03:07,850 --> 00:03:10,945
it will move into a failure bucket and the job will end.

50
00:03:10,945 --> 00:03:14,680
Let's take a look in data flow to see how things are going.

51
00:03:15,080 --> 00:03:19,560
We can see that the job has been kicked off and it's in process right now.

52
00:03:19,560 --> 00:03:25,670
Opening up the job, we can see that each of the steps within dataflow have executed.

53
00:03:25,670 --> 00:03:28,550
What's important to know here is that during a job,

54
00:03:28,550 --> 00:03:32,045
each of these will be highlighted in green and you'll see the status.

55
00:03:32,045 --> 00:03:35,210
Upon completion, the succeeded message shows us that

56
00:03:35,210 --> 00:03:39,580
everything was successful and finally the data was written to BigQuery.

57
00:03:39,580 --> 00:03:42,930
Let's go ahead and just verify that now.

58
00:03:43,750 --> 00:03:45,955
If I run this query,

59
00:03:45,955 --> 00:03:49,620
this will show us all the data that was picked up from our file.

60
00:03:50,230 --> 00:03:54,920
As you can see, all of the rows have been loaded.

61
00:03:54,920 --> 00:03:57,435
This completes the end of this demo.

62
00:03:57,435 --> 00:04:00,865
Again, the source code can be found within this course.

63
00:04:00,865 --> 00:04:03,440
We hope you found this is intuitive and is easy

64
00:04:03,440 --> 00:04:06,600
to get started with this we did. Thank you.