Next, let's see how fast we can load a dataset directly into BigQuery, from Google Cloud Storage. In this demo, we'll load taxi cab data from a csv hosted on GCS into BigQuery and preview the results. First, I go ahead and enter BigQuery, and BigQuery is at bigquery.cloud.google.com. Let's do that. Okay. I'm now in BigQuery. First thing I need to do is to create a dataset called a BigQuery example, and to do that, I use that dropdown arrow from my project name, and I hit Create new dataset. I create a dataset called bigquery_example. If I hit Okay, BigQuery has created that dataset that is shown here on the left pane. Next, I want to go ahead and create a new table under dataset. To do that, you use the dropdown menu at the end of the new BigQuery example I just created. So if I click "Create new table" it gives me some options. The first thing I need to change is a location. By default, the location is a file upload. But I'll change that because I want to pull the data, the source data from Google Cloud Storage, and when I do that, it'll ask me for the URL to my GCS bucket. I'll enter the full URL which is the CSV file containing this yellow caps data. Second thing to change here is the name. We need a name for the new table. You can call it anything you like, in this case I'll just call it,"Taxi table." Then the last thing we need to do is a schema. By default, select this option here, if you select the checkbox on schema, which tells BigQuery to automatically detect the schema from the CSV file. At that point, if you create table, BigQuery will go ahead and load that data and new see it loaded it within just a few seconds, and at the end you find it's already loaded the data, welit created the taxi table and load that data into it. Let's look at the taxi table to see what was done here. First, we'll look at the schema. It automatically detected the schema from the CSV file, which is correct. We have fields, an integer field like pickup_hour, there's another integer field for counting the sum of passengers, et cetera. If you click on details, it gives me more details of this particular table. You can see from the timestamp that we just created it right now, we also have the table size which is a very small table, 105 kilobytes. We can also preview this table. If I click on preview, we see a snapshot of that data. You can scroll down, you can scroll left, to see the remaining fields. That sit in a nutshell. I just very quickly showed you how you can import data from a CSV file sitting in Google Cloud Storage into a new table in BigQuery. You just saw how easily you can import your own CSV data from a GCS bucket into BigQuery. BigQuery supports a wide variety of import data types from CSV, Jason to Avro, ORC, and more recently, Parquet as well. Since BigQuery is part of the Google Cloud Platform, you can also load data directly from a readable data source, like Google analytics, or data from Cloud Dataflow. During ingestion, you can specify whether you want new data appended to the end of a table, or to completely replace the existing data in a table. Lastly, we will close with an end-to-end architecture demo by my colleague Tony who will show you how you can set up an automatic ETL pipeline into GCP using the newly available workflow tool, Cloud composer.