In this section, we're going to learn how to deploy a few common automatic ETL pipelines into Google Cloud Platform. A link to the sample code will be provided in the course. The patterns using these pipelines represent the two primary methods of loading data in any system, push or pull. Both patterns take advantage of Google's wide array of services from orchestration, storage, transformation and processing, monitoring and warehousing. Let's look at each of them now and then see a demo of data loading and action. In our first pattern, the push solution, we'll look at how easy it is to load data on-demand with Google Cloud Platform. This solution takes advantage of Google Cloud Functions to invoke the data loading process. This architecture is best for those wanting ad hoc or invent based loading. An ETL pipeline begins with a setup of google-cloud composer to orchestrate the pipeline. After the composer graph has been set up, it waits for an event from Cloud Functions to start. The event that the Cloud Functions is triggered on is dropping a file into Google Cloud Storage bucket. Once a new file is dropped into the Cloud Storage bucket, it is picked up and sent to transformation and processing in Google Cloud Dataflow. Dataflow also checks for any errors in the file based on business rules. Once the dataflow job is complete or a file is found to be invalid, the source files transferred into an output bucket back into Cloud Storage, and the process data is then loaded into a separate bucket for loading into BigQuery. From there, users can query the data interactively or leverage visualization tools like Google Data Studio or machine learning engine for advanced analytics. Now, let's look at the other ETL pattern, the pull model. The architecture for the pull model is very similar to the push model with the only difference being how the entire pipeline is invoked. This pattern expects that the file will be waiting there to set schedule instead of starting upon an event. As you could have guessed, pull models are better for when there is a repeatable process and scheduled interval, instead of firing on-demand. Now, let's see the pipelines in action. Now that the file has been sent and job has kicked off, I want to show you what's happening behind the scenes. Google-cloud composer, a managed version of Airflow is handling all of the orchestration for our job as you seen in the ETL diagrams. Let's take a look and see what's happening within this particular job. When I looked at the graph view, we see that there are three potential parts of this job, right? Now that data is running through the processing and transformation stage. Upon successful completion of transformation, it will then be loaded into BigQuery and pushed into the appropriate bucket in Google Cloud Storage. If dataflow finds any errors with the file, it will move into a failure bucket and the job will end. Let's take a look in data flow to see how things are going. We can see that the job has been kicked off and it's in process right now. Opening up the job, we can see that each of the steps within dataflow have executed. What's important to know here is that during a job, each of these will be highlighted in green and you'll see the status. Upon completion, the succeeded message shows us that everything was successful and finally the data was written to BigQuery. Let's go ahead and just verify that now. If I run this query, this will show us all the data that was picked up from our file. As you can see, all of the rows have been loaded. This completes the end of this demo. Again, the source code can be found within this course. We hope you found this is intuitive and is easy to get started with this we did. Thank you.