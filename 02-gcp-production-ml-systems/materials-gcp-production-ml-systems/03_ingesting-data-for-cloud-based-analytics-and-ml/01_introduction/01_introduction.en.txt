Hi, I'm Val. I'm Machine Learning and AI Specialist at Google. I want to warmly welcome you to the second course in our advanced specialization. In this module, we'll talk about how to bring your data to the cloud. There are many ways to bring your data to the cloud, to power your machine learning models. First, we'll review why your data needs to be on the cloud, to get the advantages of scale and using fully managed services, and what options you have to bring your data over. Next, we will discuss several common scenarios for where your data currently is, and the best ways you can bring it to the Google Cloud Platform. These scenarios are, having data on premise and using common tools like gsutil to transfer. Next, is working with large datasets and how typical networks can bottleneck at large scale. Then, we'll cover if your data is already on other clouds and how you can set up scheduled transfers. Lastly, if you have existing databases with structured or unstructured data, what ways you have to ingest into what Cloud-based database products. One of the key fully managed services we've discussed in previous lectures on architecture, was Cloud ML Engine or CMLE. As you recall, CMLE supports ML data in both batch and streaming form, but the key is your data must be in the cloud before you can get any of the benefits of these fully managed services. While data migration may be an easy effort if you only have a few small datasets, it's often much harder problem to tackle. Now let's cover a few of the key migration challenges. Here are some of the common data migration challenges our customers often run into. First is the sheer volume of data captured and stored, because data is growing exponentially. At digital market forecast research report by ESG, has unstructured data growth at about 56 percent year on year. In fact, we see plenty of data growth rates of up to two times a year. With more and more data, resources are simply overwhelmed and networks are constrained. Next, having dedicated bandwidth for data transfers is often too expensive and it will disrupt normal operations of the business, and building new network infrastructure is not typical expertise or time that IT departments have readily available. Which is why Google Cloud Platform offers a suite of solutions that enable you to move data to GCP fast regardless of the data's location or type, your network size, and utilization or its planned use. Let's cover some of these options now. First is online transfers. Gsutil is a command line interface that many of our customers use to get their data into GCS. We also offer easy drag-and-drop folders in Chrome. Next is a transfer appliance. A rackable high-capacity storage server that we shipped to you to fill it up and ship it back to us. One hundred terabytes and half petabyte versions that, with compression, could transfer 200 terabytes or up to a petabyte. It's best used for lots of data where your network situation won't meet transfer demands. After that, is a cloud storage transfer service that we call Cloud-2-Cloud. Common use cases are migrating off of other cloud buckets or often backing up other cloud bucket data. It's also a great way to transfer large amounts of data between regions at Google. Lastly, is a BigQuery transfer service, which automates loading data into BigQuery from YouTube, Google AdWords, and Double-click. We also have many partners that provide options for each of these use cases and often compliment these use cases. These transfer solutions were designed to help you take advantage of GCP as quickly as possible. For example, media archive migration can be accelerated with the transfer appliance and cost-effectively stored in cold line, but still served with low latency whenever needed. AdWords data can flow directly into BigQuery for analysis, or in fact, Cloud storage transfer service could be used to backup your s3 data.