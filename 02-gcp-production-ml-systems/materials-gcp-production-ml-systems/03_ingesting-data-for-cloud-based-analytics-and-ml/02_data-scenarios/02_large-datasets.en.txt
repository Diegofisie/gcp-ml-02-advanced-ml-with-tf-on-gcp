The next scenario we will cover is what to do when you have large datasets. At threshold for large here is about 60 terabytes of data. One of the newly available options is receiving a physical Google device called a Transfer Appliance, which is Google's rackable high-capacity storage server. Once received you can load data at your own data center on the transfer appliance and ship the data over to Google Cloud Platform. Each appliance can hold up to one petabyte. Consider using transfer appliance if you have over 60 terabytes of data regardless. This is common for IoT data, archival data, media libraries or backup images. Also consider it if it takes you more than one week to upload your data or if you have one terabyte or more of data and a ten megabits per second or slower connection. To better understand how network capacity can bottleneck uploads, let's visualize how long it takes to transfer data. At large volumes of data like one petabyte over a typical 100 megabit connection, it would take three years to migrate that data. Also keep in mind that large uploads also degrade network performance for other users of your network. Here's a visualization of how fast it takes to transfer a petabyte of data to the Cloud, using a typical network versus the transfer appliance. A petabyte of data in the cloud is 96 percent faster than over a typical network. As you saw, it took 43 days for a petabyte and 16 days for 100 terabytes. The larger your dataset or the slower your network, the more you stand to benefit from using transfer appliance. Let's look at some real customer use cases who've used the Google Cloud Transfer Appliance. Evernote, a mobile app designed for note-taking, organizing task lists and archiving, use the Google Cloud Transfer Appliance to transfer their datasets over. What was their constraint that made the transfer appliance the best option? Was it network bandwidth? Real-time data or data size? Well, it turns out the limiting factor was data size because Evernote had over three petabytes of data. Makani is a Google X company, that's working to make clean energy accessible for everyone. They develop energy kites, a kind of wind power technology that can access stronger and steadier winds at high altitudes to generate more energy with less materials. This requires picking optimal locations for weather and typography, which requires lots of environmental and geographic data. Makani brings petabytes of this data into their systems via Google Transfer Appliance. So, what was the data migration limiting factor here? Was it bandwidth? Real-time data or data size? It turns out from Makani, the limiting factor is real-time distributed data collection plus petabyte data size.