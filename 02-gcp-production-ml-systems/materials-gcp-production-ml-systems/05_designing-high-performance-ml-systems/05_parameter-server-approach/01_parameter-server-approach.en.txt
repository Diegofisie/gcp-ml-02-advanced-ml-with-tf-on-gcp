The parameter server approach is not always as simple and clean as the allreduce approach. However, it is more mature and it is applicable to both data parallelism and model parallelism. Now, remember when I talked about data parallelism as a great way to increase training throughput. I showed you this graph and how the training of the inception image model scales to more and more workers. Well, the inception model is nice because it fits neatly into a single GPU. But we also have things like the neural machine translation model, and this model benefits from packing eight GPUs into a single server and then using multiple servers in parallel to train the model. The diagram shows a deep LSTM model that has eight GPUs per worker. Another reason you might want to use multiple computers is if your model is too large to fit into a single machine. This is true for many of the wide and deep models that use large embedding matrices to map feature crosses down to a dense representation that we can handle in a deep neural network. To do distributed training, all you need to do is use an estimator and call the train and evaluate method. Estimator contains the implementation of three functions: training, evaluation, and serving. You provide the model as a function which returns the ops required when given inputs. It's mostly the TensorFlow code that you've written already. Estimator then provides a standard interface that you can use to perform the tasks that you need with that model. It encapsulates all the details about sessions and graphs and handles the details of actually training or running the model. Because we now have a standard interface for your model, it is easy to build infrastructure around models without having to worry about the implementation of each model. Estimator also supports exporting the model for use with TensorFlow Serving. This ensures that you can easily productionize the model once you've trained it. A train and evaluate function bundles together an estimator with input functions for reading a particular dataset, as well as other configuration options. So, you simply specify an estimator which might be a pre-canned model that fits on a slide like this DNN classifier here and then wrap it in an experiment function. You then pass that function to train and evaluate, which, for example, if you're running on Cloud ML, will pick up the configuration from the environment and run the appropriate low-level code for you.