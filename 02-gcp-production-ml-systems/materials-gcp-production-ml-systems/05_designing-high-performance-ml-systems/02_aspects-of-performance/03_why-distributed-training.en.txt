In the previous lesson, we talked about several ways of achieving high performance. We said that you could run the ML model on a hardware that was more suitable for example with faster CPUs or maybe using GPUs or TPUs. You can also scale out the training onto multiple machines and finally, you could look at more efficient model architectures. However, all these three using heterogeneous systems for ML training, training on distributed systems of machines or experimenting with model architectures, they all add complexity to your production ML system. Production machine learning systems become more complex because of this kind of flexibility that you need. Your production ML systems are complex precisely so that they can provide the high performance that you need. If you want high performance training and scalable inference, chances are that the code that you write has to work on CPUs, GPUs and maybe even TPUs. The business or will often dictate that inference has to happen right on the device itself and so, the model code has to be able to run on a phone or an embedded chip such as the Raspberry Pi or an edge TPU. We have a heterogeneous system at the very least whatever code we write, has to work on GPUs and CPUs and as you may know Google has announced that we have special hardware so it should work on our TPU as well and unless you want to rewrite your code when you deploy it in a mobile app it should also work on mobile platforms and for those of you interested in IoT the Internet of Things, It should also work really well on embedded chips such as a Raspberry Pi or an edge TPU. Deep learning works because data sets are large. Look at the x-axis on the graph here, you'll notice that the x-axis is logarithmic. For every doubling in the size of the data the error rate also falls linearly. A more complex model can also help and that's the jump from the blue line to the orange line but more data helps a lot more. As a consequence of both of these trends in terms of larger data sizes and more complex models, the compute required to build state of the art models has grown over time and as you can see this growth is exponential as well. Take a look at the y-axis on this graph, each y-axis take is showing a 10X increase in computational need. AlexNet which started the deep learning revolution in 2013 required less than 0.01 petaflops per second day in compute per day for training. By the time you get to neural architecture search, the learn to learn model published by Google in 2017, you need about a 100 petaflops per second day or a thousand times more compute than you needed for AlexNet. This growth in algorithm complexity and data size means that distributed systems are pretty much in necessity when it comes to machine learning. Real-world networks can be very large meaning millions of weights. Training complex networks with large amounts of data can often take a long time. In this graph you can see training time on the x-axis and accuracy of predictions on the y-axis when training an image recognition model on a GPU. As you can see it took more than 80 hours to reach 75% accuracy. If your training takes from a few minutes to a few hours it will make you productive and happy. You can try out different ideas first. If it takes a few days, you can still deal with it by running a few things in parallel. If it starts to take a week or more, your progress will slow down because you cannot try out new ideas quickly and if it takes more than a month then that's probably not even worth thinking about and this is not an exaggeration. Training deep neural networks such as resonant 50 can take over a week on a single GPU. A natural question to ask is, how do you make training fast? There are a number of things that you can do to make it faster. You can use a more powerful device such as TPUs, this is a Tensor Processing Unit or your input pipeline might be slow and you can make that faster. There are in fact many performance guidelines on the TensorFlow website that you can try. We'll look at some of the guidelines in this module. Let us look at distributed training i.e running training in parallel, o n many devices such as CPUs or GPUs or TPUs use in order to make your training foster. This graph shows images per second when training an image recognition model in TensorFlow. You can see that each time we double the numbers of GPUs, we're able to process close to double the number of images.