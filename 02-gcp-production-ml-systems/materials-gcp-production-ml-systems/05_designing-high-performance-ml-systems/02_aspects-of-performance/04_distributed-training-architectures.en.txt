Before going into the details on how you can achieve this scaling in TensorFlow, let's first talk about high-level concepts and architectures in distributed training. This will give us a strong foundation to understand the various solutions. So let's say you start with training on a machine with a multi-core CPU, TensorFlow will actually automatically handled all of the scaling across these multiple cores. You may speed up your training by using an accelerator to your machines such as a GPU or a TPU, and with distributed training, you can go even further. You can go from using one machine with a single device, to a machine with multiple devices attached to it. Finally, to multiple machines possibly with multiple devices each connected over a network. Eventually the various approaches will allow you to scale up to hundreds of devices, and that's indeed what we do in several Google systems. The rest of this module, will use the term device, or worker, or accelerator interchangeably to refer to processing units such as GPUs or TPUs. How does this distributed training actually work? Like everything else in software engineering, there's a few different ways to scale training. Which one you choose depends on the size of your model, the amount of your training data, and the devices that are available to you. The most common architecture for distributed training is known as data parallelism. Data parallelism you're on the same model and computation on every device, but you train each of them using different training samples. Each device computes loss and gradients based on the training samples it sees. Then we'll update the model's parameters using all of these gradients. The updated model is then used in the next round of computation. There are currently two approaches used to update the model using gradients from various devices. The first one is commonly known as the asynchronous parameter server architecture. In asynchronous parameter server architecture, some devices are designated to be parameter servers, and others as workers. Each worker independently fetches the latest parameters from the parameter server, and computes gradients based on a subset of training samples. It then sends the gradients back to the parameter server which updates its copy of the parameters with those gradients. Each worker can do this independently. Now this allows it to scale really well to a large number of workers, and this methodology has worked very well for many models, and Google were training workers might be preempted by higher priority production jobs, or machine might go down for maintenance, or maybe sometimes there's an asymmetry between the workers. Now, these don't hurt the scaling, because the workers are not waiting for each other. The downside of this approach, however, is that workers can get out of sync. They compute parameter updates based on stale values, and then this can delay convergence. This architecture has been around for some time, and it's what we did in the first specialization, when we call the train and evaluate method of the estimator. We'll look at this in more detail shortly. The second approach is called synchronous allreduce. With the rise of powerful accelerators such as TPUs and GPUs over the last few years, this approach has become more common. In this approach, each worker holds a copy of the model's parameters. There are no special service holding these parameters. Each worker then compute gradients based on the training samples that they see, and they can communicate this between themselves to propagate the gradients and update their parameters. All of the workers are synchronized, conceptually the next forward pass does not begin until each worker has received the gradients, and updated their parameters. With fast devices in a controlled environment, the variance between the step time on each worker is quite small. When combined with strong communication links between the workers, the overhead of synchronization is also small. So overall this approach can lead to faster convergence. Given these two broad strategies that's the asynchronous parameter server approach, or the synchronous allreduce approach, when should you pick one over the other? Well, there isn't really one right answer, but here are some considerations. The parameter server approach should be preferred when there are large number of not so powerful, or unreliable workers such as a cluster of machines which are CPAs. If you have fast devices with strong communication links such as multiple GPUs on one host or if you're using TPUs, then all reduces probably a better choice. So, parameter server for multiple machines allreduce for multiple devices on one machine. Another consideration is the relative maturity of the two approaches. Parameter server approaches supported very well by TensorFlow in the estimator APIs train and evaluate method. This is definitely the more mature approach. The allreduce method is getting a lot more traction recently because of the improvements in hardware. So, for example, TPUs will use the allreduce approach right out of the box. With recent improvements to TensorFlow, you can scale your training with allreduce on multiple GPUs. We'll take a look at this later in this module, but the code is still in tf.contrib, which is not core TensorFlow. So the allreduce may be better if you have a machine with multiple GPUs, or if you have a TPU, but that's still experimental, and the APIs are subject to change. A third consideration is what performance constraint you will run into if your model is primarily I/o bound, and then using multiple machines can be a better approach, and you should use the parameter server approach. If your model is primarily compute bound, then you want to use more powerful processors, and then they allreduce approach could be better. Besides data parallelism, there's another type of distributed training called model parallelism. A simple way to describe model parallelism is that when your model is so big that it doesn't fit into a single devices memory. So you then have to divide it into smaller parts that can compute over the same training samples on multiple devices. For example, you could put different layers on different devices, and we'll take a look at this in a later lesson.