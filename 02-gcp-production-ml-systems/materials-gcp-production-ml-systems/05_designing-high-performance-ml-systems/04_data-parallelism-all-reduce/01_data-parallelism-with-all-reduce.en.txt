Let's look first at data parallelism. We will look at
the All Reduce approach and defer the Parameter Server approach
to the next section of this module. So why would you do data parallelism? Well, there's a few reasons. The most common reason is that you might
want to increase the throughput of training by harnessing a load of GPUs
to train the same model in parallel. This graph, taken from a Google research
paper, shows how the training throughput of the inception image recognition model
increases when using up to 200 GPUs. In the next section, we will look
at low level device placement. But here, I'll show you the new
distribution strategy API. It allows you to distribute your
training intensive flow with very little modification to your code. With the distribution strategy API,
you no longer need to place apps or parameters on specific devices. You don't need to worry about structuring
your model in a way that gradients or losses are aggregated correctly
across devices either. Distribution strategy takes
care of all of that for you. The API is easy to use and fast to train. So now, let's take a look at some code
to see how you can use this new API. We'll be using TensorFlow's
high level estimator API. If you've used it before, you might be
familiar with the following snippet of code to create a custom estimator. It requires three arguments. First, you need to function
the that defines the model, which is often called a model function. It defines the parameters of the model, the loss and gradient computation, and
of course how the parameters are updated. Next, you need to specify a directory
where the model state will be persisted. And third, a configuration called
RunConfig that specifies things like how the model state is checkpointed, how
often summaries are read in, and so on. Here, we'll simply use
the default RunConfig. Once we create the estimator, we can initiate training by calling
the train method on the estimator with an input function that
provides the data for training. In order to distribute this
training on multiple GPUs, you simply need to add one line. You create an instance of something
called a mirrored strategy and pass it to the runconfig call. Mirrored strategy is part of
the distribution strategy API. With this API, you don't need to make
any changes to your model function, or input function, or your training loop. You don't even need to specify the GPUs. The API will detect that for you, and
will run training on all available GPUs. So that's all the code changes
that you're going to need. This API is currently in contrib,
but you can still try it out today. So let me explain
Mirrored Strategy a little bit. Mirrored Strategy implements
synchronous all-reduce architecture right out of the box. The model's parameters
are mirrored across all devices, hence the name Mirrored Strategy. Each device computes loss and gradients
based on a slice of the input data. Gradients are then aggravated across all
devices using an all reduce algorithm that's best for the device topology. In the next section of the course,
as I mentioned earlier, you won't need to make any changes
to the model or training loop. That's because we've changed the
underlying component of TensorFlow, such as optimizer, batch norm summaries and
all that, to become distribution aware. Input functions do not have to change as long as it uses the recommended
TensorFlow dataset API. Saving and checkpointing work
seamlessly so you can save with one or no distribution strategy,
and resume with another. Mirrored strategy is just one type
of distribution strategy API. We're working on other strategies for
other common use cases. Mirrored strategy really helps on TPUs
to get you a world beating performance. This is from a benchmarking study
carried out by Stanford University. I pulled this in July, 2018, but
the latest data was from April, 2018. Please do visit the page to
see the latest comparisons. When it comes to training speed, the fastest three options are all
TensorFlow and all on the Cloud TPU. Note that the essential difference
between the 30-minute training run and the one-hour training run is the 30-minute
one used half of a TPU version two pod, while the one-hour one used
only a quarter of the TPU pod. Performance does scale quite
linearly on the TPU pods thanks to the mirrored strategy.