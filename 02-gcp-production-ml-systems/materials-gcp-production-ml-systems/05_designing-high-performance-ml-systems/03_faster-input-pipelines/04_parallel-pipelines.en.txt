Let's look at the second, third approaches, and other ways to improve performance even further. Here is a simplified input pipeline for an image classification model, where I'm assuming that we're doing option number three and that's reading TF records into our TensorFlow Program. TF.data API is the recommended way of building input pipelines in TensorFlow. It enables you to build complex input pipelines from simple, reusable pieces. It makes it easy to deal with large amounts of data, different data formats, and complicated transformations. In this particular pipeline, we must first use a method called list files to grab a bunch of input files containing the images and labels. We will then parse them using a TF record dataset reader. We will shuffle the records and repeat them a few times if we want to run multiple epochs. Finally, we preprocess each record using the dataset map. The preprocessing may involve cropping, flipping, extra. Finally, you batch input based on the desired batch size. This input pipeline is what we've looked at so far. A typical input pipeline can be thought of as an ETL process, i.e that's a process involving extract, transform, and load phases. In the extract phase, we read data from persistent storage, either local or remote. In the transform phase, we use CPU cores to parse and perform preprocessing operations on the data such as image decompression, cropping, flipping, shuffling, and batching. At the load stage, we load the transform data onto the accelerator devices that will then execute the model. This pattern effectively uses the CPU while reserving the accelerator for training your model. So, how does this apply to the code that we just saw? In the extraction phase, we start by getting a list of files from the local or remote data source. We will then extract the TF records from those files. In the transform, we apply a number of transformations to the input records such as shuffling, mapping, and batching. The load phase then tells TensorFlow how to get the data from the dataset. This is what our example input pipeline would look like. Each stage of the input ETL extract transform load as well as the training on the accelerator will happen sequentially. While the CPU is preparing the data, the accelerator is sitting idle. Conversely, while the accelerator is training the model, the CPU is sitting idle. The training step time is thus the sum of both the CPU preprocessing time and the accelerator training time. The different stages of the ETL process use different hardware resources. The extract phase uses local storage, the transform phase uses the CPU cores to perform transformations, and the model is trained on the GPU. When we parallelize those stages without resource contention, we're able to overlap preprocessing of the input data on the CPU with model training on the GPU, and this is called pipelining. Parallelizing and pipelining in various stages of the pipeline can really help reduce training time. Since you're dealing with a large data set sharded across Google Cloud Storage, you can then speed up by reading multiple files in parallel to increase the effective throughput. You can then use this feature with a single option to the TF record dataset constructor called num parallel reads. You can then paralyze data-parallel transformation operations across many cores using the num parallel calls argument. You would typically use the number of CPU cores to determine the number of parallel calls that you would want to execute in the map function. Finally, you should use prefetch call at the end of your input transformation. The prefetch transformation decouples the time data is produced from the time it is consumed. It prefetching the data into a buffer and parallel with the training step. This means that we have input data for the next training step before the current one is completed. This is what we had before and this is what our input pipeline looks like now. As you can see, we're able to increase accelerate a utilization which leads to faster training. The CPU and accelerator are not idle for long periods as we saw previously. While the accelerator is performing training step n, the CPU is preparing the data for training step n plus one. Neither the CPU nor the accelerator is idle in this case. Doing so reduces the step times to the maximum as opposed to the sum of the training and the time it takes to extract and transform the data. However, even with the mentioned pipelining and parallelization optimizations, you may still find your accelerator could be slightly underused. At this stage, there's a few advanced optimizations that you can try out. One common optimization is to switch to the fused versions of some of the transformations we've seen so far. Fusing is to combine operations so that specific optimizations are made possible. Let's fuse together, shuffle, and repeat and then we'll also fuse, map, and batch. Here I fuse, shuffle, and repeat, which avoids a performance stall by overlapping the buffering for Epoch n plus one while producing elements for Epoch n. I've also fuse the map and batch which parallelizes both the execution of the map function and the data transfer of each element into the batch tensors. Together, these can provide improvements for models that consume a large volume of data per step. By using some of the advanced optimization techniques, we were able to improve pipeline input efficiency further, thereby providing data to the accelerator faster. As you can see in this diagram, the accelerator is now nearly 100% utilized.