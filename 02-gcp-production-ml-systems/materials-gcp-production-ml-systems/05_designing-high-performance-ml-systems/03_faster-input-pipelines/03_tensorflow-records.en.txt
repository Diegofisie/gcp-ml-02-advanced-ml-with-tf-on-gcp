The third approach is to convert the images into TensorFlow records. You do this in a preprocessing operation. Then in the TensorFlow training program, you can read the data quite rapidly. TFRecords are set for fast efficient batch reads without the overhead of having to parse the data in Python. Here you can see I'm doing the preprocessing using Apache Beam, but you could do it in any Python program. If you are just converting images to TFRecords, there is no state to remember, and so plain Apache Beam will just work. However, if you want to remember things from the transformation, such as the vocabulary files used for scaling constants, then you could do the conversion using tf.Transform. For Spark, you can do a transformation on the DataFrame and just write it out. Repeating the preprocessing during prediction is anew, Spark and Spark Streaming don't use the same paradigm, unlike Apache Beam. Once you have the TensorFlow records, regardless of how you created it, the input function in the TensorFlow graph comes from TensorFlow transform. The metadata is things like the vocabulary and scaling, so that the input function can transform the word hello to 03897334. Of the three approaches, native Python, native TensorFlow and TFRecords, the TFRecords approach is the most efficient.