Now that we've looked at distributed training at a high level, let's turn our focus to building faster input pipelines. Regardless of which distributed TensorFlow architecture you use, whether you do data parallelism or model parallelism, and regardless of whether you implement the data parallelism with a parameter server or with all reduce, you should try your best to improve the performance of your input output pipeline. Now, this becomes very essential if your model is IO bound, and this will be the case if you're using multiple GPUs or if you're using TPUs. When running a model on a single GPU, the input pipeline is able to pre-process and provide input to the model as required. But GPUs and TPUs can process data much faster than CPUs, and they reduce the execution time of training one step. Hence, as you increase the number of GPUs, the input pipeline can no longer keep up with the training and often becomes a bottleneck. Specifically, before an iteration finishes, the data for the next iteration is not yet available for processing. Achieving peak performance in this scenario relies on having an efficient input pipeline. There are three approaches to reading data into TensorFlow. Remember that you get training data into your model graph, you'll need to use an input function. So essentially, we're looking at three different ways to implement a training input function. I've arranged them from slowest to fastest. The first and simplest approach is to directly feed from Python. This is what you will see in a lot of toy examples because this is the easiest and the most flexible way, but unfortunately it's also the slowest. The second approach is to use native TensorFlow Ops. We've looked at this already for CSV and JSON, but we'll have to do a quick recap. I will also show you how to do this if you're reading image data. The third and fastest approach is to read transformed TensorFlow records. There is some engineering complexity involved, because you have to plan for this in your architecture and convert data from its native format to tf records. Often you will do this by incorporating apache beam using tf transform or apache spark in your data pipeline. Now, let's say you've read in your data, and it's in a Pandas dataframe called dataset. Here is an input function that's able to pull the relevant columns out of the pandas dataframe, shuffle it, batch it, and then supply it to the trainer. This is really fast, but only because the entire dataset is held in memory. So, it's not very scalable. For most realistic problems, keeping all of your data and memory is a sure far a way of taking a problem that is IO bound or CPU bound, and then making it memory bound. Pandas input function is part of TensorFlow core. There is a similar function called Numpy input function, which will work with any data held in Numpy arrays. Numpy of course is the typical representation for numeric data in Python. So, if your company has some proprietary data format that all of your data is in, and you don't want to convert them into TensorFlow records, then this might be your best option.