To tune an encoder-decoder model, we have four machine learning step in the pipeline: embed, encode, attend, and predict. Before turning the encoder network, we need to turn words into some numerical tensors. We can train the embeddings, as learned in earlier modules of this course, or we can use pre-trained embeddings as the starting point of our saved the vocabularies and embeddings for the predictions. In the later example, we were trained the multi-layer GRUs to save memory and state. The latter will be similar if we used LSDMs. Well, tune the GRUs carefully to make sure that the input sequences are rich enough for the model to learn long-term dependencies. We will dynamically weigh the outputs based on the current input words. Also, we need to keep in mind that the number of attention heads is an important hyperparameter for our model training. Once we have the model trained, we're ready for the predictions. These several lines of codes implements a translation model without attention. We will add annotation network later. Using the embedding look-up method, the English words are transformed into vectors or embeddings. We instantiates a GRU cell for the encoder. A dropout wrapper can be used for regularizations. Then, the GRU cell is enrolled as needed by using the dynamic RE method. A second and the separate GRU cell is instantiated for the decoder. Here, we specify that we want the decoder to use Bing search algorithm for decoding. The beam width parameter is a trade-off between speed and accuracy. Beam width equals to one is equivalent to greedy search where the most approbable words is choosing at each time stamp of the decoding process. Beam width equals to two or more means that two or more most approbable words will be retained and the decoding will proceed from them in a branching tree fashion. The most approbable sequence is retained in the end. Finally, dynamic decode enrolls the decoding process across as many time steps as necessary to reach the end of sentence token or maximum iterations. This is how we add the attention network into our model. Take the GRU cell that we instantiate it for the decoder. To add attention network, here, we use Louong attention, inspired by Minh-Thang Luong, as an example. There are other attention networks available such as Bento attention. Those attention networks have their implementations in TensorFlow and their corresponding names. The encoders sentences is the output of the encoder. The goal of the attention network is to compute a weighted sum of those outputs to obtain a single output representing the input sentence. Lastly, we add the attention network to decoder cell. This will automatically pipe the outputs of the decoder cell into the attention network, and also pipe the output of the attention network back to the decoder cell, so that the output of the decoder cell can take attention vector into accounts. Now, we can use attention Rob decoder cell to replace the original decoder cell in the predict stop. If we have multiple layer of GRU cells, we typically do the dropouts on output nodes instead of the states. To do so, for each cell, we need to specify the output keep probability in the dropout wrapper before composing the GRU cells sequentially using multi RE cell method.