So, we started with encoder and decoder, which are both recurrent neural networks. Then we added attention, dropout, and etcetera. By that time, the code becomes extremely complex. But we still don't know whether it's going to work properly or not. Just writing LSTM or TRU model and getting it to work, can be very hard. With language, it becomes even harder because we have different sentence length in the natural language. We don't know whether we need to go back for five words or 10 words to find the property contacts. So, it is super helpful that we have a easy to reuse open source library that has already implemented for various tasks. That is why tensor2tensor are laboratory of passing class machine-learning models developed by the Google Brain team, a super exciting. We now have a standard interface that ties together all the pieces needed in a deep learning system. The dataset, the model, the architecture, optimizers, and hyperparameter in a coherent and standardized way that enables us to try many models on the same dataset or apply the same model to many datasets. But we still need to parameterize the tensor2tensor, so that we can apply it to our problem. So, in the rest of the module, we will talk about how we could use this high level library tensor2tensor to solve a sequence to sequence problem. Let's learn the tensor2tensor by going through a lab to complete lines of poetry. Let's see. We want to train a machine learning model to write poetry. Essentially, giving one line of the verse, the model should predict the next line. This is a hard problem. Poetry is probably the highest form of the word play and sophistication. It seems harder than translation, because there is no one-to-one relationship between the input and the output. It is sound was similar to a model that provides answers to questions, except the answers are quite creative because we never repeat. A key concept in the tensor2tensor library, is called problem, which ties together all the pieces needed to train a machine learning model. The easiest way is to inherit from a property base class in the tensor2tensor, and then change only the pieces that are different for our own model. Writing the poetry, is essentially taking text sequence as inputs and producing text sequence as output. This type of problem can be inherit from a base class called text2text problem. The first part of our problem, is the embed step, which defines how we break up the text and represent them as numbers. Here, we specify 4,096 as the vocabulary size. Essentially, tensor2tensor will find and create embeddings from the most frequent 4,096 words in the training dataset. In the prediction step, we will return one of those values to represent a word. The final part of the problem is that we have to define how to generate the actual TF train example files, consisting of the tokens. This is done in the generate samples method, which simply yields a dictionary of the input-output pairs. Recall that the input in our case, is attack sequence and the output is also attack sequence. Once the problem has been defined, we will put it into a Python package called Poetry Trainer, and then invoke a utility called t2t datagen on the input-output pairs. This generates the actual training and evaluation files that will be used for training the language model. The base model that we inherited from text2text problem, is meant for a dataset of 400 million records. But we have only about 22,000 records in this lab. How to customize the model to fit our own dataset. To do so, we will make the model smaller by adjusting the number of hidden layers, the hidden size, and etcetera. We can also add more regularizations using dropout. Regularization helps reduce overfitting, which often is a great concern in smaller datasets. Lastly, getting a lower learning rate could also be a good idea for smaller datasets. Now, we're ready to train the model. We can train the model locally, such as running the training job on Promise, a cloud Datalab instance, with a GPU or on Compute Engine instance with an attached GPU or TPU. We can also train the model on Cloud Machine Learning Engine, to then the t2t trainer job to cloud machine learning engine. All that we have to do, is to add Cloud ML Engine to the parameters of the t2t trainer. Note that, we specified the number of GPUs here. Cloud ML Engine can also acquire GPU or TPU resources for just the duration of the job. When we run the training job and monitoring its tensor boards, we notice that a clear signs of overfitting the training loss, the orange curve here, keep decreasing. But the evaluation loss, the blue curve started going in the wrong direction. Our initial choice of the hyper-parameters, was a wild gas. Next step is to tune those hyperparameters, to hopefully obtain better accuracy. To carry out hyperparameter tuning, we need to register a hyperparameter range. Here, we will like to tune four parameters, the learning rates, the number of hidden layers, the number of nodes in each of the hidden layers, and the dropout probability for the attention network. Having defined those ranges, we added a few more flags to the t2t trainer to submit a hyperparameter tuning job at Cloud Machine Learning Engine. We can specify the hyperparameter ranges that we register and clarified that we wish to maximize the accuracy per sequence. There are other metrics such as perplexity, that are reported by tensor2tensor. But the accuracy per sequence is a better fit for this problem. This job can also be run on TPU.