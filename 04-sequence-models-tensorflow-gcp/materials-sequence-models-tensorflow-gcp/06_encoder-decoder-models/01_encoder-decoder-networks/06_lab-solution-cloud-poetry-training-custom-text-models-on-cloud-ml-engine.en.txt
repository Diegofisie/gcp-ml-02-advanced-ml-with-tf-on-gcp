Congratulations on finishing the lab. Let's quickly go through the lab solutions here. This notebook shows how to train a sequence to sequence model using the Tensor2Tensor library to complete new lines of poetry. To get started, I need to install some necessary packages here. First, I want to see whether the environment has the packages that I need. As shown here, the environment only installed TensorBoard and TensorFlow. So, I need to install Tensor2Tensor and Project Gutenberg that will give me the access to the historical poems. After finishing the pip install, I am going to reset the session, so that the Python environment can pick up the new packages. Now, let me do a quick check again. See, Tensor2Tensor has successfully installed in my environment. I also need to define some environment variables, such as project, bucket, region and problem. The following gcloud command can update the properties in the configurations. Next, I am going to get some poetry anthologies from the Project Gutenberg. Here, I save the data into a text file called raw.txt. Let's quickly do a word count. As we can see here, there are 22,544 lines of poetry in the raw.txt file. The machine learning problem here is to train a model to write poetry giving a starting point. To do so, I will train the model on the odd-numbered lines, such as the first, second, fifth lines, and predict on the even number of lines, such as the second, fourth, sixth lines and so forth. Therefore, the raw data needs to be split into two files, input and output. The input file will consists of the input lines of poetry. The output file will consists of the corresponding output lines or the true labels. Let's take a look how the inputs and output file look like. As we can see here, the first line of the raw file becomes the first line of the input file. The second line of the raw file becomes the first line of the output file. To productionize the model, I don't need to generate those training data files beforehand, instead Tensor2Tensor can create them on the fly. That's what I'm going to do later. In the next field code chunks, I am going to set up the problem in Tensor2Tensor. Here, I defined a new problem called PoetryLineProblem, which is inherited from a base problem called Text2TextProblem. I also define the vocabulary size, dataset splitting ratio, hyper-parameters, and et cetera. Let's take a look what I get here. I have defined a trainer with innate and set up Python scripts. T2t-datagen creates text sequence from the training datasets. Here, I specify the trainer, problem, data directory and a temporary directory. Now, the data is generated as a list of files here. Previously, all generated data is stored locally by using the GSUT commands. I can copy the data to a bucket in the Google Cloud Storage. Then I will authorize the Cloud ML Service account to access the data in the storage buckets. Let me firstly run the model locally on a subset of the data to make sure it works. Great, it worked. Now, I'm ready to train the model on the entire dataset. The training job has completed locally. Except to training the model locally, I can also train it on the Cloud Machine Learning Engine. Tensor2Tensor has a convenient Cloud ML Engine option to kick off the training job. I also increase the training staff here to 7,500. This training job were executed on one GPU. Notice that the echo here is because the t2t-trainer will ask me to confirm before submitting the job to the Cloud. I can also monitor the training progress at GCP Console through this link here. This job took about 20 to 25 minutes and finished successfully. Let's go back to the notebook again. The training job reported some evaluation metrics such as accuracy, accuracy per sequence, approximate Blue score and etc. To evaluate my model, I am more interested in accuracy per sequence. Notice that the accuracy per sequence equals to zero here. That is not too surprising as I asked the neural network to be rather creative. Here shows me a list of the model files that we just created. What if I have more GPUs that allow me to train the model for a longer period of time? Y gets a better accuracy per sequence. Let's try out. I'm not going to ask you the following code in this solution as it takes a few hours to complete. Instead, I have previously trained the model using four GPUs for 75,000 steps. That training job took me about 12 hours and ended up with this matrix. I listed the accuracy per sequence is no longer zero, is now 0.019. This is because I am using a relatively small dataset, only 12,000 lines of poetry. This is tiny in the world of natural language problems. To set the correct expectation, a high-performing translation model needs 400 million lines of inputs and takes one entire day to train on a TPU machine. So, how well my poetry model do when faced with Rumi's spiritual couplets? Let's try out. Here on writing Rumi's couplets into a text file called rumi.txt. Let me print out the odd number lines and see how they look. So, I get where did the handsome bluff go as the first output line? He spread his lights among us like a candle, as the second output line, and so forth. Let's see how my model predicts for those lines. Some of the predictions are still phrases, not complete sentences. This indicate that I might need to train the model for a longer period of time or adjust some hyperparameters. To diagnose the model, I can look at the evaluation metrics on the TensorBoard and see what I can improve in the next iteration. As we can see here, the loss of this model is pretty bad. We can also look at the accuracy per sequence. Similarly, it is not as good as we expected, that is because the TensorBoard shows the evaluation on 7,500 training steps. To stop the TensorBoard, I just need to insert the process ID 8562 into this line here and run. So now, the TensorBoard is stopped Remember, I also ran the model for 75,000 steps on four GPUs. Let's look at the loss curve for that model. It is clear that my model is overfitting. Note that the orange training curve is well below the blue evaluation curve. Both loss curve and accuracy per sequence curve plateaus after 40,000 steps. So, how can I improve my model even further? Well, I need to reduce the overfitting and make sure the evaluation metrics keeps going down as well as loss is also going down. Well, I need to reduce overfitting and make sure the evaluation metrics keep going down as long as the loss is also going down. What I really need to do is to get more data, but if that's not an option here, I could try to reduce the size of the neural network and increase the dropout regorization. I could also do hyperparameter tuning on the dropout and network sizes. Tensor to tensor also supports hyperparameter tuning on Cloud Machine Learning Engine. So, in the next section here, I am going to add some auto-tune flags. While I ran this job it took about 15 hours, and found the trials 37 has the best model performance. It also found the best set of hyperparameters for me. So, as we can see here, the best number of hidden layers is four. The best learning rate is 0.0267. The best hidden size is 512, and the best attention dropout is 0.6058. As we can see here, the accuracy per sequence has increased to 0.027 comparing to 0.019 that I got without hyperparameter tuning. This is 40 percent of improvement. This is only 7,500 steps instead of 75,000 steps. This result is pretty good. Now, let's predict the Rumi's couplets again was this optimized model. Let's look at some example here. The first line of the training is, where did the handsome beloved go? My model predicted the second line as, where art thou worse to me than dead? And the actual second line, the Rumi had is, I wonder where did that tall, shapely cypress tree go? Wow. The predicted couplets are quite decent considering I only trained the model on American Poetry. Rumi of course has a contacts and thread running through his lines while my model doesn't. Now, I have my model ready. How do I serve those predictions? There are two ways to do so. Firstly, I can use Cloud Machine Learning Engine. This is serverless and I don't have to manage any infrastructure. Secondly, I can also use Kuberflow on Google Kubernetes Engine. This uses cluster but will also work on-prem on my own Kubernetes cluster. In either way, I need to export the model first and let TensorFlow serve the model. To serve the models on Cloud ML Engine, I need to set up some configurations. I also need to delete a model version if it has already existed. Updating the G Cloud components is a good practice here. It makes sure the Google Cloud SDKs are up to date. This might take a few minutes for us. All the components are updated. Now is time to deploy our model. My model is deployed. I'm now going to deploy the model on Kuberflows here, but you can try it out yourself by following the instruction in this notebook. What I have deployed in Cloud ML Engine or Kuberflow is only the TensorFlow model. I still need a preprocessing service that could be down by using AppEngine. I need to add it the application yaml file and create the application with AppEngine. I previously had an application built here and feel free to check it out. Inside this app, I typed "I wandered lonely as a cloud", as the first line, and got the prediction shown as the next line of the poetry. As you can see, that is not bad at all. Now, I have my model trained and served.