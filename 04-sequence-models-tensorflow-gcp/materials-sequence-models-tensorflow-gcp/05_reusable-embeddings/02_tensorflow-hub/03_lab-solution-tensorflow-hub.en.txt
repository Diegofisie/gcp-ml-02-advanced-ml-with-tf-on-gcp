Welcome to the lab, where we're going to use TensorFlow Hub to download and use some pretrained embeddings, and then assess the quality of those embeddings. This lab illustrates how to instantiate a TensorFlow Hub module, how to find pre-trained TensorFlow Hub modules for a variety of purposes, how to examine the quality of those embeddings, how one takes the representation of a single word and composes it into be a representation of a sentence, and then finally how you can assess the quality of your word embeddings using a more formal test. In the beginning parts of this code, we install the TensorFlow Hub library. As we talked about in lecture, the first thing you need to know about TensorFlow Hub is this concept of a module. A module is a reusable part of TensorFlow graph, and because it's a part of a TensorFlow graph, in order to actually get some value out of it, in order to see the results of your embeddings, you need to execute it in the context of a session in a graph, but executing is actually really easy. So, here's some sort of pseudo code. In order to download a module and use it, first we create a path to a hub module. We'll figure out what the appropriate value for this is in a second. We pass that to the constructor, so hub.module, we pass in our module url, and now we have this function called embed, and calling it as simply as easy as putting some parenthesis in front and treating it as a function, and the way you pass in is going to vary from module to module. In this case, the intention is that is as easy as passing in a list of words. Once you have the embeddings, of course you need an actual session to collect the values there. So, that's what we do later on. We've created a session, we initialize our variables and then we simply run the embeddings tensor, and that will then print the values. So, the first thing we need to do is explore on the documentation page a little bit and see what hub modules there are. You'll notice that the hub modules are aligned by specific input domains. So, there's an M1 for some for images some protects and some for other modules entirely. In our case we're working with texts, so you can browse this page right here, and you'll notice that it's organized by the different types of models, and so there's a universal sentence encoder and a bunch of others. Within each model you can see different options, in this case the NNLM module, has versions for all these different languages, some that are normalized and some that are not, and some that are 50 dimensions and some that are 128, and of course there's a quality usefulness trade-off here. The more dimensions you have, the more memory your model is going to consume when it's running in the Cloud, but the higher quality those representations might be. Now, to collect that module URL that I told you that, it's as simple as selecting the link address for the particular link that you want to incorporate. So, in this case I've copy that link for English, and I could paste that into this particular embedding, the constructor here, in order to instantiate the module. That's exactly what I've done over here. So, I set the module URL to be the result of copying that link, and then constructed the module right here, and I've asked it to make representation for the word cat, and then I've invoked in the context of a session. You can see that this monster float right here is what the module thinks that cat represents. So that's great, but it's hard to understand anything about a bunch of floats. What you really need to do is start making comparisons between those different embeddings and have a sense that the quality of the representation, and that's what we're going to do in our informal representation. So, the next thing I asked you to do is to come up with three words, such that we assume the words are similar to each other and one of them's a little bit different. Now, in my case I chose those words to be cat, dog and potato. The next thing that we do is, we look at the embeddings for each of those three words. So, the process here is the same as we used before. We're going to have to either construct knew hub module or use the one we've already created, and then invoke it in the context of a session, and that's what I've done here. In order to make things a little easier though, I created a custom function called create embeddings, where a create embeddings accepts a list of words as well as the hub module that's used for the embedding and then invokes in the context of a session. It also then prints the representation of that embedding. It only prints the first few floats because it's exhausting to look at so many on the screen. So, in this case you'll see that the representation for cat begins with these three floats, for dog, with those three, and for potato, with the three in the bottom. The next thing we do is, that we start doing something much more interesting, which is to compare how similar these representations are. To do that, we are going to make use of a heatmap. Your task in this case was to figure out how to determine the correlation between the vector representations for each of these three words. There are many different ways of comparing vectors. In this case, you could have for instance compute the Euclidean distance between them, but what I'd like you to do is to compute what's equivalent to cosine similarity and look at the inner product of these tensors. So, that's what I've done here. I've simply passed in, I called the np.inner function and I passed in embeddings twice. That's because we're going to compute the inner product of all of the different vectors with each other. When you think about it what a heatmap is, it makes sense why we're doing this, because we're going to look at the similarity of things to themselves as well as things to other things. Then I simply use the seaborne heatmap function, with the correlations and some parameters through to make it visually more interesting, and the result is you should get something like this. Of course the most striking part is that everything is identical to itself. That's not so surprising, but the really cool thing is that you'll notice that cat is far more similar to dog because it's much redder here than it is to potato. We got all that, just all we need to do is invoke a TensorFlow Hub module. So, how would you take those representations of individual words and compose them into representations of sentences? It was actually really quite easy. All you have to do in the case of the particular module that we're using is instead of passing in a single word, is to pass an entire sentence. The reason you'll know this is, if you actually take a look at the module, you'll see that it can accept as input a spaced limited set of words, and you'll see here in the input it says, the module takes a batch of sentences in 1-D tensor of strings as input and then furthermore the pre-processing splits by spaces. So, what I've done here is I've come up with a couple different sentences. Cat, the cat sat on the mat. Dog, and the cat sat on the dog, and I've created embeddings for all those, and then I've done just as before I plotted their similarity. The reason I picked these sentences is because, what's interesting about this model is how it takes the representations of the individual words and composes it when you have a bunch of words in a sentence, and the really surprising thing is that you'll notice that cat is actually more similar to dog than it is to the sentence, the cat sat on the mat, and that's strikingly wrong. You'd think that any subject that featuring the cat as the subject. I'm sorry. Any sentence featuring the cat as a subject will be more similar to the idea of cat than cat would be to dog. The reason for this is because if you look inside the TensorFlow Hub module, you'll notice that the way these are composed together is by using a square root combiner. Which basically means we're using a deterministic procedure, independent of the number of things that we're trying to combine to put them together. In other words, it is not all as nuances as the RNNs that we've created in previous modules which are designed to handle these nuanced relationships of information over time. However, they're great as a simple way of combining a bunch of different representations together. All right. So this is all well and good. We've had this informal comparison. We've seen the limitations of this naive method of composing representations together. What if we want to do a more formal evaluation? Well, thankfully we're not the first set of people to wonder about this sort of thing. Computational linguists have been doing this for a long time and they've come up with a number of different benchmarks. In this cases, benchmark is derived from human subjects rating the similarity of different word pairs, and you can assess the degree to which the similarity that human beings think given pair of words have, with the similarities of these two vectors have in our embedding space. In this particular section, we download the Python DataFrame of the sentence pairs, and so you'll see for a given row in this DataFrame we have two sentences, in this case a man with a hard hat is dancing, and a man wearing a hard hat is dancing. That has perfect similarity. What if you have, a young child is riding a horse versus a child is riding a horse? While they have a similarity of 4.75. So it's a little bit lower. The next thing that we do is we build an evaluation graph. So, we retrieve the embeddings, we normalize them and then we compute the cosine similarity is by using a couple of nested Tensorflow functions right here, we clip them and then we set the scores to be one minus the arc cosine of the cosine similarities. Then after we've done all that we can take the Pearson correlation coefficient between the embedding space and the ones that the human subjects generated, in this case it's 0.51, indicating a moderate correlation. The really powerful thing is that you'll notice that the score you get will depend in part upon which module you choose. So, you should always be sure to choose the one with its most suited to your task, and there, of course, are many different modules for you to choose from. Finally, check out this blog post we have about how bias can affect texts embeddings. It's definitely worth a read.