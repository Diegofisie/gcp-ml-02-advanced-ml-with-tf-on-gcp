In the last section, we introduced GloVe and Word2vec, two models for creating word embeddings from corpora, and we said that both of these approaches are expensive to run. Because they're expensive, it's common practice to incorporate pre-trained embeddings. However, incorporating pre-trained embeddings into your model can be an engineering challenge. You need to add the graph for the pre-trained model to your models graph and you need to make sure that it's parameters are initialized properly. Then, depending on whether you want the pre-trained model to be trainable or not, you'll have to treat these as either constants or variables. It gets complicated. Thankfully, there's now a very easy way to add a pre-trained model to your model. In this section, we'll show you how you can use TensorFlow Hub to download and use word embeddings from models trained with state of the art approaches. TensorFlow Hub is a library for the publication discovery and consumption of reusable parts of machine learning models. In TensorFlow Hub, these reusable parts are referred to as modules. A module is a self-contained piece of a TensorFlow graph along with it's weights and assets. Using TensorFlow Hub is really simple. First, you need to install the TensorFlow Hub Python library. Then, to use the module, simply pass it's module URL to a hub constructor. The constructor returns a function and using the module as a simple as passing in the arguments that the model expects. With the models that we'll demonstrate how to use, that's as easy as passing in a list of words or sentences to embed. Because a module is a part of a TensorFlow graph, you'll have to actually run the module in the context of a graph to see the embeddings.