1
00:00:00,000 --> 00:00:03,480
In the last section, we introduced GloVe and Word2vec,

2
00:00:03,480 --> 00:00:06,330
two models for creating word embeddings from corpora,

3
00:00:06,330 --> 00:00:09,405
and we said that both of these approaches are expensive to run.

4
00:00:09,405 --> 00:00:14,150
Because they're expensive, it's common practice to incorporate pre-trained embeddings.

5
00:00:14,150 --> 00:00:16,530
However, incorporating pre-trained embeddings

6
00:00:16,530 --> 00:00:18,930
into your model can be an engineering challenge.

7
00:00:18,930 --> 00:00:21,900
You need to add the graph for the pre-trained model to your models

8
00:00:21,900 --> 00:00:25,705
graph and you need to make sure that it's parameters are initialized properly.

9
00:00:25,705 --> 00:00:29,770
Then, depending on whether you want the pre-trained model to be trainable or not,

10
00:00:29,770 --> 00:00:32,830
you'll have to treat these as either constants or variables.

11
00:00:32,830 --> 00:00:34,440
It gets complicated.

12
00:00:34,440 --> 00:00:38,885
Thankfully, there's now a very easy way to add a pre-trained model to your model.

13
00:00:38,885 --> 00:00:42,110
In this section, we'll show you how you can use TensorFlow Hub to

14
00:00:42,110 --> 00:00:46,665
download and use word embeddings from models trained with state of the art approaches.

15
00:00:46,665 --> 00:00:50,120
TensorFlow Hub is a library for the publication discovery

16
00:00:50,120 --> 00:00:54,205
and consumption of reusable parts of machine learning models.

17
00:00:54,205 --> 00:01:00,100
In TensorFlow Hub, these reusable parts are referred to as modules.

18
00:01:00,100 --> 00:01:02,330
A module is a self-contained piece of

19
00:01:02,330 --> 00:01:05,865
a TensorFlow graph along with it's weights and assets.

20
00:01:05,865 --> 00:01:08,695
Using TensorFlow Hub is really simple.

21
00:01:08,695 --> 00:01:12,060
First, you need to install the TensorFlow Hub Python library.

22
00:01:12,060 --> 00:01:13,855
Then, to use the module,

23
00:01:13,855 --> 00:01:17,405
simply pass it's module URL to a hub constructor.

24
00:01:17,405 --> 00:01:20,780
The constructor returns a function and using the module

25
00:01:20,780 --> 00:01:24,395
as a simple as passing in the arguments that the model expects.

26
00:01:24,395 --> 00:01:26,990
With the models that we'll demonstrate how to use,

27
00:01:26,990 --> 00:01:30,720
that's as easy as passing in a list of words or sentences to embed.

28
00:01:30,720 --> 00:01:33,980
Because a module is a part of a TensorFlow graph,

29
00:01:33,980 --> 00:01:39,100
you'll have to actually run the module in the context of a graph to see the embeddings.