Once you've decided you're sliding window size and concatenated the data in the window, the next step is putting it into your model. The first thing you could try is to use a linear model. In our example, the n columns on the left hand side of our dataset comprise our feature matrix x, and the rightmost column is our label, y. To obtain the weights for our linear model, we could either solve w analytically, or when you have large amounts of data, you could use gradient descent. Let's practice training a linear model on a synthetic dataset. Imagine that you want to predict the height of water being sprayed from an oscillating sprinkler. Oscillating sprinklers consist of a cylindrical metal tube that rotates about its small axis and sprays water through holes drilled down the length of the tube. In our synthetic data set, instead of a fixed rotation speed and a fixed water pressure, we will actually randomly generate both. This is the code that we'll use to generate our data. Note how do we make use of NumPy's random function to randomly generate both the frequency and an amplitude as well as to introduce a small bit of noise. We're doing this because it's representative of what the model would encounter in production for many use cases. For example, in speech transcription, everyone speaks at different cadences and has a slightly different pitch to their voice. Production can introduce all sorts of challenges to modeling sequences like the one that we've recreated in our sprinkler dataset. For example, if you look at the data that Noah has collected on CO2 in the atmosphere, in the years since 1984, you'll notice both an upward trend as well as a cycle that corresponds with the seasons. Using the create_time_series function, we'll then write CSV files to disk. We'll write them to disk such that each line in each file consists of a comma-delimited string. In this example, which columns are features and which are labels? You probably said that everything but the last column is a feature, and that's certainly one way to interpret these results. But, as we'll see later on, it's not the only way. We'll return to this point later in module three. Our input function is very similar to what we've used before. As always, whenever you're building a null model, it's best to have a benchmark to compare your model's performance against. In our code, we've accomplished this by creating a benchmark model that uses the observation from the prior time point as the prediction. To see the performance of this benchmark alongside the models we'll train, we've introduced a new evaluation metric called RMSE same as last. Our linear model looks very similar to what we've seen previously. Know how we're extracting the time series data from our features dictionary using a key that is a constant, and that the dense layer has no non-linear activation function.