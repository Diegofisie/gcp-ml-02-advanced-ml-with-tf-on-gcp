At this point, you've now trained a linear model on our sprinkler data, and surprisingly, it did a pretty good job. It was better than our benchmark model. If this level of performance were sufficient for our business needs, we would consider stopping there and deploying our linear model. But sometimes it's important that models agree with how we think the underlying relationship between the features and the labels works. For example, previously, we use regularization to penalize model weights. In part because of our belief that the true relationship between the features and the labels, was likely to be a simple, and sparse one. In a sequence context, we might also have another constraint. We might believe that more recent observations are more important than less recent ones. How might we force our model to learn ways that meet this constraint? One method of constraining the weights so that more recent observations weigh more heavily, is to use exponential smoothing. Exponential smoothing assumes that the contribution of some prior observation, t minus x, to some future observation at time t is determined by the exponential function, p to the x, where p is a number between 0 and 1. If you graphed p to the x for different values of p, you can see that the bigger P is, the fatter the tail becomes, and the more important less recent observations become. For p equals 0.9, observations ten units of time in the past still contribute about 35% of their original value. Model fitting for such a model consists of estimating the values for two terms: the bias term, and the p term. Another related model is an autoregressive model. An ARMA model in machine learning terms, consists of doing a bit of feature engineering. Instead of doing a straightforward linear regression of the future values of the time series as a weighted sum of the past values, we model the difference between the future values, and the moving average or the exponential moving average. We do this as a linear combination of the past values of the time series. In your head though, you should think of exponential smoothing models, and ARMA models as a special case of linear models. So, in the lab that you're about to do, when you train a linear model, you're essentially considering the class of traditional time-series techniques. As we've seen, oftentimes, the relationship between features, and labels is non-linear. Thankfully, using our sliding window approach, it's very easy to model non-linear relationships. All we need to do is pass are input into a DNN. Our DNN model looks very similar to our linear model, but we have multiple layers, and we use a non-linear activation function. Let's practice implementing a DNN model of our sprinkler dataset, and see if it does any better than our linear model.