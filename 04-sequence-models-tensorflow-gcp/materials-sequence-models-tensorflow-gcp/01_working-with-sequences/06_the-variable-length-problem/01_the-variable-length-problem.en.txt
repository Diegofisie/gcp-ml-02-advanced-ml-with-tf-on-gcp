In the last section in lab, we learned how CNN's can be applied to the sequence domain. Then practiced applying them to our synthetic time series data. But you may have noticed that our CNN didn't improve much on the RMSE that our DNN achieved. To understand why? Think back to our original observations about the limitations of DNNs and linear models in the image domain. These model types are capable of learning the contributions of specific regions and feature space. Thus, they are well-suited to images because in images, objects are often the same under special transformations like rotation. Our solution was to decouple filters from specific locations in the image, and that's what CNNs are. However, powerful this approach may be, it is still limited. To understand why? Consider these two equivalent sounds and what their representations would look like. Here's the first one. Here's the second. Just like in the image domain, where we expect models that understand the domain to be robust against certain transformations like rotation and translation, in the sequence domain, it's important then models be robust to changes in the length of the pattern. That's precisely what we tested in our sprinkler dataset when we randomly set the frequency. Your ear recognizes the fact that they're the same sound, but think about what it would take for a model to recognise this. To the model, one appears to have a very different frequency, just like the orange line here. In addition to being able to handle variable length patterns, another important criterion for sequence models is that they be able to handle variable length inputs and outputs. This is important in a variety of contexts. For example, imagine you're trying to predict customer lifetime value given the previous purchases of that customer. Not all customers will have the same number of purchases. In a natural language models, you might have to ingest or output variable length sequences of words. There are two techniques that we can use to handle variable length sequences, cutting and padding, and bagging. But as we'll see, both of these approaches have their problems. Let's say we're learning to predict the future of price of oil, and we've been told the prices at various points in time. If we reshape this data so that our model accepted four events at a time, our linear model could learn a weight value for every time point. In this case, it might learn these four weights. In production, we might not get all four events though. So, what would happen if you gave our model six, nine and 12? Which has three events instead of the four that the model expects. Well, linear models and DNNs are essentially matrix multiplications. So, it's imperative that the inputs match what is expected. So, you could pad the input to be the right size. If you put the zero at the front, the first weight that the model learn doesn't contribute. If you put the zero at the back, the last weight of the model learn doesn't contribute. Neither of these is a good outcome. Perhaps, you want to avoid padding by simply picking a smaller fixed input size and cutting all sequences down to that size. The smallest size we pick, the less information that the model gets to work with, and ultimately, this might hamper performance. But, if you do the opposite and pick a really a large size, then you have to figure out how to pad shorter sequences at run-time. Consequently, no matter whether we cut or we pad, we still have problems concatenating events together to form a fixed length sequence. Another thing we could do is abandon our concatenation approach entirely, and instead of concatenating time points together, take the average value for each characteristic that we're measuring. So, if we add three-dimensional representations of words, we could say that the representation of a sequence is just the average value for each of those three-dimensions. This approach is known as bagging, and when used in the context of natural language is called the bag of words model. Surprisingly, it's effective, assuming you have a good way of representing every unit of time. We'll come back to this point when we talk about embeddings. But regardless of how good our representations of individual points of time are, this approach still throws away order. It means that the cat sat on the mat is equal to the mat sat on the cat. Order can be extremely important. Fortunately, there are model types that are designed specifically to handle variable length sequences. Let's take a look at recurrent neural networks in the next module.