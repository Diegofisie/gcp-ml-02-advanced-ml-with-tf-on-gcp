1
00:00:00,000 --> 00:00:01,960
In our last course,

2
00:00:01,960 --> 00:00:05,625
we expanded the set of domains we know how to model to include images,

3
00:00:05,625 --> 00:00:07,860
and we learned how CNNs function as

4
00:00:07,860 --> 00:00:10,380
efficient image feature extractors by learning

5
00:00:10,380 --> 00:00:12,960
a set of location independent local filters,

6
00:00:12,960 --> 00:00:15,660
while optimizing for performance on a task,

7
00:00:15,660 --> 00:00:16,680
and in so doing,

8
00:00:16,680 --> 00:00:19,620
our departure from traditional image processing techniques

9
00:00:19,620 --> 00:00:22,520
that emphasized hand-tuning model parameters.

10
00:00:22,520 --> 00:00:25,050
The image domain isn't the only domain,

11
00:00:25,050 --> 00:00:26,400
they both poses a challenge for

12
00:00:26,400 --> 00:00:30,755
traditional machine learning methods and is loaded with potential applications.

13
00:00:30,755 --> 00:00:35,180
Sequences are also such a domain and they too have lots of applications,

14
00:00:35,180 --> 00:00:38,670
like predicting the future supply of commodities or transcribing

15
00:00:38,670 --> 00:00:43,520
audio or translating from one language to another or summarizing text.

16
00:00:43,520 --> 00:00:46,700
In this module, you'll learn what a sequence is,

17
00:00:46,700 --> 00:00:49,225
see how you can prepare a sequence data for modeling,

18
00:00:49,225 --> 00:00:52,040
see some classical approaches to sequence modeling,

19
00:00:52,040 --> 00:00:53,805
and practice applying them.

20
00:00:53,805 --> 00:00:55,665
So, what is a sequence?

21
00:00:55,665 --> 00:00:59,330
Sequences are data points that can be meaningfully ordered such

22
00:00:59,330 --> 00:01:03,345
that earlier observations provide information about later observations.

23
00:01:03,345 --> 00:01:06,980
One way of thinking about this is that sequences should be available

24
00:01:06,980 --> 00:01:10,620
in a form that looks like this table or equivalently this graph,

25
00:01:10,620 --> 00:01:12,390
but that's not all.

26
00:01:12,390 --> 00:01:15,920
You should also be able to take a slice of observations say,

27
00:01:15,920 --> 00:01:18,120
from last Friday until today,

28
00:01:18,120 --> 00:01:20,690
and use these to get a better than chance prediction

29
00:01:20,690 --> 00:01:23,860
of some later observation say, tomorrow.

30
00:01:23,860 --> 00:01:28,180
Let's practice recognizing sequence data with a few quiz questions.

31
00:01:28,180 --> 00:01:30,800
Let's say that you flip a fair coin for

32
00:01:30,800 --> 00:01:33,690
a few hours and collected data that looked like this.

33
00:01:33,690 --> 00:01:36,885
Is this sequence data? Yes or no?

34
00:01:36,885 --> 00:01:41,030
No, and the reason is not sequence data is that observations at

35
00:01:41,030 --> 00:01:45,900
earlier time points don't provide any information about later observations.

36
00:01:45,900 --> 00:01:48,530
Okay, time to put our creative hats on.

37
00:01:48,530 --> 00:01:52,310
What can we do to make coin flipping into a sequence data problem,

38
00:01:52,310 --> 00:01:53,840
or to put it another way,

39
00:01:53,840 --> 00:01:57,910
to make earlier observations provide information about later ones?

40
00:01:57,910 --> 00:02:00,800
One way you could do it would be to bend the coin

41
00:02:00,800 --> 00:02:03,490
after every flip depending on the result.

42
00:02:03,490 --> 00:02:06,230
For example, if after every head value,

43
00:02:06,230 --> 00:02:09,830
we made had slightly more likely by bending our coin,

44
00:02:09,830 --> 00:02:13,315
then instead of oscillating unpredictably between heads and tails,

45
00:02:13,315 --> 00:02:16,470
eventually, the coin would show up heads every time.

46
00:02:16,470 --> 00:02:18,945
What about natural language?

47
00:02:18,945 --> 00:02:24,115
Let's say, you have a corpus of texts that looks like this. Is this a sequence?

48
00:02:24,115 --> 00:02:29,485
Yes, and you can tell because you likely have a good idea of what comes next.

49
00:02:29,485 --> 00:02:32,290
Languages filled with all sorts of dependencies,

50
00:02:32,290 --> 00:02:37,205
some short like how we and eat agree in the sentence we eat,

51
00:02:37,205 --> 00:02:40,030
and some long like in this paragraph.

52
00:02:40,030 --> 00:02:41,865
What about image data?

53
00:02:41,865 --> 00:02:44,435
Can we treat image data as a sequence?

54
00:02:44,435 --> 00:02:48,220
This is a tricky one and the answer is sometimes.

55
00:02:48,220 --> 00:02:52,325
A camera row scan is sometimes considered a sequence.

56
00:02:52,325 --> 00:02:56,935
Images in sequence are still similar in one respect and we'll come back to this later on,

57
00:02:56,935 --> 00:02:59,760
but in the meantime, can you guess what it is?

58
00:02:59,910 --> 00:03:02,195
If you think about movies,

59
00:03:02,195 --> 00:03:05,220
a movie is a sequence of still frames and audio.

60
00:03:05,220 --> 00:03:09,700
Movies are also interesting because they highlight how models can operate in concert.

61
00:03:09,700 --> 00:03:13,970
The architecture for modeling a movie might use a CNN to extract information from

62
00:03:13,970 --> 00:03:15,500
a frame for passing on

63
00:03:15,500 --> 00:03:20,095
the extracted features to a model better suited to modeling the sequence.

64
00:03:20,095 --> 00:03:24,560
Sequences can be the input and the output of machine learning model,

65
00:03:24,560 --> 00:03:29,370
and it's common to think of the type of model as a function of where sequences show up.

66
00:03:29,370 --> 00:03:33,910
Broadly, sequence models fit into three types: one to sequence,

67
00:03:33,910 --> 00:03:36,890
sequence to one, and sequence to sequence.

68
00:03:36,890 --> 00:03:38,795
In a one to sequence model,

69
00:03:38,795 --> 00:03:43,200
one non-input is passed in and the model yields a sequence.

70
00:03:43,200 --> 00:03:45,330
In a sequence to one model,

71
00:03:45,330 --> 00:03:49,010
a sequence is passed in and there is one non-sequence output.

72
00:03:49,010 --> 00:03:51,890
Finally, in a sequence to sequence model,

73
00:03:51,890 --> 00:03:56,205
sequences serve as both the input two and the output of the model.

74
00:03:56,205 --> 00:03:59,350
What sort of model would you use for translation?

75
00:03:59,350 --> 00:04:01,925
One to sequence, sequence to one,

76
00:04:01,925 --> 00:04:05,530
sequence to sequence, or a non-sequence model?

77
00:04:05,860 --> 00:04:10,340
Translation typically uses a sequence to sequence model because

78
00:04:10,340 --> 00:04:14,495
the inputs are a sequence of natural language and so are the outputs.

79
00:04:14,495 --> 00:04:17,035
What about image captioning?

80
00:04:17,035 --> 00:04:21,375
Image captioning can be done as a sequence of sequence too, but usually,

81
00:04:21,375 --> 00:04:23,140
we treat images as one entity,

82
00:04:23,140 --> 00:04:25,245
and use a CNN to extract features,

83
00:04:25,245 --> 00:04:28,750
and then use a sequence model to produce the output.

84
00:04:28,750 --> 00:04:31,070
What about SmartReply?

85
00:04:31,070 --> 00:04:34,940
SmartReply is a model that suggests responses to conversations

86
00:04:34,940 --> 00:04:38,720
with an applications like inbox or messages on Android.

87
00:04:38,720 --> 00:04:42,670
SmartReply is actually a sequence to one model.

88
00:04:42,670 --> 00:04:45,225
SmartReply accepts a sequence as input,

89
00:04:45,225 --> 00:04:48,665
but chooses from a predefined set of responses for it's output,

90
00:04:48,665 --> 00:04:52,170
and you should keep this in mind when you're thinking about framing your problem,

91
00:04:52,170 --> 00:04:56,860
just because your output looks like a sequence doesn't mean that it needs to be.

92
00:04:56,860 --> 00:05:00,430
What about predict the next X models?

93
00:05:00,430 --> 00:05:03,590
Actually, there are many ways of formulating this problem,

94
00:05:03,590 --> 00:05:06,980
some of which treat the data as sequences and some of which don't.

95
00:05:06,980 --> 00:05:09,530
Predicting next thing can be done by feature

96
00:05:09,530 --> 00:05:12,880
engineering the inputs and then ignoring their order.

97
00:05:12,880 --> 00:05:16,310
Recommendation systems, which we'll talk about in a later course,

98
00:05:16,310 --> 00:05:18,620
may or may not involve sequence models,

99
00:05:18,620 --> 00:05:24,480
but if you're predicting the next video in order to preload it, order becomes important.