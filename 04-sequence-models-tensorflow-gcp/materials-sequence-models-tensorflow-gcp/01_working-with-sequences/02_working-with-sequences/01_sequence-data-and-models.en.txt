In our last course, we expanded the set of domains we know how to model to include images, and we learned how CNNs function as efficient image feature extractors by learning a set of location independent local filters, while optimizing for performance on a task, and in so doing, our departure from traditional image processing techniques that emphasized hand-tuning model parameters. The image domain isn't the only domain, they both poses a challenge for traditional machine learning methods and is loaded with potential applications. Sequences are also such a domain and they too have lots of applications, like predicting the future supply of commodities or transcribing audio or translating from one language to another or summarizing text. In this module, you'll learn what a sequence is, see how you can prepare a sequence data for modeling, see some classical approaches to sequence modeling, and practice applying them. So, what is a sequence? Sequences are data points that can be meaningfully ordered such that earlier observations provide information about later observations. One way of thinking about this is that sequences should be available in a form that looks like this table or equivalently this graph, but that's not all. You should also be able to take a slice of observations say, from last Friday until today, and use these to get a better than chance prediction of some later observation say, tomorrow. Let's practice recognizing sequence data with a few quiz questions. Let's say that you flip a fair coin for a few hours and collected data that looked like this. Is this sequence data? Yes or no? No, and the reason is not sequence data is that observations at earlier time points don't provide any information about later observations. Okay, time to put our creative hats on. What can we do to make coin flipping into a sequence data problem, or to put it another way, to make earlier observations provide information about later ones? One way you could do it would be to bend the coin after every flip depending on the result. For example, if after every head value, we made had slightly more likely by bending our coin, then instead of oscillating unpredictably between heads and tails, eventually, the coin would show up heads every time. What about natural language? Let's say, you have a corpus of texts that looks like this. Is this a sequence? Yes, and you can tell because you likely have a good idea of what comes next. Languages filled with all sorts of dependencies, some short like how we and eat agree in the sentence we eat, and some long like in this paragraph. What about image data? Can we treat image data as a sequence? This is a tricky one and the answer is sometimes. A camera row scan is sometimes considered a sequence. Images in sequence are still similar in one respect and we'll come back to this later on, but in the meantime, can you guess what it is? If you think about movies, a movie is a sequence of still frames and audio. Movies are also interesting because they highlight how models can operate in concert. The architecture for modeling a movie might use a CNN to extract information from a frame for passing on the extracted features to a model better suited to modeling the sequence. Sequences can be the input and the output of machine learning model, and it's common to think of the type of model as a function of where sequences show up. Broadly, sequence models fit into three types: one to sequence, sequence to one, and sequence to sequence. In a one to sequence model, one non-input is passed in and the model yields a sequence. In a sequence to one model, a sequence is passed in and there is one non-sequence output. Finally, in a sequence to sequence model, sequences serve as both the input two and the output of the model. What sort of model would you use for translation? One to sequence, sequence to one, sequence to sequence, or a non-sequence model? Translation typically uses a sequence to sequence model because the inputs are a sequence of natural language and so are the outputs. What about image captioning? Image captioning can be done as a sequence of sequence too, but usually, we treat images as one entity, and use a CNN to extract features, and then use a sequence model to produce the output. What about SmartReply? SmartReply is a model that suggests responses to conversations with an applications like inbox or messages on Android. SmartReply is actually a sequence to one model. SmartReply accepts a sequence as input, but chooses from a predefined set of responses for it's output, and you should keep this in mind when you're thinking about framing your problem, just because your output looks like a sequence doesn't mean that it needs to be. What about predict the next X models? Actually, there are many ways of formulating this problem, some of which treat the data as sequences and some of which don't. Predicting next thing can be done by feature engineering the inputs and then ignoring their order. Recommendation systems, which we'll talk about in a later course, may or may not involve sequence models, but if you're predicting the next video in order to preload it, order becomes important.