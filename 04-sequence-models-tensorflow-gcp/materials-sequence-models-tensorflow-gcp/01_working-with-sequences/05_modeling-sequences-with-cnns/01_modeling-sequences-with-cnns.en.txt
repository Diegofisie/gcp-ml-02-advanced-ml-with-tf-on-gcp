In the last section, we saw how our DNN was able to model time-series data, and I told you that we could still improve upon its performance. In this section, we'll try to do just that. But in order to do so, we actually need to go backwards a bit. Earlier, I asked you to start thinking about what images and sequences have in common. Did you come up with anything? Think again about exponential smoothing models and what inspired us to constrain the weight values. The intuition was that more recent observations played a greater role in determining what happens at time t than less recent observations. To put it another way, just as locality played a role in the image domain, where pixels in the neighborhood of a given pixel were much more likely to be related, so too does locality play a role in sequence modeling. To see what I mean, consider how convolution could work in one dimension. Let's say that we have this sequence, and let's say that our task is to detect anomalies like the one that occurs at t equals 4. Which convolution filters would be highly active at t equals 4? To figure out how active a filter is, slide it across the sequence and compute the product of the filter times the elements within the current window and then add the result. The answer is the 33, negative 67, 33 filter, and the 1.2, 1.0, negative 0.75, 1.0 filters. Here's the result of applying a filter to the raw sequence data. Note how it spikes when the raw data matches the pattern in the filter. Now, of those that were active, which is more specific? The filter that begins with 1.2 is. The reason this filter is more specific is because it's longer. To understand how length is analogous to specificity, think back to the last course on image models. In the last course, we saw how CNNs learn a hierarchy of features that grow more specific as we progress through the network. Whereas earlier layers will be most active for simple highly local patterns, later layers would respond to more complicated things like parts of faces. If you think about these more complicated patterns, they were also bigger. The same thing applies in this context. So, what if instead of a single drop anomaly detector, we wanted to detect when there were two drops in quick succession? One approach we could take is to use a longer filter. Another, is to use additional layers. To apply convolution in one dimension requires five steps. First, flatten the input sequence. Then, use conv1d to apply a number of filters to the sequence. Then, use max_pooling to add some spatial invariance and down scaling. Then, flatten the resulting output into a sequence, and then send it through a fully connected layer with the appropriate output node. Let's see how we accomplish that encode. First, we reshape the inputs to be batch size by n inputs by 1. This is because the conv1d function requires a 3D tensor. For the number of filters, I chose to use half the number of inputs. We want to avoid overfitting by introducing too many filters. But as you know, the number of filters is something you could hyperparameterize. We then pass the convolved tensor through a max pooling layer to add some spatial invariance and to reduce the size of the tensor. Finally, we remove the inner dimension we added to accommodate the conv1d function and pass the tensor through a dense layer and a final linear prediction layer. Now, it's your turn. Let's apply a CNN to time-series data and see how it does.