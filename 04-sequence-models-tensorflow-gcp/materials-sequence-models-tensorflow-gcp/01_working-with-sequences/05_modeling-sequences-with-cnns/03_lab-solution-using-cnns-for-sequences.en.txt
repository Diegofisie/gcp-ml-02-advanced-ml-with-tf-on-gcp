Okay, welcome to the third lab where we're going to use our CNN model to model the same synthetic dataset that we've been looking at so far. In the previous labs, we've been using some synthetic data that was generated from randomly varying some parameters inside some sine waves. So if you recall, we have a great time series function that randomly generates a frequency, and amplitude and some noise, and as a result, you're going to have data that looks like this, where each sine wave consists of 50 observations with a randomly generated frequency, randomly generated amplitude, and a little bit of noise injected there as well. Then after that, what we've done is modified the model.py file in order to train models locally using G Cloud ML Engine local train. In order to ensure that the proper model function executes within our code, we've been varying this model parameter inside this code block. We started with a set of linear in order to train linear models and we changed it later on DNN. In this case, what we're going to do is we're going to after we make modifications to the model.py file, we'll change this to CNN in order to run it. Let's take a look now inside model.py. As we've done in previous labs, we're going to take a model function that we get features as input and we're going to need to map those features to our predictions. Now, the CNN models a little bit different from the other ones. We should have ultimately something that looks like this. The reason the CNN models a little bit different is that we have to reshape our inputs initially. So instead of simply using a dense layer here, we need to add this third dimension, because the convolutional function is expected. So, we take our features dictionary, we retrieve our data, and then we reshape it to be batch size by N inputs by one. After that, we can pass x to the convId one d function, and just as we've seen in the previous course, we have to set the number of filters, the size of those filters, the set of the stride value, the padding, and the activation function for this kernel one d function. In this case, rather than piper parameterizing the number of filters, and the size of those filters and so forth, I've just set some constants here that are based on some general intuitions about network structure. But if you wanted to code up a model for real, this is where you'd pass in your hyperparameters. After we've called the kernel one d function, we pass it into the max pooling function which we know add some spatial invariance as well as reduces the size of the intermediate results, and then we repeat this whole process one more time. We pass the results of the max pooling function to another convolutional layer with the same parameters, same filter is equal to the number of inputs divided by two, same kernel size etc, and then finally call max pooling one more time. Then after having done this, we then reshape that final layer's outputs in order to pass it through a dense layer, and then finally pass that to a predictions layer which is the same thing as a dense layer except as no non-linear activation. After we've done that, we should be able to run this code block and train a model locally. If you've got a model that's trends locally, then you're ready to trade in the Cloud on much more data, and if you modify this final block over here so that the iteration includes not just linear DNN and CNN, then you will initiate three different cloud ML Engine training jobs for each of those model types. Here are the results that I got when I train my model, and you'll notice that the CNN model didn't seem to perform that much better, and in fact in my case they performed a little worse on the DNN we trained previously. Why do you think that's the case? Stay tuned for our lecture we will talk about this more.