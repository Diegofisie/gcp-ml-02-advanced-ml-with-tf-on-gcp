1
00:00:00,000 --> 00:00:05,150
In this section, we will review what RNNs can and can't capture from their inputs.

2
00:00:05,150 --> 00:00:09,570
One of the ways we can understand what RNNs are and are not capable of learning,

3
00:00:09,570 --> 00:00:11,670
is to train a sequence to sequence model and

4
00:00:11,670 --> 00:00:14,675
ask it to make predictions given some random input.

5
00:00:14,675 --> 00:00:17,580
Ordinarily, examining the predictions of the model

6
00:00:17,580 --> 00:00:20,385
for a random point in feature space wouldn't be informative.

7
00:00:20,385 --> 00:00:23,520
For example, if we look at this random point in feature space,

8
00:00:23,520 --> 00:00:25,530
the model's output here really doesn't tell us

9
00:00:25,530 --> 00:00:28,345
about what this classification model learned.

10
00:00:28,345 --> 00:00:32,760
However, with one to sequence and sequence to sequence RNNs,

11
00:00:32,760 --> 00:00:36,215
the output is much more nuance than simply a class or a label.

12
00:00:36,215 --> 00:00:37,950
It's an entire sequence.

13
00:00:37,950 --> 00:00:42,380
If you take such a model and train it to predict the next word in the sequence,

14
00:00:42,380 --> 00:00:44,225
then you have a mapping between its inputs

15
00:00:44,225 --> 00:00:46,850
and the domain from which its training data came from.

16
00:00:46,850 --> 00:00:50,570
That means that you can assess its output for random input based on how

17
00:00:50,570 --> 00:00:54,515
likely it is to be found within the corpus on which the model was trained.

18
00:00:54,515 --> 00:00:56,870
The better the model understands the domain,

19
00:00:56,870 --> 00:00:59,915
the more plausible it's phantom predictions will be.

20
00:00:59,915 --> 00:01:02,235
As we review these examples,

21
00:01:02,235 --> 00:01:06,725
imagine that you had the same job as the model does to create a sample from this domain.

22
00:01:06,725 --> 00:01:08,590
Instead of using machine learning though,

23
00:01:08,590 --> 00:01:10,405
you'd use traditional programming.

24
00:01:10,405 --> 00:01:12,370
How would you write such a program?

25
00:01:12,370 --> 00:01:14,810
What sort of state would it have?

26
00:01:14,810 --> 00:01:19,075
For our examples, we'll use a character sequence sequence model.

27
00:01:19,075 --> 00:01:22,525
Unlike language models, which accepts sequences of words,

28
00:01:22,525 --> 00:01:25,985
character models accept sequences of characters.

29
00:01:25,985 --> 00:01:29,210
We use two very different domains to train our models,

30
00:01:29,210 --> 00:01:32,015
each with a set of rules that varies in its complexity.

31
00:01:32,015 --> 00:01:35,100
The first domain is the complete works of William Shakespeare,

32
00:01:35,100 --> 00:01:37,820
and there are lots of different rules in Shakespeare.

33
00:01:37,820 --> 00:01:40,459
There are rules that govern all natural language,

34
00:01:40,459 --> 00:01:43,850
which are so complicated that linguists still haven't figured them all out.

35
00:01:43,850 --> 00:01:46,365
They do agree on some very basic ones though,

36
00:01:46,365 --> 00:01:48,410
like subject verb agreement.

37
00:01:48,410 --> 00:01:51,100
There are also rules that are more drama specific.

38
00:01:51,100 --> 00:01:53,780
For example, all plays have titles and consists of

39
00:01:53,780 --> 00:01:57,035
a certain number of acts that are labeled in ascending order.

40
00:01:57,035 --> 00:02:00,630
Within a scene, actors enter and do things.

41
00:02:00,630 --> 00:02:03,080
Now, think about your program and its state,

42
00:02:03,080 --> 00:02:08,630
and remember that all we have to work with for state is a fixed size vector of floats.

43
00:02:08,630 --> 00:02:12,320
Perhaps your program would have some sort of nested loops for

44
00:02:12,320 --> 00:02:16,475
the acts and scenes and loops require maintaining some sort of counter.

45
00:02:16,475 --> 00:02:19,640
To generate a scene, we'd need to know who's there.

46
00:02:19,640 --> 00:02:23,255
So, maybe we need a set of all English names to sample from.

47
00:02:23,255 --> 00:02:26,200
Then perhaps within a generate scene function,

48
00:02:26,200 --> 00:02:28,760
we did know which characters are actually in

49
00:02:28,760 --> 00:02:32,015
the current scene because characters enter and leave all the time.

50
00:02:32,015 --> 00:02:34,030
So, perhaps we could maintain a set that we

51
00:02:34,030 --> 00:02:37,275
update with the characters currently on stage.

52
00:02:37,275 --> 00:02:40,130
We then need to give the characters things to say.

53
00:02:40,130 --> 00:02:42,830
So, perhaps we'd use a Markov model compiled from

54
00:02:42,830 --> 00:02:48,150
word co-occurrence statistics although that requires a very large table of numbers.

55
00:02:48,150 --> 00:02:50,660
So, now let's summon our imaginations

56
00:02:50,660 --> 00:02:53,510
a bit and think about this program that we've been writing.

57
00:02:53,510 --> 00:02:57,165
Which parts of its state and code are more complicated than others?

58
00:02:57,165 --> 00:02:59,460
Control structures seem pretty simple.

59
00:02:59,460 --> 00:03:01,525
That's just a few counters.

60
00:03:01,525 --> 00:03:07,895
A set, like the set of characters is more expensive because it could have unlimited size.

61
00:03:07,895 --> 00:03:10,550
Remembering which characters have died already,

62
00:03:10,550 --> 00:03:15,270
so we don't inadvertently add goes to our play, that's really complicated.

63
00:03:15,930 --> 00:03:19,120
Let's look at what the model is able to produce.

64
00:03:19,120 --> 00:03:21,790
The model generated a plausible title,

65
00:03:21,790 --> 00:03:24,265
and even knows that after titles come acts.

66
00:03:24,265 --> 00:03:26,620
But then you see the first mistake,

67
00:03:26,620 --> 00:03:28,600
the scene numbering is wrong.

68
00:03:28,600 --> 00:03:31,570
Our naive program use counters to loop,

69
00:03:31,570 --> 00:03:33,700
but RNNs don't actually have counters.

70
00:03:33,700 --> 00:03:37,550
It did remember to put a number for act and seen though.

71
00:03:37,590 --> 00:03:40,680
After the scene which has a plausible location,

72
00:03:40,680 --> 00:03:45,190
two characters enter, and note how the stage directions are correctly bracketed.

73
00:03:45,190 --> 00:03:50,100
The state for remembering to close brackets could be as simple as a bit or a counter.

74
00:03:50,100 --> 00:03:54,040
Now, I'm no Shakespearian scholar but the dialog here seems pretty reasonable,

75
00:03:54,040 --> 00:03:57,950
and that's notable because our naive method of generating text was to compile

76
00:03:57,950 --> 00:04:02,370
co-occurrence frequency statistics in a table that was far bigger than our memory.

77
00:04:02,370 --> 00:04:06,720
This is even more impressive given that this is actually a character RNN,

78
00:04:06,720 --> 00:04:08,235
not a word RNN,

79
00:04:08,235 --> 00:04:10,910
which means it doesn't even have direct access to words.

80
00:04:10,910 --> 00:04:13,895
But there's still a problem with the output.

81
00:04:13,895 --> 00:04:18,865
The characters in the place who speak never actually entered the scene.

82
00:04:18,865 --> 00:04:23,185
We see a similar story when we train the model on the TensorFlow library,

83
00:04:23,185 --> 00:04:25,045
which is written in Python.

84
00:04:25,045 --> 00:04:28,415
The models capable of remarkable amounts of memorization.

85
00:04:28,415 --> 00:04:30,365
It learn the entire Apache license,

86
00:04:30,365 --> 00:04:32,690
though keep in mind there was overwhelming evidence

87
00:04:32,690 --> 00:04:35,610
for this because it's at the top of every file.

88
00:04:35,610 --> 00:04:40,855
The model learn to correctly triply nest the NumPy arrays, and make indentations,

89
00:04:40,855 --> 00:04:43,310
which like the stage directions might be some sort of

90
00:04:43,310 --> 00:04:46,045
counter or Boolean in our naive implementation.

91
00:04:46,045 --> 00:04:47,720
But much like with Shakespeare,

92
00:04:47,720 --> 00:04:52,365
the model doesn't learn that some things require a much more complicated representations.

93
00:04:52,365 --> 00:04:55,045
For example, it doesn't understand variable scope,

94
00:04:55,045 --> 00:04:57,530
and uses variables that haven't yet been declared,

95
00:04:57,530 --> 00:04:59,900
which makes sense because variable scope requires

96
00:04:59,900 --> 00:05:04,160
a much more complicated bit of state in your computer to correctly maintain.