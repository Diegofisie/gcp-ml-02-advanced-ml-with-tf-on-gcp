1
00:00:00,000 --> 00:00:03,070
In the previous section, we introduced RNNs,

2
00:00:03,070 --> 00:00:06,780
a new type of model that accepts sequences one event at a time and

3
00:00:06,780 --> 00:00:10,995
develops representations of what it seen previously as it scans its input.

4
00:00:10,995 --> 00:00:13,560
But we haven't talked about how RNNs are able to

5
00:00:13,560 --> 00:00:16,440
create such powerful representations of the past.

6
00:00:16,440 --> 00:00:18,960
The reason that RNNs are able to remember what they've

7
00:00:18,960 --> 00:00:21,089
seen previously is because firstly,

8
00:00:21,089 --> 00:00:24,040
they have a recurrent connection between their hidden layer and their input,

9
00:00:24,040 --> 00:00:27,670
and secondly, because they use a clever trick during optimization.

10
00:00:27,670 --> 00:00:30,690
First, let's talk about that recurrent connection.

11
00:00:30,690 --> 00:00:33,230
The recurrent connection is depicted here in red,

12
00:00:33,230 --> 00:00:36,530
and it connects the hidden layer to the fixed size state.

13
00:00:36,530 --> 00:00:40,710
Note how the hidden layer has the same number of units as the fixed sized state.

14
00:00:40,710 --> 00:00:44,120
Because the recurrent connections simply copies the values in

15
00:00:44,120 --> 00:00:47,180
the hidden layer to the fixed sized state for the next iteration,

16
00:00:47,180 --> 00:00:50,380
the two must have the exact same number of dimensions.

17
00:00:50,380 --> 00:00:55,415
It's not obvious how recurrent connection helps an RNN remember what it seen previously.

18
00:00:55,415 --> 00:00:57,130
To understand how that helps,

19
00:00:57,130 --> 00:00:59,315
we need to look at the optimization procedure.

20
00:00:59,315 --> 00:01:02,085
To do that, we need to zoom out a bit.

21
00:01:02,085 --> 00:01:06,020
This is what the RNN model looks like as it scans the sentence.

22
00:01:06,020 --> 00:01:07,575
In the green rectangle,

23
00:01:07,575 --> 00:01:09,050
I've depicted the RNN cell.

24
00:01:09,050 --> 00:01:10,395
In then the orange circle,

25
00:01:10,395 --> 00:01:12,485
what might come after it in the network.

26
00:01:12,485 --> 00:01:17,115
Note how we can capture the operations we've talked about earlier mathematically.

27
00:01:17,115 --> 00:01:19,700
The input is represented by X of t,

28
00:01:19,700 --> 00:01:22,610
and it is concatenated with H of t minus one,

29
00:01:22,610 --> 00:01:25,750
which is the output of the hidden layer from the previous iteration.

30
00:01:25,750 --> 00:01:30,500
This new term is added to the bias term before being passed to the activation function,

31
00:01:30,500 --> 00:01:32,890
which traditionally was a tanh.

32
00:01:33,280 --> 00:01:38,430
The RNN cell feeds directly into a dense layer followed by a linear layer.

33
00:01:38,430 --> 00:01:42,100
Note that the weights and biases for all three of these layers,

34
00:01:42,100 --> 00:01:46,140
tanh, dense, and linear, are all different.

35
00:01:46,170 --> 00:01:50,335
For example, let's say that our current input is our representation of wag,

36
00:01:50,335 --> 00:01:52,325
which is the three component vector.

37
00:01:52,325 --> 00:01:56,175
We can concatenate this vector with the hidden layer from the previous iteration,

38
00:01:56,175 --> 00:01:57,890
compute the value of the hidden layer,

39
00:01:57,890 --> 00:01:59,735
which would require multiplying by the weights,

40
00:01:59,735 --> 00:02:01,265
and then adding a bias term,

41
00:02:01,265 --> 00:02:04,070
and finally passing the result through the tanh function.

42
00:02:04,070 --> 00:02:06,760
Then we would make a prediction for this time step,

43
00:02:06,760 --> 00:02:10,400
which we represent as y of t. Hopefully,

44
00:02:10,400 --> 00:02:13,640
the predicted output will be close to our representation of their,

45
00:02:13,640 --> 00:02:16,240
which is the next word in our sequence.

46
00:02:16,240 --> 00:02:19,710
Finally, we'd pass the contents of our hidden layer,

47
00:02:19,710 --> 00:02:22,385
H of t, to the next iteration.

48
00:02:22,385 --> 00:02:25,880
The forward propagation then looks like this diagram.

49
00:02:25,880 --> 00:02:28,220
I've denoted time on the vertical axis to

50
00:02:28,220 --> 00:02:31,140
capture the different events in the sequence being fed into the model.

51
00:02:31,140 --> 00:02:34,100
I've depicted iterations on the horizontal axis

52
00:02:34,100 --> 00:02:37,495
to capture which inputs the model accepts, at which iteration.

53
00:02:37,495 --> 00:02:43,425
As before, the model passes an H to the next iteration via the recurrent connection.

54
00:02:43,425 --> 00:02:47,450
The reason I use the same symbols to depict the model at every iteration,

55
00:02:47,450 --> 00:02:50,845
is because even though we're using the model multiple times,

56
00:02:50,845 --> 00:02:52,820
once for each event in our sequence,

57
00:02:52,820 --> 00:02:55,775
the model still has only one set of parameters.

58
00:02:55,775 --> 00:02:58,525
In other words, there's only one linear set of weights,

59
00:02:58,525 --> 00:02:59,780
one dense set of weights,

60
00:02:59,780 --> 00:03:02,140
and one tanh set of weights.

61
00:03:02,140 --> 00:03:04,900
The model uses these at both training time,

62
00:03:04,900 --> 00:03:09,595
and inference time, which means that each iteration is a blocking operation.

63
00:03:09,595 --> 00:03:13,770
You might be wondering what this means for our normal optimization procedure.

64
00:03:13,770 --> 00:03:17,990
Normally, during backpropagation, we could compute the loss of the final layer and

65
00:03:17,990 --> 00:03:20,000
use this value to compute the partial derivative

66
00:03:20,000 --> 00:03:22,050
of the loss with respect to every parameter,

67
00:03:22,050 --> 00:03:23,575
p^i, in our model,

68
00:03:23,575 --> 00:03:25,820
and then use these partial derivatives to update

69
00:03:25,820 --> 00:03:28,900
our parameters with a formula that looks like this one.

70
00:03:28,900 --> 00:03:32,865
However, for RNNs, that leads us to the following challenge.

71
00:03:32,865 --> 00:03:35,995
We have more partial derivatives than we do updates.

72
00:03:35,995 --> 00:03:40,630
In this case, we would have four partial derivatives for every parameter.

73
00:03:40,630 --> 00:03:43,970
This is where that clever bit of optimization comes in.

74
00:03:43,970 --> 00:03:47,090
We addressed this problem by updating each parameter using

75
00:03:47,090 --> 00:03:50,485
the average of all the partial derivatives from our iterations.

76
00:03:50,485 --> 00:03:53,685
This approach is called backpropagation through time.

77
00:03:53,685 --> 00:03:56,030
Because each parameter is updated with

78
00:03:56,030 --> 00:03:58,795
the combination of losses from all four iterations,

79
00:03:58,795 --> 00:04:00,800
the model as a whole has pressure to preserve

80
00:04:00,800 --> 00:04:03,875
information that's useful in the short-term and the long-term,

81
00:04:03,875 --> 00:04:05,875
and throw away what isn't.

82
00:04:05,875 --> 00:04:09,490
Remarkably, these two tweaks of our DNN architecture,

83
00:04:09,490 --> 00:04:11,120
the introduction of a recurrent connection,

84
00:04:11,120 --> 00:04:12,840
and some clever bit of optimization,

85
00:04:12,840 --> 00:04:16,505
make RNNs extremely powerful sequence feature extractors.

86
00:04:16,505 --> 00:04:18,815
However, they still have their limitations.

87
00:04:18,815 --> 00:04:21,360
In the next section, we'll review what those are.