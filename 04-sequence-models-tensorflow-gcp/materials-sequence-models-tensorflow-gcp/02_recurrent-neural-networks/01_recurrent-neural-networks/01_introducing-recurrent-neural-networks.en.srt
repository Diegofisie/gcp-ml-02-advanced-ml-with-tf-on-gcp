1
00:00:00,036 --> 00:00:02,000
As we discussed in the last module,

2
00:00:02,000 --> 00:00:06,059
variable length sequences are common
placed in many sequence use cases,

3
00:00:06,059 --> 00:00:10,452
but cannot be elegantly handled by the
types of models we looked at previously.

4
00:00:10,452 --> 00:00:14,048
Our options, which include cutting and
padding as well as bagging,

5
00:00:14,048 --> 00:00:17,035
either throw away information or
else are inefficient.

6
00:00:17,035 --> 00:00:21,400
What we need is a model that is designed
to handle variable-length sequences.

7
00:00:21,400 --> 00:00:25,503
Thankfully such a model exists and
it's called a recurrent neural net.

8
00:00:25,503 --> 00:00:28,650
In this module,
we'll introduce recurrent neural networks,

9
00:00:28,650 --> 00:00:31,864
explain how they address the variable
length sequence problem.

10
00:00:31,864 --> 00:00:36,551
Explain how our traditional optimization
procedure applies RNNs, and

11
00:00:36,551 --> 00:00:40,256
review the limits of what RNNs can and
can't represent.

12
00:00:40,256 --> 00:00:44,148
Recurrent neural networks, or
RNNs, handle variable-length

13
00:00:44,148 --> 00:00:48,960
sequences by recasting the problem of
representing an entire variable-length

14
00:00:48,960 --> 00:00:53,080
sequence to representing a single
event given what has come before.

15
00:00:53,080 --> 00:00:56,199
To gain some intuition for
why this is not just reasonable but

16
00:00:56,199 --> 00:01:00,251
actually consistent with what you do
every day, let's take a little detour.

17
00:01:00,251 --> 00:01:03,727
Think about when someone's speaking
to you and gets interrupted.

18
00:01:03,727 --> 00:01:07,009
For example, if I were to say,
dogs are my favorite animals,

19
00:01:07,009 --> 00:01:08,398
I love how they wag their,

20
00:01:08,398 --> 00:01:12,840
and then I stopped, you'd probably have a
good idea of what I was going to say next.

21
00:01:14,456 --> 00:01:18,322
The fact that you do, that you aren't
incapacitated by an incomplete sentence,

22
00:01:18,322 --> 00:01:21,863
suggests that you've built up
a representation of what I've said so far.

23
00:01:21,863 --> 00:01:22,913
And in this case,

24
00:01:22,913 --> 00:01:26,773
you can use that representation
to predict what might come next.

25
00:01:26,773 --> 00:01:29,741
RNNs are designed to
function in a similar way.

26
00:01:29,741 --> 00:01:34,655
Instead of accepting a fixed size input
representing an entire sequence like DNNs

27
00:01:34,655 --> 00:01:38,999
do, they accept a fixed size
representation of a particular event along

28
00:01:38,999 --> 00:01:43,150
with a fixed size representation
of what they've seen previously.

29
00:01:43,150 --> 00:01:47,838
By a particular event, I'm referring
to a day, in a sequence of days, or

30
00:01:47,838 --> 00:01:49,741
a word in a sequence of words.

31
00:01:49,741 --> 00:01:54,837
So for example, in order to pass in
the sequence, dogs wag their to a DNN,

32
00:01:54,837 --> 00:01:59,856
we'd probably either concatenate our
representations for each word or

33
00:01:59,856 --> 00:02:04,071
else bag those representations
to get the fixed size input.

34
00:02:04,071 --> 00:02:07,901
Of course, saying that we have
a representation of the important parts of

35
00:02:07,901 --> 00:02:10,990
what we've seen previously,
which is what RNNs require,

36
00:02:10,990 --> 00:02:14,424
trivializes the most interesting and
complicated part of RNNs.

37
00:02:14,424 --> 00:02:18,808
Because if we knew how to represent what
we've seen previously regardless of how

38
00:02:18,808 --> 00:02:22,297
long it was, we wouldn't have
this problem in the first place.

39
00:02:22,297 --> 00:02:25,797
We could try to tackle this
problem using feature engineering.

40
00:02:25,797 --> 00:02:28,400
But in deep learning,
we have another option,

41
00:02:28,400 --> 00:02:32,119
letting the model engineer its
own features during optimization.

42
00:02:32,119 --> 00:02:35,096
And that's exactly what RNNs do.

43
00:02:35,096 --> 00:02:38,773
In that sense, much the way that
CNNs learn filters as they train and

44
00:02:38,773 --> 00:02:41,095
can evolve those filters across images and

45
00:02:41,095 --> 00:02:44,340
ultimately learn how to be
good feature image extractors.

46
00:02:44,340 --> 00:02:46,996
RNNs scan their input
layer across sequences and

47
00:02:46,996 --> 00:02:50,815
learn how to extract information from
a given event in order to make use

48
00:02:50,815 --> 00:02:52,900
of it at a later point in the sequence.

49
00:02:52,900 --> 00:02:56,922
And ultimately become good
sequence feature extractors.

50
00:02:56,922 --> 00:02:59,830
There are two key ideas
in the RNN architecture.

51
00:02:59,830 --> 00:03:02,444
First, RNNs learn to
predict the output but

52
00:03:02,444 --> 00:03:05,918
they also learn how to compact
all of the previous inputs.

53
00:03:05,918 --> 00:03:09,675
And secondly,
the input to an RNN is a concatenation of

54
00:03:09,675 --> 00:03:13,610
the original stateless input and
this new hidden state.

55
00:03:13,610 --> 00:03:18,090
This idea of a persistent hidden state
that is learned from ordered inputs is

56
00:03:18,090 --> 00:03:21,946
what distinguishes an RNN from linear and
deep neural networks.

57
00:03:21,946 --> 00:03:25,195
In a DNN, the hidden state is
not updated during prediction.

58
00:03:25,195 --> 00:03:27,671
In RNN, it is.

59
00:03:27,671 --> 00:03:31,205
In the next section we'll talk about
how these aspects of RNNs allow them to

60
00:03:31,205 --> 00:03:33,236
remember what they've seen previously.