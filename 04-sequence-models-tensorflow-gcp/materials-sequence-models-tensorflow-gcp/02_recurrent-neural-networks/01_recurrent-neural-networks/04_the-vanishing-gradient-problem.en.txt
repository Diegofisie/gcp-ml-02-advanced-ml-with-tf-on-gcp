Despite the limits of their representational power, it would be easy to conclude that the RNNs we've seen so far are the perfect tools for modelling variable length input, but in fact there are a few major caveats. Firstly, RNNs are sequence models and so, are only appropriate when earlier observations provide an information about later observations. Secondly, it turns out that the RNNs we've seen still struggle on some common sequence modeling tasks. For example, consider this passage, as a human, it's easy to refer back earlier in the paragraph to infer the language at the end. But think about how much state and how many iteration's the model will have to go through in order to capture this. This sort of relationship is called a long-term dependency. In order to model longer passages, it's critical that our model be able to handle them. Unfortunately, the RNNs we've seen cannot do this, and it's because of a problem we discussed earlier in this course called the vanishing or exploding gradient problem. However, with RNNs, the problem is slightly more nuanced. Because of the way the hidden state from the previous iteration is carried along, the weights in an RNN are particularly susceptible to become uncorrelated. Uncorrelated weights reduce the power of the model. In the extreme if weights were perfectly correlated, it would be as if the model had a single parameter. Correlated weights are also problematic because they're more likely to explode or vanish. In the first specialization, we introduced a method for mitigating the vanishing gradient problem which was to use the ReLU activation function, and this technique can't still help but doesn't get us all the way there. Other techniques have been developed as well. For example, weight initialization like he- or xavier initializations, L1 or L2 weight regularization to control the size of the weights, gradient clipping by monitoring the norm of the gradient and taking the max against some pre-determined bound. But for RNNs, the major advance was actually architectural. This new architecture is what we'll talk about in the next module.