Congratulations. You've made it to the end of the course. Let's recap what we've covered so far. We started by learning about what makes something a sequence. We also introduce some classical approaches to sequence modeling. You saw some of the common business applications for using ML on sequences, like predicting the future supply of commodities for coffee beans and time series data. You also learned the different types of sequence models, like one-to-sequence, sequence-to-one and sequence-to-sequence. Document summarization for example is a sequence-to- sequence model. Then you saw the similarities between image data, and sequence data when it comes to locality. A more recent events may be more important than less recent ones. We then introduce recurrent neural networks and explained how they maintain state. Recall the two key ideas in RNN architecture. RNNs learn to predict the output, but they also learn how to compact all the previous inputs. The input to an RNN is a concatenation of the original stateless input and the hidden state. This idea of a persistent hidden state is learned from ordered inputs is what distinguishes an RNN from linear and deep neural networks. In a deep neural network, the hidden state is not updated during prediction. In a recurrent neural network, it is. The reason that RNNs are able to remember what they've seen previously is because of two innovations. Firstly, they have a recurrent connection between their hidden layer in their input, and secondly, because they use a clever trick during optimization, which forces the network to learn how to use this memory effectively. The recurrent connection is depicted here in red. Note how the hidden layer has the same number of units as the fixed size state. Because the recurrent connections simply copies the value in the hidden layer to the fixed size state for the next iteration, the two must have the same number of dimensions. But RNNs suffer from the vanishing gradient problem and so can't model long-term dependencies. Then we dove into LSTMs, which can. The LSTM network looks exactly the same as an RNN from the outside, it's only when we zoom into the cell we see there's a lot more going on. The shortcut line along the bottom of the cell, allows information to pass through in a more straightforward path in the hidden state. GRU cells like LSTMs, use sigmoid gates to control what to remember and what to forget. But they do it in a simpler and more efficient manner. Then we discussed how transfer learning applies to natural language. We discussed using the word2Vec approach to building embeddings for natural language. We also learned about GloVe embeddings and how they're similar to and different from previous approaches. Rather than build these embeddings from scratch, we use TensorFlow hub to download pretrained state of the art models. Finally, we looked at a sequence-to-sequence model called the encoder-decoder network which can be used to solve tasks like machine translation, text summarization, and question answering. We looked at how to train the model to pay more attention to the word cat, instead of the word black at the first time step using an attention network. Then we looked at a solution that abstracts away the model training called AutoML translation. Using AutoML translation required three steps. Prepare training and test data, build the model and finally evaluate the results. For the model building step, we can either use a prebuilt model, or build a custom model for our specific domain. If what you want as a convenient way to train a conversation system, such as a chatbot, you can use dialogue flow. We built a rich conversational experience through intent and entity classification. This brings us to the end of the course on sequence modeling. Join us in the final course in this specialization to learn how to build a recommendation system.