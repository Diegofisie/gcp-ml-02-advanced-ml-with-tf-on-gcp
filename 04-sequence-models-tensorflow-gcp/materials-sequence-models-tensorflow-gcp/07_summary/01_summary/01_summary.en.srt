1
00:00:00,000 --> 00:00:01,105
Congratulations.

2
00:00:01,105 --> 00:00:02,955
You've made it to the end of the course.

3
00:00:02,955 --> 00:00:05,415
Let's recap what we've covered so far.

4
00:00:05,415 --> 00:00:08,485
We started by learning about what makes something a sequence.

5
00:00:08,485 --> 00:00:12,160
We also introduce some classical approaches to sequence modeling.

6
00:00:12,160 --> 00:00:16,380
You saw some of the common business applications for using ML on sequences,

7
00:00:16,380 --> 00:00:20,980
like predicting the future supply of commodities for coffee beans and time series data.

8
00:00:20,980 --> 00:00:23,670
You also learned the different types of sequence models,

9
00:00:23,670 --> 00:00:27,305
like one-to-sequence, sequence-to-one and sequence-to-sequence.

10
00:00:27,305 --> 00:00:31,820
Document summarization for example is a sequence-to- sequence model.

11
00:00:31,820 --> 00:00:34,890
Then you saw the similarities between image data,

12
00:00:34,890 --> 00:00:37,344
and sequence data when it comes to locality.

13
00:00:37,344 --> 00:00:41,405
A more recent events may be more important than less recent ones.

14
00:00:41,405 --> 00:00:46,340
We then introduce recurrent neural networks and explained how they maintain state.

15
00:00:46,340 --> 00:00:49,685
Recall the two key ideas in RNN architecture.

16
00:00:49,685 --> 00:00:51,785
RNNs learn to predict the output,

17
00:00:51,785 --> 00:00:55,260
but they also learn how to compact all the previous inputs.

18
00:00:55,260 --> 00:00:57,770
The input to an RNN is a concatenation of

19
00:00:57,770 --> 00:01:00,580
the original stateless input and the hidden state.

20
00:01:00,580 --> 00:01:03,965
This idea of a persistent hidden state is learned from

21
00:01:03,965 --> 00:01:08,420
ordered inputs is what distinguishes an RNN from linear and deep neural networks.

22
00:01:08,420 --> 00:01:09,840
In a deep neural network,

23
00:01:09,840 --> 00:01:12,245
the hidden state is not updated during prediction.

24
00:01:12,245 --> 00:01:14,865
In a recurrent neural network, it is.

25
00:01:14,865 --> 00:01:17,510
The reason that RNNs are able to remember what they've

26
00:01:17,510 --> 00:01:20,110
seen previously is because of two innovations.

27
00:01:20,110 --> 00:01:22,570
Firstly, they have a recurrent connection between

28
00:01:22,570 --> 00:01:24,810
their hidden layer in their input, and secondly,

29
00:01:24,810 --> 00:01:27,280
because they use a clever trick during optimization,

30
00:01:27,280 --> 00:01:30,820
which forces the network to learn how to use this memory effectively.

31
00:01:30,820 --> 00:01:33,700
The recurrent connection is depicted here in red.

32
00:01:33,700 --> 00:01:37,900
Note how the hidden layer has the same number of units as the fixed size state.

33
00:01:37,900 --> 00:01:41,110
Because the recurrent connections simply copies the value

34
00:01:41,110 --> 00:01:43,960
in the hidden layer to the fixed size state for the next iteration,

35
00:01:43,960 --> 00:01:46,665
the two must have the same number of dimensions.

36
00:01:46,665 --> 00:01:48,610
But RNNs suffer from

37
00:01:48,610 --> 00:01:52,645
the vanishing gradient problem and so can't model long-term dependencies.

38
00:01:52,645 --> 00:01:55,305
Then we dove into LSTMs, which can.

39
00:01:55,305 --> 00:01:59,500
The LSTM network looks exactly the same as an RNN from the outside,

40
00:01:59,500 --> 00:02:02,905
it's only when we zoom into the cell we see there's a lot more going on.

41
00:02:02,905 --> 00:02:05,360
The shortcut line along the bottom of the cell,

42
00:02:05,360 --> 00:02:09,830
allows information to pass through in a more straightforward path in the hidden state.

43
00:02:09,830 --> 00:02:12,615
GRU cells like LSTMs,

44
00:02:12,615 --> 00:02:16,020
use sigmoid gates to control what to remember and what to forget.

45
00:02:16,020 --> 00:02:18,970
But they do it in a simpler and more efficient manner.

46
00:02:18,970 --> 00:02:22,880
Then we discussed how transfer learning applies to natural language.

47
00:02:22,880 --> 00:02:27,495
We discussed using the word2Vec approach to building embeddings for natural language.

48
00:02:27,495 --> 00:02:30,020
We also learned about GloVe embeddings and how they're

49
00:02:30,020 --> 00:02:33,250
similar to and different from previous approaches.

50
00:02:33,250 --> 00:02:35,985
Rather than build these embeddings from scratch,

51
00:02:35,985 --> 00:02:40,205
we use TensorFlow hub to download pretrained state of the art models.

52
00:02:40,205 --> 00:02:43,490
Finally, we looked at a sequence-to-sequence model called

53
00:02:43,490 --> 00:02:48,185
the encoder-decoder network which can be used to solve tasks like machine translation,

54
00:02:48,185 --> 00:02:51,040
text summarization, and question answering.

55
00:02:51,040 --> 00:02:54,770
We looked at how to train the model to pay more attention to the word cat,

56
00:02:54,770 --> 00:02:59,005
instead of the word black at the first time step using an attention network.

57
00:02:59,005 --> 00:03:01,970
Then we looked at a solution that abstracts away

58
00:03:01,970 --> 00:03:04,900
the model training called AutoML translation.

59
00:03:04,900 --> 00:03:08,060
Using AutoML translation required three steps.

60
00:03:08,060 --> 00:03:09,980
Prepare training and test data,

61
00:03:09,980 --> 00:03:13,085
build the model and finally evaluate the results.

62
00:03:13,085 --> 00:03:14,565
For the model building step,

63
00:03:14,565 --> 00:03:16,130
we can either use a prebuilt model,

64
00:03:16,130 --> 00:03:19,275
or build a custom model for our specific domain.

65
00:03:19,275 --> 00:03:23,070
If what you want as a convenient way to train a conversation system,

66
00:03:23,070 --> 00:03:24,365
such as a chatbot,

67
00:03:24,365 --> 00:03:26,085
you can use dialogue flow.

68
00:03:26,085 --> 00:03:30,865
We built a rich conversational experience through intent and entity classification.

69
00:03:30,865 --> 00:03:34,235
This brings us to the end of the course on sequence modeling.

70
00:03:34,235 --> 00:03:35,750
Join us in the final course in

71
00:03:35,750 --> 00:03:39,150
this specialization to learn how to build a recommendation system.