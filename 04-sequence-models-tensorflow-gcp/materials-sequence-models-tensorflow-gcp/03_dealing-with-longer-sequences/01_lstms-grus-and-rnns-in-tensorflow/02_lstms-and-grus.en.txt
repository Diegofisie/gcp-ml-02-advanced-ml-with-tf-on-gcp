Before we look at an LSTM cell, here's a reminder of what a basic RNN cell looks like. It has two inputs, X_t which contains the features for the current time step and H_t minus one which is the hidden state from the previous time step. This represents the context of all features seen before. We concatenate X_t and H_t minus one, multiplied by our weight matrix, add our bias vector and finally squash the resulting vector to between negative one and one using the tanh function. This gives us our state to pass to the next time step, H_t. You may also hear H_t be referred to as the activation because it is the result after applying the activation function, tanh. Some people also refer to H_t as the output of the RNN. So, that was a simple RNN cell, let's contrast that with an LSTM cell. First observe that an unrolled LSTM network, pictured in the lower left, looks exactly the same as a simple RNN network from the outside. It's only when we zoom into the cell that we see the differences. The first thing you may notice is that instead of having two inputs and one output, our cell now has three inputs and two outputs. Namely, we've added a second state vector called C. By convention, we call this new state vector the cell state and the original state vector, our hidden state. You may also hear the cell state referred to as the LSTM's memory. Now, another thing you may notice is that the line along the bottom of the cell, which represents the cell state, allows information to pass through in a much more straightforward path than the hidden state. One intuition is to think of this added cell state as a conveyor belt that makes it easy for state to pass through unchanged. It's actually the same principle as a shortcut connection like the ones used in resonance which we talked about in the course on image models. Isn't it cool to see a similar solution to the same problem just applied in different context? If the intuition isn't clicking yet, maybe it'll make more sense when you look at this cell from a mathematical point of view. Now, fair warning, there will be a lot of equations on the next slide but don't panic. The ideas are far more simple than the equations make them look. Here, we'll use an abbreviated notation that X equals the concatenation of X_t and H_t minus one. This is just to make the remaining equations more compact. In an LSTM cell, we compute three gates. We call these forget, update and output gates. The gates correspond to the lowercase sigmas on the diagram. Each gate has its own weight matrix which allows the cell to independently learn what is important to remember and what can be forgotten. So, while simple RNNs have only one weight matrix that is shared across time, LSTMs have four. After multiplying each gate by its respective weight matrix, each gate squashes the resulting vector to numbers between zero and one using the sigmoid function. Now, if you think about multiplying the squash vector by some other signal, you can see how it acts as a gate. Values close to zero will kill most of the input signal, values close to one will let the signal pass through nearly unchanged. Next, we have our input layer. You may recognize this as the equation we had in our simple RNN cell. In the LSTM cell, however, is just one of six equations. The next equation is where things start coming together. Remember that C is our cell state, our conveyor belt of sorts. The cell state for the next time step is calculated as the cell state from the previous time step multiplied by what we learn to forget plus our new input layer multiplied by what we choose to remember. Finally, our hidden state is calculated as the cell state squashed to between one and negative one by tanh then multiplied by our output gate which determines what part of the cell state to expose to the hidden state. Our final hidden state is what we'd expose as our cell output. So, perhaps it's a bit of a misnomer that this is called the hidden state. In summary, we're giving our cell the ability to control, what to forget from the cell state, what new data to store into the cell state and what data from the cell state to expose to the hidden state. Now, if that particular collection of equations seemed somewhat arbitrary to you, you're onto something. In fact, there are many variants of those equations that all works similarly well with long sequences. You may have heard of one of such variants of LSTM called GRU which stands for Gated Recurrent Unit. GRU cells, like LSTMs, use sigmoid gates to control what to remember and what to forget. But GRU cells do it in a simpler and more efficient manner requiring only one state vector instead of two and only two sigmoid gates instead of three. Many people prefer to use GRU as their default over LSTM because it yields similar performance while requiring less compute power. We'll use GRU as our cell of choice in the upcoming labs. How many weight matrices do a simple RNN, LSTM, and GRU cell have respectively? The correct answer is one, four, and three. A simple RNN has just one weight matrix which is used to calculate its hidden state. LSTMs and GRUs add one weight matrix for each sigmoid gate they calculate. An LSTM has three gates but a GRU only has two. The number of weight matrices does not depend on the number of time steps because the same weights are shared across time.