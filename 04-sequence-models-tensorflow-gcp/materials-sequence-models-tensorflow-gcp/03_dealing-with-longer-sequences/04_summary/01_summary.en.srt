1
00:00:00,000 --> 00:00:06,265
Welcome back. I hope that lab opened your eyes to the usefulness of RNNs.

2
00:00:06,265 --> 00:00:08,840
While RNNs are extremely powerful,

3
00:00:08,840 --> 00:00:11,325
they can also be extremely finicky.

4
00:00:11,325 --> 00:00:15,870
You have to consider the length of your sequences if you want them to be

5
00:00:15,870 --> 00:00:22,000
overlapping or not overlapping and write a function to preprocess them in such a way.

6
00:00:22,000 --> 00:00:24,284
For your model architecture,

7
00:00:24,284 --> 00:00:28,110
you have to experiment with the number and type of RNN cells,

8
00:00:28,110 --> 00:00:30,075
the size of the hidden state,

9
00:00:30,075 --> 00:00:34,195
and how many hidden states to use when calculating your loss function.

10
00:00:34,195 --> 00:00:37,950
Finally, you have to think about if and how to

11
00:00:37,950 --> 00:00:42,430
incorporate non-sequential data into your sequential model.

12
00:00:42,430 --> 00:00:45,650
If even one hyperparameter is off,

13
00:00:45,650 --> 00:00:49,580
the whole model may perform badly as we saw in the lab.

14
00:00:49,580 --> 00:00:53,090
So don't be discouraged if modeling time series data with

15
00:00:53,090 --> 00:00:56,390
an RNN takes a lot of trial and error to get right.

16
00:00:56,390 --> 00:00:58,355
This is normal.

17
00:00:58,355 --> 00:01:02,630
If possible, be sure to start with easier to tune models like

18
00:01:02,630 --> 00:01:08,010
linear DNN and CNN first to establish a benchmark.