I hope you enjoyed that last demo, and got a sense for how, with RNNs, we can build a more robust loss function by taking into account multiple predictions instead of just one. Now, let's shift gears away from the model for a moment and focus on the data. Up until now, we've been using synthetic data. This was great for getting our feet wet, but it protected you from many of the logistical details of modeling a real-world sequential data-set. Now, it's time to take the training wheels off. We're going to build a model to predict temperature at a given location, given previous daily temperatures, at that same location. Luckily, there's a publicly available data set that gives us daily temperature readings. It's called the Global Historical Climate Network daily, and it consists of daily temperature and precipitation records from over 100,000 weather stations across over 180 countries. To allow for fast iteration, we've selected data from 36 of these weather stations to use in our model. The first issue we'll tackle is how to split up our sequences. In our sprinkler lab, we limited our generated sequences to a length of 50. This was short enough that we didn't have to split up our sequences further. In our new data set, instead of 50 time steps, each weather station has 50 years of daily temperature readings. That gives us a sequence length of over 18,000. Now, while it's true that LTMs are better than simple RNNs at dealing with long sequences, even they have their limit. Clearly, we're no longer going to be able to feed in the whole sequence at once. Instead, we'll chop the sequence, and feed it in bit by bit. This gives us two advantages. First, shorter sequences are easier to train, and second, it's the kind of data augmentation. Instead of having just 36 really long sequences, one for each weather station, we get more training records, because we're creating several smaller sequences from each larger real-world sequence. Now, that we know why we need to split up our sequences, let's talk about how. The blessing and the curse of RNNs is that they're incredibly flexible. There are several ways we can feed the same source data into an RNN, and it's not always obvious what the best way is. Ultimately, empirical evidence will be our guide. We try something, observe some objective metric, then try something else to see if we can improve that metric. Two hyper-parameters to experiment with are, number one, how long should each sub-sequence be, and number two, how much should each sub-sequence overlap. For the first consideration, sequence length, we want a sequence long enough to capture any relevant context, but no longer, because at that point we're just making it harder to train. If I'm trying to predict tomorrow's weather, maybe knowing the weather patterns for the last ten days is enough context. But then again, since temperature cycles are annual, maybe I'll get better performance if I unroll for a full year, which would be a sequence length of 365. It's good to form such hypotheses by reasoning about the problem, but ultimately, we have to test to see what works best. Once we have our sequence length determined, we also need to determine if we want overlapping or non-overlapping sequences. Here's an example of non-overlapping sequences. Our original sequence is split into two mutually exclusive sequences, and you can see sequence two picks up where sequence one left off. Here, we overlap our sequences. We still have the same sub-sequence length, but overlapping enables us to get more training examples from the same source sequence. This is why, most commonly, you'll see overlapping sub-sequences. However, if we don't overlap, we can take advantage of a clever trick which indirectly allows us to train our longer sequences, while still applying weight updates over a smaller sequence length. The trick is to use the final state vector from a previous sequence to initialize the state of the next sequence. We'll use this trick in our upcoming lab. Of course, this will only work if the next sequence actually picks up where the last sequence left off. In other words, successive sequences need to be continuous and non-overlapping. If you take this approach, you need to take care in your input function, to make sure each row and each batch picks up where the respective row and the last batch left off. Note, in the diagram how the first row of the batch is always a continuation of weather station one, and the second row is always a continuation of weather station two. To summarize, you can split your source sequence into smaller sub-sequences, and those sub-sequences can be overlapping or non-overlapping. However, you decide to split up your sequence, you'll need to write code to implement it in your input function. We'll provide this code for you in the lab, but I encourage you to look over it and try to understand it. Another issue you may need to handle, is predicting not just one, but multiple time steps ahead. For example, let's say I want to be able to forecast the temperatures for the next ten days, instead of just tomorrow. One way to approach this, is to use a different model architecture called an encoder decoder model. We'll cover encoder decoder models later on in this course. However, there's another way to do this without changing our model at all. The idea is to train a model to predict just the next time step, but then take that prediction and feed it back into the model to generate the next prediction. That prediction would now represent two time steps ahead. If we continue in this manner, we can generate sequences as long as we'd like. Of course, the further out we go, the more we can expect our prediction quality to decline. It's important here, that we need to pass both the output 'y' and output state 'h' as the next input and input state respectively. If we forget to pass the state, then the next prediction would be based on just one time step of information, instead of the contexts from all previous time steps. Here are some other things to consider when working with real world data. Do we need to re-sample our data? Should I build one model or multiple models, and should I incorporate non-sequential data? Let's briefly consider each of these. First, we'll consider re-sampling data. Let's say that we wanted to predict daily average temperature, but our source data provided temperature reading every minute. That's far more granularity than we need, and would result in unnecessarily long sequences. One reading per minute means 1,440 readings per day. Since we only care about daily granularity, we'd be better off using a pre-processing step that takes all 1,000 plus readings from each day and averages them into a single value. The creation of a daily average temperature data-set from a source set that had temperature by minute, is called re-sampling. Now, let's consider the one model versus multiple models issue. We have data from 36 different weather stations. Does this mean we need to train multiple models, one per-station? It depends on whether we want to assume that weather patterns learn from one group of stations, generalize to another group of stations. In our lab, we will make that assumption, and so we can feed in all station data into a single model. If we didn't want to make that assumption and believed that weather patterns from different weather stations were fundamentally different, we could then build a separate model for each station. However, if we have thousands of stations, training a model for every station would be inefficient. Also, we're unlikely to have enough training data for all weather stations we want to make forecasts for. A better approach would be to pick some descriptive variables that we know about all stations. For example, latitude and longitude, and combine those variables into our model. This approach would generalize to new stations, yet still take into account characteristics about each station that make it different from other stations. This brings us to our last consideration. Let's say we did want to take the approach of feeding in latitude and longitude into our model. How would we do that? A naive approach would be to simply add these as inputs to our RNN during each time step. However, since these variables are constant, which is to say, they don't change over-time, feeding them into an RNN which is designed for time series data, is at best, wasted computation, and at worst, adding noise to the signal of the true time series features. A better approach would be to add the non-sequential features as inputs to the DNN layer that sits after the RNN. In other words, instead of just passing our hidden state from the RNN as the input to the DNN, we would pass the concatenation of our non-sequential features, and our RNN hidden state to the DNN. In our daily temperature model, we could even use this approach to pass the temperature on a given day from a previous year, as a feature. That would really give our model a head-start.