1
00:00:00,000 --> 00:00:03,880
In this lab, we'll be working with real data for the first time.

2
00:00:03,880 --> 00:00:09,420
Specifically, temperature data that has been gathered from 36 weather stations.

3
00:00:09,420 --> 00:00:14,660
Each of those weather stations has daily temperature readings for the last 50 years.

4
00:00:14,660 --> 00:00:18,655
This enables us to build a long-term forecasting model.

5
00:00:18,655 --> 00:00:21,420
In particular, we're going to build

6
00:00:21,420 --> 00:00:24,820
a model to predict the weather for the next 1.5 years.

7
00:00:24,820 --> 00:00:28,815
Now, this lab is structured a little differently from previous labs.

8
00:00:28,815 --> 00:00:31,035
The starting lab code runs fine,

9
00:00:31,035 --> 00:00:32,915
but the resulting model is poor.

10
00:00:32,915 --> 00:00:34,330
Throughout the course of this lab,

11
00:00:34,330 --> 00:00:38,075
we'll make incremental improvements until we have an acceptable model.

12
00:00:38,075 --> 00:00:41,160
Let's start by observing the status quo,

13
00:00:41,160 --> 00:00:43,590
which is to just from the notebook as is,

14
00:00:43,590 --> 00:00:45,460
and observe the results.

15
00:00:45,460 --> 00:00:48,315
To do so, click run,

16
00:00:48,315 --> 00:00:50,330
and then run all cells.

17
00:00:50,330 --> 00:00:52,885
I've already done so to save time.

18
00:00:52,885 --> 00:00:54,865
So, let's skip to the bottom,

19
00:00:54,865 --> 00:00:56,585
and check out the results.

20
00:00:56,585 --> 00:01:00,830
Don't worry, I'll go back and explain the intermediate cells later.

21
00:01:00,830 --> 00:01:02,645
Here we generate predictions for

22
00:01:02,645 --> 00:01:05,490
eight weather stations that we didn't see during training.

23
00:01:05,490 --> 00:01:08,915
The green represents the ground truth data,

24
00:01:08,915 --> 00:01:13,615
and the blue represents the predicted data from our base model.

25
00:01:13,615 --> 00:01:17,740
The predictions are being generated three years out,

26
00:01:17,740 --> 00:01:22,315
but we're only measuring RMSE on the first 1.5 years.

27
00:01:22,315 --> 00:01:26,805
As you can see, our model is performing pretty poorly right now.

28
00:01:26,805 --> 00:01:28,360
If you look closely,

29
00:01:28,360 --> 00:01:31,180
you can see the blue prediction line does follow

30
00:01:31,180 --> 00:01:36,310
the green ground truth line for a very brief period of time for some of the stations,

31
00:01:36,310 --> 00:01:42,040
but soon thereafter it just produces the same prediction, over and over.

32
00:01:42,040 --> 00:01:45,650
Also note that the lines have a width to them,

33
00:01:45,650 --> 00:01:50,040
that's because we're modeling both the minimum and maximum temperatures.

34
00:01:50,040 --> 00:01:53,930
The bottom of the line represents the minimum temperature prediction,

35
00:01:53,930 --> 00:01:56,640
and the top of the line represents the maximum.

36
00:01:56,640 --> 00:02:02,205
My RMSE averaged across the eight weather stations is 11.77.

37
00:02:02,205 --> 00:02:05,130
Now that we've seen the performance of our base model,

38
00:02:05,130 --> 00:02:07,520
let's try to improve it.

39
00:02:10,530 --> 00:02:14,515
The first hyperparameter we'll play with is sequence length.

40
00:02:14,515 --> 00:02:17,100
Currently, it's only 32.

41
00:02:17,100 --> 00:02:19,190
Does that seem right to you?

42
00:02:19,190 --> 00:02:22,190
Since we're generating predictions over one year out,

43
00:02:22,190 --> 00:02:24,245
and our data granularity is daily.

44
00:02:24,245 --> 00:02:27,650
We really need at least 365 time-steps

45
00:02:27,650 --> 00:02:30,670
to capture the pattern of a yearly temperature cycle.

46
00:02:30,670 --> 00:02:33,130
With only 32 time-steps,

47
00:02:33,130 --> 00:02:37,835
it's no surprise our model performs poorly after just a few predictions.

48
00:02:37,835 --> 00:02:41,360
We're asking it to model a pattern during prediction,

49
00:02:41,360 --> 00:02:43,790
that we never gave it examples of during training.

50
00:02:43,790 --> 00:02:49,645
The naive solution would be to simply increase our sequence length to 365.

51
00:02:49,645 --> 00:02:51,400
But that many steps is a bit much for

52
00:02:51,400 --> 00:02:55,205
an RNN and will make training again convergence difficult.

53
00:02:55,205 --> 00:02:59,630
Instead, let's resample our data to have each time-step

54
00:02:59,630 --> 00:03:04,790
represent the average minimum and maximum temperature for a five-day window.

55
00:03:04,790 --> 00:03:08,055
Whereas previously, it was just for one day.

56
00:03:08,055 --> 00:03:13,520
This is what the resample by hyperparameter controls.

57
00:03:16,300 --> 00:03:18,830
With this broader sampling,

58
00:03:18,830 --> 00:03:22,990
I can use an easier to train sequence length of 128,

59
00:03:22,990 --> 00:03:28,190
and still have enough data to capture annual weather patterns.

60
00:03:29,310 --> 00:03:34,140
Of course, when we resample to five-day granularity,

61
00:03:34,140 --> 00:03:38,300
we give up the ability to make predictions at single day granularity.

62
00:03:38,300 --> 00:03:40,680
But, for this particular model,

63
00:03:40,680 --> 00:03:42,395
we're looking at long-term trends,

64
00:03:42,395 --> 00:03:45,400
and don't care about single day predictions.

65
00:03:46,010 --> 00:03:51,500
To confirm we now have sequences long enough to capture annual weather patterns,

66
00:03:51,500 --> 00:03:56,525
let's look at the section of the notebook called Visualizing Training Sequences.

67
00:03:56,525 --> 00:04:01,285
This shows us what sequences fed into our model from a given weather station look like.

68
00:04:01,285 --> 00:04:05,035
Currently, we're seeing just 32-day Windows.

69
00:04:05,035 --> 00:04:12,030
Now, let's rerun our data preprocessing with the new resample size of five days,

70
00:04:12,030 --> 00:04:14,695
and sequence length of 128.

71
00:04:14,695 --> 00:04:17,215
This will take a few seconds.

72
00:04:17,215 --> 00:04:19,300
Now, that looks much better.

73
00:04:19,300 --> 00:04:22,405
I can clearly see the yearly oscillations.

74
00:04:22,405 --> 00:04:24,280
If I look at my predictions,

75
00:04:24,280 --> 00:04:26,435
they're also looking much better than before.

76
00:04:26,435 --> 00:04:28,675
But, still far from good.

77
00:04:28,675 --> 00:04:31,685
One reason our model is struggling,

78
00:04:31,685 --> 00:04:36,300
is because temperatures have micro trends that don't follow the macro trends.

79
00:04:36,300 --> 00:04:40,340
For example, if I'm transitioning from winter to spring,

80
00:04:40,340 --> 00:04:44,380
the macro trend is that each day should be warmer than the previous.

81
00:04:44,380 --> 00:04:49,040
But as we know, there are individual days that may be colder than the previous,

82
00:04:49,040 --> 00:04:53,130
which makes it harder for the model to learn the macro signal.

83
00:04:53,130 --> 00:04:56,330
Even now that we're looking at five-day averages,

84
00:04:56,330 --> 00:04:58,300
this is still an issue.

85
00:04:58,300 --> 00:05:01,310
However, the further we look ahead,

86
00:05:01,310 --> 00:05:04,115
the more stable the macro signal will be.

87
00:05:04,115 --> 00:05:07,700
For example, when transitioning from winter to spring,

88
00:05:07,700 --> 00:05:12,590
it's very unlikely that a day one month from today will be colder than today.

89
00:05:13,420 --> 00:05:16,360
To take advantage of this phenomenon,

90
00:05:16,360 --> 00:05:20,570
we adjust the hyperparameter and underscore forward.

91
00:05:20,570 --> 00:05:24,855
This determines how many steps ahead the label is from the input.

92
00:05:24,855 --> 00:05:28,610
If we make an underscore forward too small,

93
00:05:28,610 --> 00:05:31,335
the model will be distracted by anomalies.

94
00:05:31,335 --> 00:05:32,960
If we make it too large,

95
00:05:32,960 --> 00:05:35,825
we'll overshoot even the macro trend.

96
00:05:35,825 --> 00:05:40,180
Let's try predicting eight steps ahead and rerunning.

97
00:05:49,210 --> 00:05:52,055
Before we look at the results,

98
00:05:52,055 --> 00:05:55,990
let's take a minute to understand or inference code.

99
00:05:56,730 --> 00:05:59,185
For our first prediction,

100
00:05:59,185 --> 00:06:03,355
we initialize our hidden state here called H to zero,

101
00:06:03,355 --> 00:06:10,130
and we feed in five years of data here called Yin to prime our network.

102
00:06:10,410 --> 00:06:18,210
We then invoke our model function by passing in our features and state.

103
00:06:19,140 --> 00:06:22,645
We make sure to disable dropout since we want

104
00:06:22,645 --> 00:06:26,780
all neurons in our model active during prediction.

105
00:06:27,090 --> 00:06:34,075
Our model function returns Yout which contains N_FORWARD predictions,

106
00:06:34,075 --> 00:06:35,750
which in this case is A,

107
00:06:35,750 --> 00:06:40,400
as well as the final hidden state from the RNN.

108
00:06:41,640 --> 00:06:43,875
From this point forward,

109
00:06:43,875 --> 00:06:49,270
we call the model in a loop passing in the outputs from the last run as our features,

110
00:06:49,270 --> 00:06:53,935
and initializing the state with the final hidden state from the last run.

111
00:06:53,935 --> 00:06:56,455
After each prediction in a loop,

112
00:06:56,455 --> 00:07:00,380
the predictions are appended to a list called results,

113
00:07:00,380 --> 00:07:02,695
so that list can be plotted,

114
00:07:02,695 --> 00:07:07,280
and we can visually compare our model predictions to the ground truth.

115
00:07:07,500 --> 00:07:11,285
Speaking of which, let's look at those plots now.

116
00:07:11,285 --> 00:07:13,535
How much did it shift in our label?

117
00:07:13,535 --> 00:07:17,090
Eight steps ahead instead of just one help.

118
00:07:18,950 --> 00:07:24,380
Nice. Now, our results are finally starting to look reasonable.

119
00:07:24,780 --> 00:07:29,780
The RMSE is down to 5.95.

120
00:07:30,300 --> 00:07:33,775
Let's try one last set of adjustments.

121
00:07:33,775 --> 00:07:38,840
Up until this point, we've been improving our model by changing how we feed in our data.

122
00:07:38,840 --> 00:07:41,830
But, we haven't changed the RNN model itself.

123
00:07:41,830 --> 00:07:45,275
Let's do that now. To start,

124
00:07:45,275 --> 00:07:49,385
let's inspect what our existing RNN model code looks like.

125
00:07:49,385 --> 00:07:52,980
Here we have a relatively simple RNN model.

126
00:07:52,980 --> 00:07:55,509
It uses a single GRUCell,

127
00:07:55,509 --> 00:07:58,570
then takes the outputs called Hr,

128
00:07:58,570 --> 00:08:03,235
and feeds them through a linear layer to produce two predictions.

129
00:08:03,235 --> 00:08:05,580
One for the minimum temperature,

130
00:08:05,580 --> 00:08:08,190
and the other for the maximum temperature.

131
00:08:08,340 --> 00:08:10,930
As a side note,

132
00:08:10,930 --> 00:08:13,500
you may notice here that we're calculating

133
00:08:13,500 --> 00:08:17,965
a loss using predictions from all of our hidden states.

134
00:08:17,965 --> 00:08:20,530
Previously in the lecture video,

135
00:08:20,530 --> 00:08:22,465
I said this wasn't a good idea,

136
00:08:22,465 --> 00:08:27,129
because the early time-steps don't have enough context to make good predictions.

137
00:08:27,129 --> 00:08:31,270
However, in this model we've structured our input so

138
00:08:31,270 --> 00:08:35,270
that each sequence is a continuation of the previous batch.

139
00:08:35,270 --> 00:08:37,960
We're actually initializing the hidden state

140
00:08:37,960 --> 00:08:41,210
with the output state from the previous batch.

141
00:08:41,210 --> 00:08:45,735
So, for all sequences safe for the very first batch,

142
00:08:45,735 --> 00:08:51,310
the sequence actually has adequate context starting at time-step zero.

143
00:08:51,310 --> 00:08:57,790
Okay. Now, we're going to make three improvements to our model.

144
00:08:57,790 --> 00:09:02,870
First, we will make this a stacked RNN by adding a second layer.

145
00:09:02,870 --> 00:09:07,550
Second, we will add dropout to ourselves to combat overfitting,

146
00:09:07,550 --> 00:09:11,780
and lastly we'll increase the capacity of our RNN cells by

147
00:09:11,780 --> 00:09:16,890
changing RNN cell size from 80 to 128.

148
00:09:16,900 --> 00:09:21,990
You should make these changes one at a time to observe the impact of each.

149
00:09:21,990 --> 00:09:26,730
But here, I will make them all at once by pasting in the new model code.

150
00:09:26,730 --> 00:09:30,700
You can now see that I'm creating a list of cells,

151
00:09:31,280 --> 00:09:35,820
and I wrap each cell with the dropout wrapper.

152
00:09:37,180 --> 00:09:40,700
The remaining changes are to the hyperparameters.

153
00:09:40,700 --> 00:09:45,600
So, let's scroll back to the top of the notebook where hyperparameters are set.

154
00:09:47,390 --> 00:09:52,250
First, we'll change number of layers from one to two,

155
00:09:52,250 --> 00:09:55,565
because we're now using two RNN cells,

156
00:09:55,565 --> 00:10:01,230
and we'll increase our RNN cell size from 80 to 128.

157
00:10:02,330 --> 00:10:06,410
Finally, we'll rerun the notebook.

158
00:10:17,010 --> 00:10:20,000
Now that I have a more complex model,

159
00:10:20,000 --> 00:10:22,125
this will take slightly longer to run.

160
00:10:22,125 --> 00:10:25,150
But, it shouldn't take more than a few minutes.

161
00:10:25,150 --> 00:10:29,010
I'll skip the video ahead to when the model is done.

162
00:10:30,460 --> 00:10:35,720
Here's the model performance for the more complex RNN.

163
00:10:38,940 --> 00:10:43,280
Our RMSE is 5.51.

164
00:10:45,380 --> 00:10:50,540
Now, the performance here wasn't that much better than the single layer RNN.

165
00:10:50,540 --> 00:10:53,450
Perhaps, that means this extra capacity is

166
00:10:53,450 --> 00:10:57,375
superfluous or perhaps it means that I need to tune more.

167
00:10:57,375 --> 00:11:01,900
Experiment on your own to try to beat my RMSE.

168
00:11:02,850 --> 00:11:09,435
There we have it, our first modeling of a real-world sequencial dataset using an RNN.

169
00:11:09,435 --> 00:11:13,040
Note, there are details in this code that I've glossed over

170
00:11:13,040 --> 00:11:16,110
for the sake of keeping this walk-through to a reasonable length.

171
00:11:16,110 --> 00:11:21,410
For example, the code to break our source sequences properly into batches.

172
00:11:21,410 --> 00:11:26,295
I'd encourage you to go back and examine the code in more detail when you have more time.

173
00:11:26,295 --> 00:11:31,500
It will surely help when you have a time series modeling project of your own.