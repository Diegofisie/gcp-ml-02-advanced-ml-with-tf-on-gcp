In this lab, we'll be working with real data for the first time. Specifically, temperature data that has been gathered from 36 weather stations. Each of those weather stations has daily temperature readings for the last 50 years. This enables us to build a long-term forecasting model. In particular, we're going to build a model to predict the weather for the next 1.5 years. Now, this lab is structured a little differently from previous labs. The starting lab code runs fine, but the resulting model is poor. Throughout the course of this lab, we'll make incremental improvements until we have an acceptable model. Let's start by observing the status quo, which is to just from the notebook as is, and observe the results. To do so, click run, and then run all cells. I've already done so to save time. So, let's skip to the bottom, and check out the results. Don't worry, I'll go back and explain the intermediate cells later. Here we generate predictions for eight weather stations that we didn't see during training. The green represents the ground truth data, and the blue represents the predicted data from our base model. The predictions are being generated three years out, but we're only measuring RMSE on the first 1.5 years. As you can see, our model is performing pretty poorly right now. If you look closely, you can see the blue prediction line does follow the green ground truth line for a very brief period of time for some of the stations, but soon thereafter it just produces the same prediction, over and over. Also note that the lines have a width to them, that's because we're modeling both the minimum and maximum temperatures. The bottom of the line represents the minimum temperature prediction, and the top of the line represents the maximum. My RMSE averaged across the eight weather stations is 11.77. Now that we've seen the performance of our base model, let's try to improve it. The first hyperparameter we'll play with is sequence length. Currently, it's only 32. Does that seem right to you? Since we're generating predictions over one year out, and our data granularity is daily. We really need at least 365 time-steps to capture the pattern of a yearly temperature cycle. With only 32 time-steps, it's no surprise our model performs poorly after just a few predictions. We're asking it to model a pattern during prediction, that we never gave it examples of during training. The naive solution would be to simply increase our sequence length to 365. But that many steps is a bit much for an RNN and will make training again convergence difficult. Instead, let's resample our data to have each time-step represent the average minimum and maximum temperature for a five-day window. Whereas previously, it was just for one day. This is what the resample by hyperparameter controls. With this broader sampling, I can use an easier to train sequence length of 128, and still have enough data to capture annual weather patterns. Of course, when we resample to five-day granularity, we give up the ability to make predictions at single day granularity. But, for this particular model, we're looking at long-term trends, and don't care about single day predictions. To confirm we now have sequences long enough to capture annual weather patterns, let's look at the section of the notebook called Visualizing Training Sequences. This shows us what sequences fed into our model from a given weather station look like. Currently, we're seeing just 32-day Windows. Now, let's rerun our data preprocessing with the new resample size of five days, and sequence length of 128. This will take a few seconds. Now, that looks much better. I can clearly see the yearly oscillations. If I look at my predictions, they're also looking much better than before. But, still far from good. One reason our model is struggling, is because temperatures have micro trends that don't follow the macro trends. For example, if I'm transitioning from winter to spring, the macro trend is that each day should be warmer than the previous. But as we know, there are individual days that may be colder than the previous, which makes it harder for the model to learn the macro signal. Even now that we're looking at five-day averages, this is still an issue. However, the further we look ahead, the more stable the macro signal will be. For example, when transitioning from winter to spring, it's very unlikely that a day one month from today will be colder than today. To take advantage of this phenomenon, we adjust the hyperparameter and underscore forward. This determines how many steps ahead the label is from the input. If we make an underscore forward too small, the model will be distracted by anomalies. If we make it too large, we'll overshoot even the macro trend. Let's try predicting eight steps ahead and rerunning. Before we look at the results, let's take a minute to understand or inference code. For our first prediction, we initialize our hidden state here called H to zero, and we feed in five years of data here called Yin to prime our network. We then invoke our model function by passing in our features and state. We make sure to disable dropout since we want all neurons in our model active during prediction. Our model function returns Yout which contains N_FORWARD predictions, which in this case is A, as well as the final hidden state from the RNN. From this point forward, we call the model in a loop passing in the outputs from the last run as our features, and initializing the state with the final hidden state from the last run. After each prediction in a loop, the predictions are appended to a list called results, so that list can be plotted, and we can visually compare our model predictions to the ground truth. Speaking of which, let's look at those plots now. How much did it shift in our label? Eight steps ahead instead of just one help. Nice. Now, our results are finally starting to look reasonable. The RMSE is down to 5.95. Let's try one last set of adjustments. Up until this point, we've been improving our model by changing how we feed in our data. But, we haven't changed the RNN model itself. Let's do that now. To start, let's inspect what our existing RNN model code looks like. Here we have a relatively simple RNN model. It uses a single GRUCell, then takes the outputs called Hr, and feeds them through a linear layer to produce two predictions. One for the minimum temperature, and the other for the maximum temperature. As a side note, you may notice here that we're calculating a loss using predictions from all of our hidden states. Previously in the lecture video, I said this wasn't a good idea, because the early time-steps don't have enough context to make good predictions. However, in this model we've structured our input so that each sequence is a continuation of the previous batch. We're actually initializing the hidden state with the output state from the previous batch. So, for all sequences safe for the very first batch, the sequence actually has adequate context starting at time-step zero. Okay. Now, we're going to make three improvements to our model. First, we will make this a stacked RNN by adding a second layer. Second, we will add dropout to ourselves to combat overfitting, and lastly we'll increase the capacity of our RNN cells by changing RNN cell size from 80 to 128. You should make these changes one at a time to observe the impact of each. But here, I will make them all at once by pasting in the new model code. You can now see that I'm creating a list of cells, and I wrap each cell with the dropout wrapper. The remaining changes are to the hyperparameters. So, let's scroll back to the top of the notebook where hyperparameters are set. First, we'll change number of layers from one to two, because we're now using two RNN cells, and we'll increase our RNN cell size from 80 to 128. Finally, we'll rerun the notebook. Now that I have a more complex model, this will take slightly longer to run. But, it shouldn't take more than a few minutes. I'll skip the video ahead to when the model is done. Here's the model performance for the more complex RNN. Our RMSE is 5.51. Now, the performance here wasn't that much better than the single layer RNN. Perhaps, that means this extra capacity is superfluous or perhaps it means that I need to tune more. Experiment on your own to try to beat my RMSE. There we have it, our first modeling of a real-world sequencial dataset using an RNN. Note, there are details in this code that I've glossed over for the sake of keeping this walk-through to a reasonable length. For example, the code to break our source sequences properly into batches. I'd encourage you to go back and examine the code in more detail when you have more time. It will surely help when you have a time series modeling project of your own.