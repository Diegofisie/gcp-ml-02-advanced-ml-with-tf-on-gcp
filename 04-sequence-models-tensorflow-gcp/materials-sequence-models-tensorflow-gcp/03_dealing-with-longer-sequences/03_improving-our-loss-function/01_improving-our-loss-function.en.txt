Welcome back. I hope you were able to get some practice stacking RNNs cells to create deeper more expressive RNNs. Use with caution though. Deep RNNs takes significantly more time to compute. Another improvement we can make to our RNN is making the loss function more robust. Up until now, we've only been using the activation from our final timestep to make a prediction. This diagram illustrates this. We take the final activation from the RNN, H6 in this case pass it through a dense layer to allow a nonlinear relationship between the state vector and the final output, and then through a linear layer to output a single value. This makes sense because in our sprinkler problem, we're only asking for a single value namely the height of the sprinkler and the next time step after the input sequence. However, you could argue that a model that can predict the last value in a sequence accurately should also be able to predict the second to last value in the sequence accurately. Otherwise, it's just a lucky guess. So, one way to improve the robustness of our loss function against these lucky guesses during training is to measure loss not just on the final prediction in the sequence, but to issue predictions over the last k time steps in the sequence and then average the loss over all these predictions. If this sounds familiar, it's because you've seen a model that takes into account losses from intermediate outputs before. Remember Google net the inception CNN from the image classification course after certain amount of convolution and pooling layers, the model will output predictions midway through the whole model and average that loss into the total loss function. So, how many prediction should we average loss over? Well, that's the hyperparameter. If k is too small like k equals one, our loss function may be too noisy. It will unduly reward lucky guesses and punish unlucky guesses. What if K is too large? Let's say for example k was equal to the entire sequence length, in this case, we'll be measuring loss on predictions that have very little context. In fact, our first prediction will have only seed one previous datapoint. We probably don't care if the loss on that prediction is high because we'll never be making predictions with so little context. In this diagram, we've chosen k to be half of the total sequence length. Now, let's see a demo of how implementing this looks in TensorFlow