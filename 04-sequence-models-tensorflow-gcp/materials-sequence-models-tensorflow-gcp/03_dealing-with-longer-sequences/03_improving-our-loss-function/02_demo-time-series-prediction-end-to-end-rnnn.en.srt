1
00:00:00,000 --> 00:00:03,025
The code for the rnnN_model,

2
00:00:03,025 --> 00:00:04,790
is already implemented for you.

3
00:00:04,790 --> 00:00:07,120
But we'll walk through how to build it here.

4
00:00:07,120 --> 00:00:10,305
Once again, let's start from our previous model,

5
00:00:10,305 --> 00:00:13,750
rnn2, and modify it from there.

6
00:00:19,880 --> 00:00:24,295
Now, the main modification we're going to make here,

7
00:00:24,295 --> 00:00:28,420
is instead of returning just one prediction using the final hidden state,

8
00:00:28,420 --> 00:00:33,305
we're going to generate a prediction from every intermediate hidden state as well.

9
00:00:33,305 --> 00:00:37,760
Multiple predictions allow us to calculate multiple losses,

10
00:00:37,760 --> 00:00:40,165
and then take the average.

11
00:00:40,165 --> 00:00:47,730
To do this, we now take advantage of the outputs tensor return by our dynamic rnn.

12
00:00:48,860 --> 00:00:51,765
Recall that outputs contains,

13
00:00:51,765 --> 00:00:53,485
not just the final hidden state,

14
00:00:53,485 --> 00:00:56,090
but the hidden state at every time step.

15
00:00:56,090 --> 00:01:01,790
So now, instead of having just one state vector, we have 49.

16
00:01:01,790 --> 00:01:06,290
We want to generate a prediction for every one of these 49 hidden states.

17
00:01:06,290 --> 00:01:10,780
So, we'll feed the outputs tensor as our input to the dense layer,

18
00:01:10,780 --> 00:01:14,600
whereas, previously we fed in the state tensor.

19
00:01:18,640 --> 00:01:23,090
Effectively, we're creating 49 equivalent DNNs.

20
00:01:23,090 --> 00:01:27,625
Each one takes in a state vector and outputs a single prediction.

21
00:01:27,625 --> 00:01:32,110
Finally, we flatten this from a 49 by one tensor,

22
00:01:32,110 --> 00:01:37,980
into a single vector that contains 49 predictions and return this.

23
00:01:40,870 --> 00:01:43,985
Now at this point, you may be thinking,

24
00:01:43,985 --> 00:01:46,765
why are we returning 49 predictions?

25
00:01:46,765 --> 00:01:50,320
Don't we just want to compute loss on the last k state vectors,

26
00:01:50,320 --> 00:01:52,405
as we talked about before.

27
00:01:52,405 --> 00:01:56,540
Well, you're right, but we handle that in a separate part of the code.

28
00:01:56,540 --> 00:02:00,650
Specifically, the compute errors function below.

29
00:02:04,760 --> 00:02:07,570
Here is where we distinguish between

30
00:02:07,570 --> 00:02:12,140
our regular loss function and our rnnN loss function.

31
00:02:12,970 --> 00:02:15,255
In the regular case,

32
00:02:15,255 --> 00:02:17,265
where we just return one prediction,

33
00:02:17,265 --> 00:02:21,095
we simply compute the loss on that single prediction.

34
00:02:21,095 --> 00:02:25,210
However, if we return multiple predictions,

35
00:02:25,210 --> 00:02:27,550
then we implement our loss averaging code.

36
00:02:27,550 --> 00:02:30,380
Now to calculate loss over multiple predictions,

37
00:02:30,380 --> 00:02:32,810
you need a label for each of those predictions.

38
00:02:32,810 --> 00:02:36,950
Our input function only provides a label for the final prediction.

39
00:02:36,950 --> 00:02:40,610
So, we can calculate the remaining labels here.

40
00:02:41,050 --> 00:02:46,940
Since our label is actually just our input features shifted one time step ahead,

41
00:02:46,940 --> 00:02:51,620
we can generate a labels tensor by extracting from the features tensor,

42
00:02:51,620 --> 00:02:54,160
starting one position forward.

43
00:02:54,160 --> 00:02:57,275
We do that using slicing notation.

44
00:02:57,275 --> 00:03:00,320
Next, is where we determine what portion

45
00:03:00,320 --> 00:03:03,980
of the predictions we want to calculate loss over.

46
00:03:03,980 --> 00:03:08,025
This determination is a hyperparameter that you can experiment with.

47
00:03:08,025 --> 00:03:11,075
But here we chose to use the last third.

48
00:03:11,075 --> 00:03:15,760
Finally, we use the tf.losses.mean_squared_error

49
00:03:15,760 --> 00:03:20,755
function and pass in our labels tensor and our predictions tensor.

50
00:03:20,755 --> 00:03:23,800
But slice to just the last third of the values,

51
00:03:23,800 --> 00:03:27,200
because those are the only ones we want to consider in our loss function.

52
00:03:27,200 --> 00:03:29,240
It's important to note here,

53
00:03:29,240 --> 00:03:34,750
that customizing our loss function doesn't affect how we calculate our evaluation metric.

54
00:03:34,750 --> 00:03:42,660
When calculating rmse, we still only consider loss on the single last prediction,

55
00:03:42,660 --> 00:03:46,480
because that's ultimately what we care about for our model.

56
00:03:46,480 --> 00:03:49,325
Averaging our loss over multiple predictions,

57
00:03:49,325 --> 00:03:52,760
is just a more efficient way to optimize to that goal.