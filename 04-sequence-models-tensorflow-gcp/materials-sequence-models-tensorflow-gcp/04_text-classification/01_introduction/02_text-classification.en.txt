Text classification is a common machine learning problem. Let's take a look at some common examples. Email, is it spam or not spam? Customer reviews, is the customer is satisfied or not satisfied? Document classification, is this document about project Falcon? Style classification, was this play written by Shakespeare? All of these are text classification problems. In the upcoming lab, we'll tackle a version of the document classification problem. In particular, we'll try to identify the publisher of an article given the article's headline. We'll have headlines from three publishers, the New York Times, TechCrunch, and GitHub. Our features are the words in the article title, our label is the respective publication, and the model learns how to predict the label given the features. Simple, right? Well, not so fast. As we know, ML models just perform mathematical operations and mathematical operations can only be performed on numbers. So, we can't just feed our text directly into the model. The trick to working with text data is to first find a numerical representation for our text that maintains as much of the meaning of the text as possible. Here's one recipe to convert our sentences to numbers. First, create a mapping from each word to a unique integer. Second, encode each sentence as a sequence of integers using the mapping from step one. Third, pad each sequence to a constant length, and finally, convert each integer into an embedded representation with meaningful magnitude. We'll walk through each of these steps next. But before doing so, it's worth noting that this is just one type of text representation. Depending on your application, a simpler representation may suffice. Now, onto the details. Our first step is to create a mapping from each word to a unique integer. To reduce noise in our training data, we typically will only encode the top K most common words. K is also referred to as our vocabulary size. A common choice for K is 10,000 or 20,000. Limiting our vocabulary size allows us to ignore words that only occur once or twice in our training set which will only confuse our model. Once we define our vocabulary size, then we feed in all the titles of all the articles that we have in our database to a tokenizer which creates a dictionary object. A dictionary maps a unique integer to each of the top K most frequent words in the corpus. Finally, we save this mapping to disk so that clients wanting to use our model later can encode text using the same mapping. Note, relying on our clients to encode text isn't ideal and we'll discuss an alternative approach to this later. Now, that we have our mapping, we can use it to encode any sequence into a sequence of integers. In this example, the word supreme maps to 100, the word court mapped to 2,031, and so on. Words that are not in our vocabulary will map to zero. Next, we pad our sequence to some constant length. You may be thinking, why do we need to have a constant length input? Aren't RNNs able to handle variable length inputs? Well, they are. But in practice, we don't just feed one example into an RNN at a time, we feed a whole batch; and each example in the batch needs to have the same length, otherwise it wouldn't be a legal tensor and we couldn't do efficient matrix operations on it. Kera provides a one-liner to pad sequences to length called pad_sequences. It will also truncate sequences that are longer than the max sequence length. At this point, we have a simple numeric representation of our sentence, but it's not the numeric representation we want. Why is that? It's because this is a categorical representation. The numbers don't have meaningful magnitude, they just serve as numeric IDs. So, in order to use them, we'd have to one-hot encode each number. If our vocabulary size is 20,000, that means a vector with 1,999 zeros and a single one. The advantage of using an embedding over a one-hot encoding is that it avoids this sparsity which neural networks struggle with. Also, the embeddings can learn which words are similar to each other by assigning them similar numbers. We discussed how embeddings are created in detail earlier in this specialization. I'll provide a link if you want to review. In short, embeddings are learned by adding a dense layer to our network that is much smaller than the width of our one-hot encoding, and the network learns widths to translate from the one-hot encoding to a dense floating point representation that captures meaning. So, how do we create this embedding? Like most things in TensorFlow, there are multiple ways to do it. First, let's see how to do this using Keras. We simply start our model with an embedding layer, specify the number of words in our vocabulary, specify how many dimensions we want our embedding to have, and specify our max sequence length. That's it. Keras does the rest. What if you weren't using Keras though? Then, we could create embeddings using TensorFlow's feature columns API. We'd first convert our words to their one-hot representation using the categorical column with vocabulary list function. Then, we'd wrap that categorical column in an embedding column and specify the number of dimensions we want to embed into. Note that to save memory, TensorFlow actually stores categorical columns as sparse tensors behind the scenes, not as one-hot vectors. But that's just an implementation detail. We still call them one-hot vectors because functionally they act as such.