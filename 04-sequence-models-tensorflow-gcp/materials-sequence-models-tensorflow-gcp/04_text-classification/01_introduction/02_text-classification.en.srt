1
00:00:00,000 --> 00:00:03,845
Text classification is a common machine learning problem.

2
00:00:03,845 --> 00:00:06,815
Let's take a look at some common examples.

3
00:00:06,815 --> 00:00:10,265
Email, is it spam or not spam?

4
00:00:10,265 --> 00:00:15,650
Customer reviews, is the customer is satisfied or not satisfied?

5
00:00:15,650 --> 00:00:21,510
Document classification, is this document about project Falcon?

6
00:00:21,510 --> 00:00:26,685
Style classification, was this play written by Shakespeare?

7
00:00:26,685 --> 00:00:30,375
All of these are text classification problems.

8
00:00:30,375 --> 00:00:32,040
In the upcoming lab,

9
00:00:32,040 --> 00:00:35,680
we'll tackle a version of the document classification problem.

10
00:00:35,680 --> 00:00:38,130
In particular, we'll try to identify

11
00:00:38,130 --> 00:00:42,225
the publisher of an article given the article's headline.

12
00:00:42,225 --> 00:00:45,315
We'll have headlines from three publishers,

13
00:00:45,315 --> 00:00:49,400
the New York Times, TechCrunch, and GitHub.

14
00:00:49,400 --> 00:00:53,000
Our features are the words in the article title,

15
00:00:53,000 --> 00:00:56,195
our label is the respective publication,

16
00:00:56,195 --> 00:01:00,295
and the model learns how to predict the label given the features.

17
00:01:00,295 --> 00:01:02,335
Simple, right?

18
00:01:02,335 --> 00:01:04,630
Well, not so fast.

19
00:01:04,630 --> 00:01:07,330
As we know, ML models just perform

20
00:01:07,330 --> 00:01:13,105
mathematical operations and mathematical operations can only be performed on numbers.

21
00:01:13,105 --> 00:01:16,590
So, we can't just feed our text directly into the model.

22
00:01:16,590 --> 00:01:20,979
The trick to working with text data is to first find a numerical representation

23
00:01:20,979 --> 00:01:25,960
for our text that maintains as much of the meaning of the text as possible.

24
00:01:25,960 --> 00:01:30,370
Here's one recipe to convert our sentences to numbers.

25
00:01:30,370 --> 00:01:35,215
First, create a mapping from each word to a unique integer.

26
00:01:35,215 --> 00:01:41,950
Second, encode each sentence as a sequence of integers using the mapping from step one.

27
00:01:41,950 --> 00:01:46,170
Third, pad each sequence to a constant length,

28
00:01:46,170 --> 00:01:49,495
and finally, convert each integer

29
00:01:49,495 --> 00:01:53,425
into an embedded representation with meaningful magnitude.

30
00:01:53,425 --> 00:01:56,950
We'll walk through each of these steps next.

31
00:01:56,950 --> 00:01:58,960
But before doing so,

32
00:01:58,960 --> 00:02:03,025
it's worth noting that this is just one type of text representation.

33
00:02:03,025 --> 00:02:04,990
Depending on your application,

34
00:02:04,990 --> 00:02:07,940
a simpler representation may suffice.

35
00:02:08,280 --> 00:02:10,880
Now, onto the details.

36
00:02:10,880 --> 00:02:15,560
Our first step is to create a mapping from each word to a unique integer.

37
00:02:15,560 --> 00:02:18,050
To reduce noise in our training data,

38
00:02:18,050 --> 00:02:21,965
we typically will only encode the top K most common words.

39
00:02:21,965 --> 00:02:25,690
K is also referred to as our vocabulary size.

40
00:02:25,690 --> 00:02:29,710
A common choice for K is 10,000 or 20,000.

41
00:02:29,710 --> 00:02:34,190
Limiting our vocabulary size allows us to ignore words that only

42
00:02:34,190 --> 00:02:38,850
occur once or twice in our training set which will only confuse our model.

43
00:02:38,850 --> 00:02:42,080
Once we define our vocabulary size,

44
00:02:42,080 --> 00:02:45,470
then we feed in all the titles of all the articles that we have in

45
00:02:45,470 --> 00:02:50,365
our database to a tokenizer which creates a dictionary object.

46
00:02:50,365 --> 00:02:53,390
A dictionary maps a unique integer to

47
00:02:53,390 --> 00:02:57,025
each of the top K most frequent words in the corpus.

48
00:02:57,025 --> 00:03:01,310
Finally, we save this mapping to disk so that clients

49
00:03:01,310 --> 00:03:05,960
wanting to use our model later can encode text using the same mapping.

50
00:03:05,960 --> 00:03:09,670
Note, relying on our clients to encode text isn't

51
00:03:09,670 --> 00:03:13,285
ideal and we'll discuss an alternative approach to this later.

52
00:03:13,285 --> 00:03:15,319
Now, that we have our mapping,

53
00:03:15,319 --> 00:03:19,085
we can use it to encode any sequence into a sequence of integers.

54
00:03:19,085 --> 00:03:23,075
In this example, the word supreme maps to 100,

55
00:03:23,075 --> 00:03:27,930
the word court mapped to 2,031, and so on.

56
00:03:27,930 --> 00:03:31,840
Words that are not in our vocabulary will map to zero.

57
00:03:31,840 --> 00:03:36,000
Next, we pad our sequence to some constant length.

58
00:03:36,000 --> 00:03:40,250
You may be thinking, why do we need to have a constant length input?

59
00:03:40,250 --> 00:03:43,910
Aren't RNNs able to handle variable length inputs?

60
00:03:43,910 --> 00:03:45,740
Well, they are.

61
00:03:45,740 --> 00:03:49,705
But in practice, we don't just feed one example into an RNN at a time,

62
00:03:49,705 --> 00:03:51,600
we feed a whole batch;

63
00:03:51,600 --> 00:03:55,520
and each example in the batch needs to have the same length,

64
00:03:55,520 --> 00:03:57,995
otherwise it wouldn't be a legal tensor

65
00:03:57,995 --> 00:04:01,505
and we couldn't do efficient matrix operations on it.

66
00:04:01,505 --> 00:04:07,965
Kera provides a one-liner to pad sequences to length called pad_sequences.

67
00:04:07,965 --> 00:04:13,055
It will also truncate sequences that are longer than the max sequence length.

68
00:04:13,055 --> 00:04:17,410
At this point, we have a simple numeric representation of our sentence,

69
00:04:17,410 --> 00:04:22,080
but it's not the numeric representation we want. Why is that?

70
00:04:22,080 --> 00:04:25,810
It's because this is a categorical representation.

71
00:04:25,810 --> 00:04:28,730
The numbers don't have meaningful magnitude,

72
00:04:28,730 --> 00:04:31,540
they just serve as numeric IDs.

73
00:04:31,540 --> 00:04:33,520
So, in order to use them,

74
00:04:33,520 --> 00:04:36,645
we'd have to one-hot encode each number.

75
00:04:36,645 --> 00:04:39,980
If our vocabulary size is 20,000,

76
00:04:39,980 --> 00:04:45,915
that means a vector with 1,999 zeros and a single one.

77
00:04:45,915 --> 00:04:49,400
The advantage of using an embedding over

78
00:04:49,400 --> 00:04:54,205
a one-hot encoding is that it avoids this sparsity which neural networks struggle with.

79
00:04:54,205 --> 00:04:57,380
Also, the embeddings can learn which words are

80
00:04:57,380 --> 00:05:01,120
similar to each other by assigning them similar numbers.

81
00:05:01,120 --> 00:05:06,380
We discussed how embeddings are created in detail earlier in this specialization.

82
00:05:06,380 --> 00:05:09,100
I'll provide a link if you want to review.

83
00:05:09,100 --> 00:05:13,040
In short, embeddings are learned by adding a dense layer to

84
00:05:13,040 --> 00:05:17,710
our network that is much smaller than the width of our one-hot encoding,

85
00:05:17,710 --> 00:05:20,900
and the network learns widths to translate from

86
00:05:20,900 --> 00:05:25,245
the one-hot encoding to a dense floating point representation that captures meaning.

87
00:05:25,245 --> 00:05:27,800
So, how do we create this embedding?

88
00:05:27,800 --> 00:05:30,050
Like most things in TensorFlow,

89
00:05:30,050 --> 00:05:31,975
there are multiple ways to do it.

90
00:05:31,975 --> 00:05:35,995
First, let's see how to do this using Keras.

91
00:05:35,995 --> 00:05:39,740
We simply start our model with an embedding layer,

92
00:05:39,740 --> 00:05:43,280
specify the number of words in our vocabulary,

93
00:05:43,280 --> 00:05:47,490
specify how many dimensions we want our embedding to have,

94
00:05:47,490 --> 00:05:50,480
and specify our max sequence length.

95
00:05:50,480 --> 00:05:53,565
That's it. Keras does the rest.

96
00:05:53,565 --> 00:05:56,505
What if you weren't using Keras though?

97
00:05:56,505 --> 00:06:01,895
Then, we could create embeddings using TensorFlow's feature columns API.

98
00:06:01,895 --> 00:06:04,640
We'd first convert our words to

99
00:06:04,640 --> 00:06:10,640
their one-hot representation using the categorical column with vocabulary list function.

100
00:06:10,640 --> 00:06:14,010
Then, we'd wrap that categorical column in

101
00:06:14,010 --> 00:06:19,160
an embedding column and specify the number of dimensions we want to embed into.

102
00:06:19,160 --> 00:06:21,815
Note that to save memory,

103
00:06:21,815 --> 00:06:26,945
TensorFlow actually stores categorical columns as sparse tensors behind the scenes,

104
00:06:26,945 --> 00:06:28,805
not as one-hot vectors.

105
00:06:28,805 --> 00:06:31,190
But that's just an implementation detail.

106
00:06:31,190 --> 00:06:35,990
We still call them one-hot vectors because functionally they act as such.