That concludes our module on text classification. We started by talking about how working with text data requires finding a meaningful numeric representation. One hot encoding isn't ideal because it doesn't capture anything about the meaning of words and because its sparsity would kill the gradients in our neural network. Learning an embedded representations solves both these problems. We then apply this representation to solve a text classification problem. We used a sequential model so that we could learn patterns between adjacent words, and not just words in isolation. In particular, we used a CNN because it's easier to tune than an RNN, and is known to produce good results on sequential classification problems. Finally, we talked about the difference between implementing pre-processing in native Python functions versus native TensorFlow functions. Things are often of less lines of code in native Python, but we give up flexibility when it comes to production deployment. I hope this module has given you the tools to go forth and build your own text classifier.