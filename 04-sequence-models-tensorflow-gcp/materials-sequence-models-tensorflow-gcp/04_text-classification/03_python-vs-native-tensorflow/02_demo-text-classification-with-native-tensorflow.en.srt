1
00:00:00,000 --> 00:00:01,520
In the lecture video,

2
00:00:01,520 --> 00:00:02,700
I stated that porting

3
00:00:02,700 --> 00:00:07,635
native python preprocessing code to TensorFlow was a two-part process.

4
00:00:07,635 --> 00:00:13,045
The first part is to identify the functions we're relying on that are pure python.

5
00:00:13,045 --> 00:00:18,415
To do that, let's open up our original model.py and look for the input function,

6
00:00:18,415 --> 00:00:20,720
which is responsible for preprocessing.

7
00:00:20,720 --> 00:00:26,100
In here, the two Python functions I need to replace are tokenizer.text

8
00:00:26,100 --> 00:00:32,410
to sequences, and sequence.pad_sequences.

9
00:00:32,930 --> 00:00:37,250
The first is responsible for transforming our text to a sequence of

10
00:00:37,250 --> 00:00:42,965
integers and the second is responsible for padding those sequences to a constant length.

11
00:00:42,965 --> 00:00:48,414
The problem is that they work on Python objects not on TensorFlow tensors.

12
00:00:48,414 --> 00:00:53,795
Now, let's switch over to the model underscore native.py

13
00:00:53,795 --> 00:00:56,240
file to look at how we can reproduce

14
00:00:56,240 --> 00:01:00,320
this preprocessing with native TensorFlow operations.

15
00:01:00,320 --> 00:01:06,695
Here I have my model_native.py file opened up to the input function.

16
00:01:06,695 --> 00:01:12,085
The first step is to convert our Python list of strings into a string tensor.

17
00:01:12,085 --> 00:01:18,145
We do that by passing the Python strings as input to the tf.constant operation.

18
00:01:18,145 --> 00:01:21,724
This outputs a matrix of string tensors.

19
00:01:21,724 --> 00:01:25,240
Now, to map the string tensors to integers,

20
00:01:25,240 --> 00:01:29,245
I created a helper function called vectorize sentences.

21
00:01:29,245 --> 00:01:32,860
Recall, we did all of this with one line of code using

22
00:01:32,860 --> 00:01:37,055
the native Python function tokenizer.text to sequences,

23
00:01:37,055 --> 00:01:40,830
that took care of a lot for us behind the scenes.

24
00:01:40,830 --> 00:01:44,080
Before, we took these things for granted,

25
00:01:44,080 --> 00:01:46,734
but now we have to implement them explicitly.

26
00:01:46,734 --> 00:01:51,545
One thing that it did for us was removed punctuation from our input sentences.

27
00:01:51,545 --> 00:01:55,400
Because we don't want the fact that a word ends with a period versus

28
00:01:55,400 --> 00:02:00,895
an exclamation point versus a question mark to make it be tokenized differently.

29
00:02:00,895 --> 00:02:04,759
If we did, our tokens would be overly specific

30
00:02:04,759 --> 00:02:08,980
and there wouldn't be enough examples of each token to learn a meaningful embedding.

31
00:02:08,980 --> 00:02:16,080
In TensorFlow, we can use the tf.regex replace function to accomplish this.

32
00:02:16,080 --> 00:02:22,295
Next, we split our string tensor from sentence-level to word-level.

33
00:02:22,295 --> 00:02:25,330
This is a two-part process.

34
00:02:25,330 --> 00:02:28,750
The tf.string split operation,

35
00:02:28,750 --> 00:02:32,195
splits this sentence tensors into word tensors.

36
00:02:32,195 --> 00:02:36,685
But in order to accommodate the fact that different sentences have different lengths,

37
00:02:36,685 --> 00:02:39,975
it returns a sparse representation.

38
00:02:39,975 --> 00:02:46,335
Variable length records along the same dimension are not allowed in a dense tensor.

39
00:02:46,335 --> 00:02:48,960
To convert back to a dense tensor,

40
00:02:48,960 --> 00:02:51,030
we use the tf.sparse tensor to

41
00:02:51,030 --> 00:02:55,705
dense operation which allows us to specify a default value.

42
00:02:55,705 --> 00:02:59,360
The default value is used to pad all records to equal

43
00:02:59,360 --> 00:03:03,635
the length of the longest record in order to make it illegal tensor.

44
00:03:03,635 --> 00:03:08,400
Finally, we're ready to map each word to its respective integer.

45
00:03:08,400 --> 00:03:10,635
To do this in native TensorFlow,

46
00:03:10,635 --> 00:03:16,025
we create a lookup table using the index table from a file operation.

47
00:03:16,025 --> 00:03:19,190
The lookup table as instantiated using

48
00:03:19,190 --> 00:03:23,840
a vocabulary file which we generated from our tokenizer object.

49
00:03:23,840 --> 00:03:29,630
The code for the creation of the vocabulary file is in the train and evaluate function.

50
00:03:29,630 --> 00:03:32,580
I encourage you to pause the video,

51
00:03:32,580 --> 00:03:35,835
locate that code, and then comeback.

52
00:03:35,835 --> 00:03:38,175
Now, using the table lookup,

53
00:03:38,175 --> 00:03:41,695
we convert our word tensors into numbers.

54
00:03:41,695 --> 00:03:44,765
Now, we have an integer representation

55
00:03:44,765 --> 00:03:48,675
of our sentence using native TensorFlow operations.

56
00:03:48,675 --> 00:03:52,450
So, that's one of the two Python functions replaced.

57
00:03:52,450 --> 00:03:57,930
Now, let's address replacing the sequences.pad underscore sequences function.

58
00:03:57,930 --> 00:04:00,565
Before we get to the actual padding,

59
00:04:00,565 --> 00:04:03,080
we first have to do one more thing to achieve

60
00:04:03,080 --> 00:04:06,925
parody with a tokenizer text to sequences function.

61
00:04:06,925 --> 00:04:13,610
The text to sequences function skips out of vocabulary words when integerizing.

62
00:04:13,610 --> 00:04:16,265
Our TensorFlow lookup table however,

63
00:04:16,265 --> 00:04:19,820
keeps these words and maps them to the number zero.

64
00:04:19,820 --> 00:04:22,330
When porting between different functions,

65
00:04:22,330 --> 00:04:25,505
be careful to watch out for these little differences.

66
00:04:25,505 --> 00:04:28,010
To get back to parody,

67
00:04:28,010 --> 00:04:32,920
I need to remove the zeros added by the TensorFlow lookup table function.

68
00:04:32,920 --> 00:04:41,140
I can do this using a tf.where operation to locate the indices of the non-zero integers,

69
00:04:42,600 --> 00:04:46,345
and a tf.gather operation to extract

70
00:04:46,345 --> 00:04:50,540
only the elements corresponding to those non-zero indices.

71
00:04:50,540 --> 00:04:55,825
I then do a TF squeezed to get back to the proper dimensions.

72
00:04:55,825 --> 00:05:01,180
Note that, removing the zeros which represent the out-of-vocabulary word,

73
00:05:01,180 --> 00:05:04,150
results in records of variable length because

74
00:05:04,150 --> 00:05:07,745
some records may have more out of vocabulary words than others.

75
00:05:07,745 --> 00:05:11,319
This isn't legal for a batch of tensors.

76
00:05:11,319 --> 00:05:15,040
The reason I'm able to do this without TensorFlow throwing

77
00:05:15,040 --> 00:05:18,695
an error is because in my input function,

78
00:05:18,695 --> 00:05:25,475
I converted the tensor returned by the vectorized sentence function into a TF dataset.

79
00:05:25,475 --> 00:05:30,680
Then I called the pad function using dataset.map.

80
00:05:30,680 --> 00:05:35,520
This effectively feeds just one record at a time to the pad function,

81
00:05:35,520 --> 00:05:38,835
and single records can be of any length.

82
00:05:38,835 --> 00:05:43,870
Eventually though, I am going to batch my tf.dataset,

83
00:05:43,870 --> 00:05:49,140
and if my records are still a variable length at that point, I'll point an error.

84
00:05:49,530 --> 00:05:58,200
So, I fixed that by padding using the tf.pad operation and the tf.slice operation.

85
00:06:01,450 --> 00:06:05,595
That was a lot of work compared to the original way of doing things.

86
00:06:05,595 --> 00:06:08,760
So, where's the payoff?

87
00:06:11,170 --> 00:06:15,425
Here's my original text classification notebook.

88
00:06:15,425 --> 00:06:19,700
Note how I have to do my preprocessing in the client using

89
00:06:19,700 --> 00:06:25,100
the tokenizer texts to sequences and sequence.path sequence functions.

90
00:06:25,100 --> 00:06:29,795
Particularly problematic is that in order to use the tokenizer,

91
00:06:29,795 --> 00:06:34,955
I need to have access to a file on disk containing the correct word to integer mapping.

92
00:06:34,955 --> 00:06:39,060
Only then can I pass my data to the model API.

93
00:06:40,610 --> 00:06:46,680
Now, contrast this to the native prediction call.

94
00:06:47,170 --> 00:06:53,300
Here I don't need to do any preprocessing other than ensuring my text is lowercase,

95
00:06:53,300 --> 00:06:57,980
and my client doesn't need to have access to any special dictionary mapping.

96
00:06:57,980 --> 00:07:00,015
Even the lower casing,

97
00:07:00,015 --> 00:07:01,984
I could have done using native TensorFlow.

98
00:07:01,984 --> 00:07:06,900
But it's an easy enough client requirement that I didn't include it in the model code.