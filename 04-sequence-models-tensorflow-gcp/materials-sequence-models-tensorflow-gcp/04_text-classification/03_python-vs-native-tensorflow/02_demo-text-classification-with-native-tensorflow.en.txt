In the lecture video, I stated that porting native python preprocessing code to TensorFlow was a two-part process. The first part is to identify the functions we're relying on that are pure python. To do that, let's open up our original model.py and look for the input function, which is responsible for preprocessing. In here, the two Python functions I need to replace are tokenizer.text to sequences, and sequence.pad_sequences. The first is responsible for transforming our text to a sequence of integers and the second is responsible for padding those sequences to a constant length. The problem is that they work on Python objects not on TensorFlow tensors. Now, let's switch over to the model underscore native.py file to look at how we can reproduce this preprocessing with native TensorFlow operations. Here I have my model_native.py file opened up to the input function. The first step is to convert our Python list of strings into a string tensor. We do that by passing the Python strings as input to the tf.constant operation. This outputs a matrix of string tensors. Now, to map the string tensors to integers, I created a helper function called vectorize sentences. Recall, we did all of this with one line of code using the native Python function tokenizer.text to sequences, that took care of a lot for us behind the scenes. Before, we took these things for granted, but now we have to implement them explicitly. One thing that it did for us was removed punctuation from our input sentences. Because we don't want the fact that a word ends with a period versus an exclamation point versus a question mark to make it be tokenized differently. If we did, our tokens would be overly specific and there wouldn't be enough examples of each token to learn a meaningful embedding. In TensorFlow, we can use the tf.regex replace function to accomplish this. Next, we split our string tensor from sentence-level to word-level. This is a two-part process. The tf.string split operation, splits this sentence tensors into word tensors. But in order to accommodate the fact that different sentences have different lengths, it returns a sparse representation. Variable length records along the same dimension are not allowed in a dense tensor. To convert back to a dense tensor, we use the tf.sparse tensor to dense operation which allows us to specify a default value. The default value is used to pad all records to equal the length of the longest record in order to make it illegal tensor. Finally, we're ready to map each word to its respective integer. To do this in native TensorFlow, we create a lookup table using the index table from a file operation. The lookup table as instantiated using a vocabulary file which we generated from our tokenizer object. The code for the creation of the vocabulary file is in the train and evaluate function. I encourage you to pause the video, locate that code, and then comeback. Now, using the table lookup, we convert our word tensors into numbers. Now, we have an integer representation of our sentence using native TensorFlow operations. So, that's one of the two Python functions replaced. Now, let's address replacing the sequences.pad underscore sequences function. Before we get to the actual padding, we first have to do one more thing to achieve parody with a tokenizer text to sequences function. The text to sequences function skips out of vocabulary words when integerizing. Our TensorFlow lookup table however, keeps these words and maps them to the number zero. When porting between different functions, be careful to watch out for these little differences. To get back to parody, I need to remove the zeros added by the TensorFlow lookup table function. I can do this using a tf.where operation to locate the indices of the non-zero integers, and a tf.gather operation to extract only the elements corresponding to those non-zero indices. I then do a TF squeezed to get back to the proper dimensions. Note that, removing the zeros which represent the out-of-vocabulary word, results in records of variable length because some records may have more out of vocabulary words than others. This isn't legal for a batch of tensors. The reason I'm able to do this without TensorFlow throwing an error is because in my input function, I converted the tensor returned by the vectorized sentence function into a TF dataset. Then I called the pad function using dataset.map. This effectively feeds just one record at a time to the pad function, and single records can be of any length. Eventually though, I am going to batch my tf.dataset, and if my records are still a variable length at that point, I'll point an error. So, I fixed that by padding using the tf.pad operation and the tf.slice operation. That was a lot of work compared to the original way of doing things. So, where's the payoff? Here's my original text classification notebook. Note how I have to do my preprocessing in the client using the tokenizer texts to sequences and sequence.path sequence functions. Particularly problematic is that in order to use the tokenizer, I need to have access to a file on disk containing the correct word to integer mapping. Only then can I pass my data to the model API. Now, contrast this to the native prediction call. Here I don't need to do any preprocessing other than ensuring my text is lowercase, and my client doesn't need to have access to any special dictionary mapping. Even the lower casing, I could have done using native TensorFlow. But it's an easy enough client requirement that I didn't include it in the model code.