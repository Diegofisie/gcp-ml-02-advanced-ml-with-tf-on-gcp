I hope you enjoyed working on a text classification lab. Now, let's go over the solution together. First, I'll create my data set. We'll take advantage of a publicly available data set from BigQuery which contains headlines posted on Hacker News, between 2006 and 2015. Hacker News is a message board where technology enthusiasts share and up vote articles. Let's look at the first query so. Here, we have a query that simply selects the URL, title, and score column from our table, and filters to titles that are at least ten characters long and have received at least ten up votes. The URL gives us the source publication of the article, which is our label. However, in its current state, it's unusable. The next query, addresses that. This query uses regular expression parsing to extract just the publisher name out of a URL. For example, http://mobile.nytimes.com will simply become NY times. Then, groups articles by source and shows us the most common ones. Now, that we have a way to generate labels, let's create our training and evaluation data sets. For this model, we'll train a three-way classifier that distinguishes between the publishers NY Times, GitHub, and TechCrunch. Notice the filter in the where clause. To create a reproducible and evenly distributed split, we'll use the modulo function on the hash of the title column. If we look at the value count, we can see our training data set as roughly three times as large as our evaluation data set. Furthermore, we can see that our classes are roughly evenly distributed. So, we can use accuracy as an evaluation metric later on. Our data sets are currently in memory in pandas data frames. So, our final step will be to write them to disk as tab-separated files. We use bash commands to verify that the data was properly written to disk. First, we use the head command to print out the first three lines of the train file. Then, we verify the line counts of the train in eval files. We have 24,000 evaluation records and 72,000 training records. Great. We now have our data set. Let's move on to the model and try to understand the code. There's a hyperlink to the model.py directory in the markdown which I'll open now. We'll start with the train and evaluate function. You can think of this function as the conductor that calls all other functions. First, we invoke the load Hacker News data utility, which reads our training and test data from disk into memory. Then, we create a vocabulary mapping the top-k words to integers using the keras tokenizer function. Then, we create a vocabulary mapping the top_k words to integers using the carriers tokenizer function. We then save that vocabulary mapping to disk for our clients to use later. Next, we initialize our estimator using our carriers estimator function which we'll look at later. Finally, we create our train spec, our eval spec, and then start training. This part of the code is familiar. Let's investigate our input function next, which transforms our data from its current Natural Language state into a vectorized representation that our model can use. Our first step in the input function is to convert our sentences and from a list of words into a list of integers. We do this using the text to sequences function from the keras tokenizer object we created earlier. Next, we pad the list of integers to a constant length using the keras paths sequences function. This allows us to feed data into our model in batches, since all records will now have the same dimensions. The remainder of the function is standard shuffling and batching. Now, let's look at our actual model function. We use the carrier sequential API to build a CNN to process our text. The first layer in our model is the embedding layer which takes our simple integer representation as input and converts each integer into a richer representation using a vector of floats. This allows our model to learn which words have similar properties instead of treating each word as independent. Note that there is an option here to load weights from a pre-trained embedding or to learn the embedding from scratch. For now, we'll learn the embedding from scratch, which is to say the embedding weights will be randomly initialized. From here on, we have a standard CNN. It has two convolution layers, here and here, two pooling layers here and here, and add dropout layers to combat over-fitting. The global average pooling layer is simply an average pooling layer whose pool size is equal to the size of the input. This results in an output of size one times the number of channels. It's an intelligent way of flattening the output of our CNN in order to feed it into our softmax layer for classification. Next, we compile Archerius model, bypass and get an optimizer, a loss function, and an evaluation metric. The global average pooling layer is simply an average pooling layer whose pool size is equal to the size of the input. This results in an output of size one times the number of channels. It's an intelligent way of flattening the output of our CNN in order to feed it into our softmax layer for classification. Next, we compile our keras model, bypass and get an optimizer, a loss function, and an evaluation metric. Finally, we convert the keras model to an estimator using the model to estimate a function. Now, that we have an understanding of our code, let's switch back to our Data Lab Notebook, and test the code locally. Great, our code works. We've only trained for 57 steps. So, our accuracy is still quite bad. Let's copy our trained data into the Cloud and run a longer training job using Cloud Machine Learning Engine. The job will take about ten minutes to complete. Here, I have the results of the training job I ran earlier. We can see that our accuracy starts at 37 percent, which is pretty much equivalent to random guessing given we have three classes. We can also see that it increases to over 80 percent over time. Those are really encouraging results. Let's deploy this model to Cloud Machine Learning Engine and make some predictions. To test our predictions, I've selected some recent headlines from Hacker News that were not part of our training or evaluation data sets. My first three headlines are from TechCrunch. Our model classified them all correctly. Although, it was more confident on some headlines than others. The next three headlines are from the New York Times, and our model gets two of the three correct. It's encouraging though that for the headline at misclassified, New York Times was second place in terms of probability. Our last three headlines or from GitHub, and our model classifies them all correctly with high confidence. Optionally, we can rerun our model using a pre-trained embedding. We won't go into the details of this now as reusible embeddings are the focus of the next module, but I will show you the difference in the training results. Here, the results from the training run using a pre-trained embedding are shown in red, and training embedding from scratch is shown in blue. You can see that, ultimately, it achieves the same accuracy. However, it converges to that accuracy much more quickly. You can see that ultimately, it achieves the same accuracy. However, it converges to that accuracy much more quickly after only 500 steps. This means we can train less, which saves time and money. You'll find out why this works in the next section.