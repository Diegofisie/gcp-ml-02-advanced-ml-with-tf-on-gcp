In this specialization, you learned how to do machine learning at scale, and how to build specialized machine learning models for images, sequences, and recommendations. We started with the course that recap the first specialization by building an end-to-end model. We started by exploring and visualizing the dataset of natality births in BigQuery. We finished by deploying an application that was capable of predicting the weights of newborn babies. We worked with a sub-sample of the dataset to develop a TensorFlow model. Once we had prototyped our model locally we used Cloud Dataflow to create a training and evaluation sets using the entire dataset. We then trained our model using Cloud ML Engine. Using this trained model, we served out a prediction service that an end user was able to consume via a Python Flask application that we deployed using App Engine. The second course was on building production machine learning models. We discussed many of the things that the system should be able to do and the components that take responsibility for doing these things. We discussed how to deal with change, and how they affect machine learning systems, and what we can do to mitigate those effects. We talked about how to squeeze the most performance artifact ML system by choosing the right hardware such as TPUs and removing bottlenecks. We talked at a high level about the technology behind hybrid systems, which may run on the Cloud or on the Edge or on Prem. The third course was on building image models. We looked at linear and deep neural networks for image classification but quickly ran into trouble. Then we looked at convolutional and pooling layers and how they can operate as feature extractors. After building the image classifiers from scratch, we experimented with AutoML, which uses a combination of transfer learning and neural architecture search. All you have to do is upload your images into Cloud storage and the product does the rest. The fourth course was on building sequence models. These are the types of models used in natural language problems like text classification and translation. We started out by looking at the similarities between image data and sequence data when it comes to locality. We were able to apply many of the image techniques to sequences. We then looked at LSTM models, which allow the model to maintain state across successive inputs, something that is very important for sequence problems. We also discussed different approaches to build embeddings for natural language. Then we learn how to reuse machine learning components from TensorFlow hub. Finally, as with images, we looked at higher level abstractions. We looked at AutoML, but this time for text translation and a building conversation systems using Dialogflow. We ended the specialization with building real world recommendation systems. This brought together all the concepts that we learned in both the previous specialization and in this one. We looked at different types of recommendation systems, content-based, collaborative filtering, and knowledge-based, and how to implement each of them in TensorFlow to build end-to-end systems. We ended with a lab on how to orchestrate the continuous retraining of the recommendation system as new ratings data comes in from users. With this, we've come to the end of this specialization on advanced machine learning with TensorFlow on GCP. Thanks to all my colleagues for their great work and their attention to detail. Creating a set of 10 courses and keeping it fresh and up-to-date is a huge undertaking. I wouldn't have been able to do it without the contributions of so many people both in front of and behind the camera. We hope that you found this set of courses clear, practical, and useful. Do tell your friends, and remember that all the code that accompanies all the labs in this specialization is open source and available on GitHub. We hope that you're able to use the code to build your own end-to-end machine learning systems. Thank you, and good luck.