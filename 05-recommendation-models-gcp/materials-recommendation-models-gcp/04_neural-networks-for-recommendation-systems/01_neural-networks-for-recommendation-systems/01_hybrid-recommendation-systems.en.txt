Hi, I'm Ryan. I'm Machine Learning scientist at Google, and I love applying math and Machine Learning to big data to better make sense of the world. In the previous modules, we learned how both content-based and collaborative filtering forums recommender systems work. In this module, we will take advantage of the power of neural networks to create hybrid recommendation systems, using all that we have learned so far about recommendation systems put together. In this module, we will learn how to combine content-based, knowledge-based, and collaborative filtering recommendation systems. We'll learn how do this by using neural networks to make hybrid recommendation systems. We've already learned several types of recommendation systems. However, we use each in a vacuum, taking advantage of different types of data to try to make the best recommendations. We saw in module two, how to build content-based recommendation systems. This involve taking properties as items. This could have been unstructured data such as the genre of movies as seen in our earlier examples. It could also have been embeddings of the text description, images, or even audio, and/or video preprocessed through sequence files. We take this matrix of item properties, and multiply it with the user vector, to give that users representation in the item embedding space. We then can use when a multiple similarity measures, to recommend similar items that that user would like. Content-based recommendation systems have many pros. First, there is no need for data about other users. We just need the data about the user of interest, and then we can use that to recommend similar items from the learning embedding space. Also, we can recommend niche items because we know about our user specific tastes, which might not be shared amongst other users, and therefore we can recommend to meet those users interest. However, content-based recommendation systems have their cons. There needs to be domain knowledge. A human has to label the movie genres. A human has to enter the text description of the items. A human has to post the item picture. A human has to create the audio or video clip attached to the item. As you can see, there's a lot of involvement by expert humans who know their items very well. Also, content-based recommendation systems tend to make only safe recommendations. They stay within a user's safe bubble of the embedding space. If a user has never expanded beyond this bubble within that space, content-based recommendation systems will only recommend similar things, which will invariably end up being within the bubble, or near the edge. There isn't any information in a purely content- based recommendation system that can push a user outside the usual boundaries to explore items that they didn't know they might actually like. We learned in module three, how we can use wall's matrix factorization or collaborative filtering, to make recommendations based on user item interactions. It essentially takes those interactions and learns the latent factors within them, to best generalizes interactions. These latent factors create a D-dimensional embedding space, where a user and item embedding are solved for simultaneously. Not only create recommend items for users, but we can target users for items because we had the two embeddings. Collaborative filtering is very powerful because it requires no domain knowledge. The data generated itself, simply by users interacting with items. Now, we can harness that information to predict other favourable user item interactions through recommending and targeting. Remember, this can be either explicit feedback, such as the number of stars, thumbs up, or like dislike button, or it could be implicit feedback such as page views, duration watched, et cetera. Some systems will have multiple layers of user interaction that it can take advantage of because in addition to the classical rating we normally think of, there can be other interactions. An example could be user comments that can be data mined for sentiment analysis, often different types of interaction data can fill the gaps between each other to make a much better overall system. Collaborative filtering can also solve the problem of only safe recommendations, that is inherent in content-based recommendation systems. This is because not only can collaborative filter and see the user of interest points in embedding space, it can also tune in to other users points in embedding space, and find similarities between them. For instance, user A might love Sci-Fi, but it's never even thought about seeing anything outside their genre bubble. With collaborative filtering, user A is found to be very similar to user B and user C due to their shared passionate for Sci-Fi. However, user B and C also both love fantasy and action movies. So, even though those may be far outside of user A's bubble and embedding space, those might be good recommendations for them to check out. Collaborative filtering is also a great starting point. With just a little user item interaction data, we can create a quick baseline model, that we can then check against other models. It can help us find gaps to fill by using other recommendation systems, such as content-based to make up for the lack of data. Just like the rest of machine learning, is important to experiment, and find out what works best. Just like most things, collaborative filtering isn't a perfect method, and does have drawbacks. It mainly suffers from the cold start problem. This happens when we have fresh items, or fresh users. For instance, when an item is interacted with a lot of users, you'll have a very good idea of what type of users will like that item. When there's little to no interaction data for that item, we don't really have a great idea because the user sample size is so small or nonexistent. We can hopefully use the item embeddings to look nearby and see if those users sharing similarities, but a lack of interaction data could be there too. Same goes for a new user. If they have to interact with a lot of items, it is hard to make accurate recommendations. We can use averages of other users, or items, or even the global average if there is very little overall interaction data. Better yet, we can tap into the other recommendation system types like content-based to help fill in the gaps. This leads right into the problem sparsity. Remember, matrix factorization collaborative filtering takes our user item interaction matrix A, and factorizes it into two hopefully smaller matrices, U for users, and V for items, each with a dimension of the number of latent factors. It is not as easy to tell when looking at toy problems with very few users and items, but as these both increase, the number of interactions between them become very sparse. Imagine millions of users and thousands or millions of items. Even the most active users will interact with only a small sample of items, and even the most popular items will usually be interacted with by a small subset of users. This can lead to scalability problems later on as well. Lastly, when a collaborative filtering is pros, also leads to one of its cons. It's great that no domain knowledge is needed, but then we have no domain knowledge in our model, which can usually be pretty useful. This lack of context features can reduce the performance for collaborative filtering models, and usually leads us to combining our model with others like content-based recommendation systems. We also learned in module one about knowledge based recommendations, where we either take data from user surveys, or entered users settings that show users preferences. One way of doing this, assuming it is legal and ethical for your model, to use user-entered data such as where they live, their age, their gender, et cetera, we use these to try to find similarities. For example, with age, most children will be more likely to prefer what other children prefer, rather than what the elderly enjoy. We can also ask users. When building knowledge-based recommendation systems, we should keep this in mind when designing them and the point of entry of data. This could be asking users what type of movies they enjoy, what types of food they like, what activities they like doing, et cetera. This could also be asking users what they don't like, so we can filter that out. A great benefit of knowledge-based recommendation systems is not needing to have user item interaction data. We simply can rely on user-centric data to link users with other users, and recommend similar things that those users liked. This also doesn't require human-generated information about the items, which is usually expensive and hard to generate well, while substantial help of many domain knowledge experts. Also, knowledge-based recommendations use data that is of high fidelity, because the user of interest has self-reported their information and preferences, and we can fairly safely assume that those are true. This gives us a much more trust in the data because rather than implicitly like some other data, users are explicitly telling us the things they like and don't like. Unfortunately, knowledge-based systems don't work well if users don't select their preferences or set their properties. Just like all machine learning, a model is going to really struggle if there's a major lack of data. The lack of user data, however, can motivate how we design our collection processes. Perhaps, we were asking users the wrong questions or weren't asking the right ones. Maybe we didn't create enough fields in the profile page for users to fill out their information. Maybe users don't feel comfortable sharing their preferences with us, and we have a messaging problem. As you can see, there can be a myriad of problems, but there are many possible solutions as well. As mentioned before, a lack of user data can be due to privacy concerns. As with all machine learning, we should all act as responsible data stewards and not just have the right messaging, but also the right actions to back that up. Sometimes, privacy concerns are too great, and it may be easier to try recommendation methods other than knowledge-based. After going through all the strengths and weaknesses of these three recommendation system types, the next question is obvious, how can we keep the strengths and get rid of the weaknesses? Well, fortunately, there's solution for that. That solution is using hybrid recommendation systems. These might sound more intimidating than they actually are. They don't have to be super complex and can be rather simple. Imagine training content-based, collaborative filtering, and knowledge-based recommendation systems that each make a recommendation for an item for a user. All three of these models might recommend different items, and some predictions maybe better than others due to things like data size, quality, and model properties. A simple way to create a hybrid model is to just take things from each of the models and combine them all in a neural network. The idea is that the independent errors within each mile will cancel out, and we'll have much better recommendations. We will soon see several examples of this. Let's now test our knowledge. We've refreshed ourselves about three popular types of recommendation systems and the pros and cons of each. We've also touched on how hybrid models can use a combination of them to produce even better recommendations than each separate model could on its own. If we have only the following data to recommend items for users to buy, what type of recommendation system should we use? User ratings of item between one to five stars, user comments about experience with item, user answered questions about item, and the number of times user added item to cart. The correct answer is B. Most people probably jump straight to answer G, thinking that a hybrid model is always the answer. That's usually true, but that might not be possible in this hypothetical example. Let's go over the reasoning behind this. Let's look at our first dataset. We have user ratings of items that are scores between one to five stars. Well, this is explicit feedback of user item interactions. So, the very first thing that comes to mind is collaborative filtering, which can use a matrix factorization algorithm like Weighted Alternating Least Squares or WALS to train. This isn't content-based because this dataset is not metadata about the item, and it's not knowledge-based because it isn't any personal user or user preference data. Now, onto the second dataset, user reviews about experience with item. Well, we might first think, "Yes, it is text, so we could embed the text and use a content-based recommendation system on that." Because based on the first dataset in a collaborative filtering and the answer is, of course, use a hybrid model. We need to stop and think more closely. This isn't a description or something written by a product expert, but from a user who has interacted with it. It's starting to sound less like content-based and more like collaborative filtering, but this time, using unstructured data. We can indeed put this into an embedding and use that, or perform sentiment analysis on it and then create a user item interaction matrix out of that. It also is still not knowledge-based because the data doesn't contain any user centric or global user preference information. So, the third dataset has user answered questions about items that other users can read when judging whether to purchase an item. It looks very similar to the previous dataset by being free text. However, this feedback is quite a bit less explicit. If we were to perform sentiment analysis, most of the answers might be neutral because users are just being factual about the item. Of course, some users might slip in some praise or dislike into their answers, but it might not be enough to go on. We can, however, use this as an implicit feedback. Because we would assume that if a person is responding about an item, they have one or more of their own, and like it enough to answer questions for others. Remember, implicit feedback usually involves an assumption which can turn out to be wrong. Perhaps, users dislike the items so much that they go out of their way to warn people in their answers to their questions. This still probably isn't content-based because it is a user clarifying the product details to the best of their ability and not a product expert who definitely should know the ground truth. Once again, this is not knowledge-based because there's no user-centric data. Last but not least, we have a dataset the number of times users edit items to their online shopping cart. Once again, this is user item interaction data, so we'll use collaborative filtering. However, let's analyze the type of feedback it is. So, the first thought that jumps out at us is that it's implicit feedback, because it's not explicit rating. The user is not explicitly telling us how much they liked or disliked the item, therefore, it must be implicit feedback. This is true, but it could also be explicit feedback if this dataset includes whether users checked out. We can make the assumption that if users add an item to their cart and then purchase it, not just once but multiple times, they must like or at least need the item. There are probably not many users continually buying items they don't want or need. Again, this isn't content-based because it's not metadata about the item. There's still no user, only information in this dataset, so there's not a knowledge-based solution. Of course, if we had data about the items themselves, and data about our users, and/or their self-reported preferences, a hybrid approach would probably give us the best results. We still wouldn't choose answer E because it excludes collaborative filtering, which assuming our four given datasets are good, we definitely would use in a hybrid model.