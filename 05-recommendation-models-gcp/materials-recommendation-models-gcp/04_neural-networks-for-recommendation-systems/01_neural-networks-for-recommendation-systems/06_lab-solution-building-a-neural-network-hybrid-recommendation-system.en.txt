All right, welcome back. Now, we're going to combine the labs before, where we did content-based and collaborative filtering-based recommendations combined it together with the neural network in a hybrid recommendation system, using once again the Google Analytics Data. First, we're going to preprocess our data to combine this together and then we're going to create our combined model. First we're going to preprocess our data using BigQuery and Cloud Dataflow to be used later in our neural network hybrid recommendation model in the next notebook. Apache Beam only works in Python 2 at the moment, so we're going to switch to Python 2 kernel. In the above menu, click the drop down arrow, and select Python 2. We're going to first switch into our Python 2 environment, uninstall Cloud Dataflow, reinstall Apache Beam here. [inaudible] once you're done with this step to reset the notebook session kernel so that way all of this can be pulled into the current session. As always, we're going to import our OS and our cloud train demos here for our projects and our bucket, and we're going to have that such our environment variables change as needed. We're going to set our project in our compute region and now we're going to create our ML dataset using Dataflow. Remember here, we're going to be combining the user and item embeddings learned from the WALS Matrix Factorization Collaborative Filtering lab that were extracted from our estimator and we're going to combine that with our content-based features such as title, author, category for instance. Okay? We've created our hybrid dataset query here, using a couple of CTE tables to do little bit of the work for us. We're pulling out the full visitor ID here, the content ID, the category, the title, the author's in array here of when users looked at this content, years and months, and any other customer dimensions. From there, we're going to process this a little bit within BigQuery. Remember, you can do preprocessing within BigQuery itself. We're going to use as our label, because remember that's the neural network, it's supervised learning, we need to have a label, something to predict, and we're going to be using this next content ID to predict. We're going to try to predict the next thing that this user will- should read and for training what they actually did. So, that'll be our label and then some of our input features can be visitor ID, content ID, the category, the title, the author, and then the month since the epoch. Epochs usually started from January 1st,1970. Just to try to get some kind of date range because of contexts, what possibly could have been going on at the time in the world when these people were reading? Maybe that's going to give some variance on why people chose what they did read. All right. From there, we have our final function where we're going to pull all that in and make sure that's all the right data types and replace any nulls because it hazes any strange nulls. We're all going to combine this with our the user factors and our item factors. When I trained this, I train this using a number of embedding dimensions, my latent factors as 10. So that's why I have 10 for users and 10 for items here. But if you chose a different memory, you'll have a different number of things to bring in. Okay? I also am going to create a hash ID here so that I can do good sampling on to get a good reproducable or repeatable results and either join with my user factors that I pulled out of my estimator and I import it into BigQuery and likewise with my item factors. Okay? So, let's look at what some of these look like. As you can see, I'm dumping this BigQuery query and putting a little limit on here so I don't have millions and millions of records going into my Data lab. I'm going to put it to dataframe, and you can see here's my label and that's content ID. Once again, it's a big integer. Visitor ID: A really big integer. Content ID: The current content that run. So it's the current content reviewing and this is the next thing that person viewed. The category: News et cetera. The title: These all look to be in German, so that'll add a little bit of complexity for those Non-German speakers out there. The author, of that article they read. The months since epochs, so that way we can tie a little together in time, so perhaps what was going on in the world at that time when there were reading it. Then our user factors and our item factors. Okay. Do it describes if we can kind of see some of the statistics of this. Are the averages enrolled. These are just have our numeric columns as you can see, just our user factors, okay? Our item factors. I'm going to call Apache Beam to do some more pre-processing. I want to create sharded files in this case, CSV files. So to do that, I'm going to create my CSV column names, so it'll be next content ID, my label and then my features, visitor ID, content ID, category, title, author and the months since the epoch. Then my factor columns, I'm going to create a list of these. This list comprehension, two of them in fact, one for user factors and then for item factors, looping across all 10 of them for each. Remember, this 10 will be a different number for you if you use a different latent number of factors in your collaborator filtering model using WALS. So, I'm going to write out the rows for each of my inputs. So I do have to be careful here of my encoding since I do have some German characters in there, that might be special. So I need to use a UTF-8 encoding here. Otherwise just cast it into string, you might get some errors and same goes for doing my other features from factor columns. When that's all done, I have one long string with commas deliminating everything and I'm going to yield it all out into one big string for every table row that gets pulled in for this row dictionary. Alright. Now I'm going to call my pre-process function and I'm going to pass in whether I'm in test mode to use local preprocessing or not in test mode to use the full power of the cloud, autoscale, elastic, take advantage of that. Look at a job name and I'm going to determine where I put these things. Map directory for local, will be local to my Datalab whereas if it's not a local training and I'm using power of Cloud Dataflow, I'm going to have it put into Google Cloud Storage. Alright? Set some options here and then I'm going to say if in test mode, direct runner, Else; dataflow runner. Alright? I initialize a beam pipeline here using my options and the runner that I designated here. I'm going to pull my query and my query is going to be the hybrid dataset from above where those CTE tables in that final feature set with my label of next content ID. If in test mode, I'm going to concatenate on a limit here so I don't pull in too much data and then I'm going to create a loop. So, for training and evaluation, I'm making both datasets right now. So, if I'm training, I want to take the modulo of the absolute value, the hash ID. Remember we have to take to modulo positive numbers here. So, since the foreign fingerprint can create negative numbers, I want to be able to get absolute value of that. I'm going to break it into 10 buckets. I did some analysis beforehand just to verify this is a good sampling and each bucket was approximately 10 percent the data. So, therefore I can take less than nine, so I'll get values zero through eight and for the last bucket, my 10 percent, I'm going to put in eval. You can choose obviously different splitting, maybe you do an 80 20 split rather than 90 10 split, it's up to you. So now I take it to the initial pipeline I created for each training and eval as you can see here when I pass in the step from this list that I'm enumerating through and I'm going to read from my BigQuery source using my query, using standard sql. From there, I'm going to call my flatmap to CSP and then I'm going to write out my CSV from memory for each of the work announced into my file at the output directory I designated. That's it. I'm going to run the job and I'm going to call my pre-process to call that entire functional and it'll create my graphing Cloud Dataflow and create all these features for me. As you can see here, I want to move it over just to do some some local checking here and let's look inside. As you can see, I have three rows, I'm doing a head on this and you can see that I have- here is the next content ID, user ID, current content ID, the category, the title, the author. Could be none in case there's no author. The months since epoch. So, since 503 months, since January, 1st 1970 and then the 10 user embeddings and ten item embedding values. Alright? That's the same for eval and all the trained shorted files we have here. We made three of them in this case. All right, I also want to create vocabulary is using Dataflow. So, I'm going to read from BigQuery, do some pre-processing and write it out to CSV files. Okay, so we're going to write out vocabulary files for our categorical features. So, here I'm going to create a query. I've put some values in here that I can then replace depending on the vocabulary I want to generate. And I'm going to do a group by on there so I get a unique list. I'm going to create another Apache Beam job. I'm pulling this in where I'm going to do group by as you can see. That's what this is going to be named as, Group By. So it's my column name. All right. So, I'm going to do that and make sure you get the encodings again, because in case you have any weird things and category or title or author, any of these other features we could possibly find vocabularies of, we want to be able to handle those unique characters outside the standard set. All right, I'm going to call our pre-process as before except now we're going to our vocabs folders since this is not our features, these are our vocabularies. And I'm going to call as before my BigQuery. I'm going to write it up to my text file above using my function I created up here, and this will create my vocabulary list. And then I write it out to my text files, with the associated name that gets passed in. To look at how I call this vocab list I pass the index. This index goes here up in the query, right up here, so it replaces that index value. So, that way I get the correct piece of content from this nested table. Content ID is number 10, category is going to be number seven and author has an index of two. All right, so these are the three vocabularies I'm going to generate to get each unique one. Also aligning the vocabulary counts from the length of vocabularies. To do this I can get to count to text, for counts, that way I can count the number of unique words in each vocabulary and also I might want to get some aggregates like means, or medians or maxes, mins for instance. So, I can create a mean to texts that write out the mean value from a query. Okay, everything else is pretty much the same except now I'm putting it into a vocab counts folder instead of vocabs, but everything is more or less the same here. I create a function called vocab count where I pass the index of the thing I want to count or aggregate, and then the column name. So, that way I can create that in the file name. I create a query here where's the simple for vocab counts is a simple count star as count number so, I'm grouping by over the entire table and just counting all the rows for that vocabulary that I passed in right here. Okay, as before I write out to a text file vocab count where I replace depending on if it's category or content ID or author I place that there. Also if I want the global column mean, here it's very similar except instead of doing a count star I'm doing an average, where I'm casting as a float for instance if it's my mean, my mean epic months I can figure that out by replacing these right here with my format. Call off the beam job and does its thing, down here if you want to do vocab count I'm going to call that function with the index for content ID, the index for category and the index for author. To get the global column mean I want that for months since epic so, I'm going to use this later on as my my default value in case there's a null for months since epic I will use this value rather than setting it to zero or something else. I'll create my job, run it and then I'll call all of that all above my pre-process call and I'm going to do false, so therefore I'm going to launch a cloud dataflow job to create the whole graph, autoscale all that great stuff to build these things. As you can see here, I can pull those in and look at the head of these. Here's my vocabs. So, here's the top three things my vocab for author. Here are the three authors names. I also have it for categories, so it looks like there's three categories listed here. Lifestyle, stars and culture news, and also three of the content IDs. My counts here will tell me how many I have of each of those. So, it looks like my vocabulary for authors only 1103 unique authors, and category is only three. So,this was apparently all of the categories in the table. Content ID looks like there's 15,634 unique piece of the content in my data set. Then lastly the months since epic mean it looks the average is 573.6. So, we'll use this in the next notebook to train our model now that we have all of our data pre-processed. Moving to that notebook, we're going to now do our build our model and we're going to train it. We're going to train locally, we're going to make a module and train it locally again to check the models correct and then once that set, we're going to send it off to the cloud do a small training, and then we can create some hyperparameter tuning to then try to find the best values for some of the things that we're going to use, and then lastly now that we know the best hyperparameters, we're going to plug that in and do a really big cloud job to have a good training model. All right, for this since we're going to be using titles, we're going to embed this. So, we're going to use rather than trying to learn our own embeddings or pull them in another way. We're going to use TensorFlow Hub. Makes it much easier just to point to an embedded and we'll load it in and we can train from that. Okay, so first we're going to install that and make sure you have your kernel to three now because we're no longer using Apache Beam. We want to use back to Python 3, since that's where things are going. Okay. So, make sure you save that and reset the notebook after you did the pip install to make sure that it's all inside in this current environment now. Import everything set your project bucket region, put that into your environment variables. Don't forget to import TensorFlow hub, since we'll be using that. Set your project and your compute region of course. And now we can get to actually building our model. So, the first things first we want to pull in some of our global variables for counts for instance. So, with that we can import a file I/O from TensorFlow's library to do that we can point to our vocabulary count, files and means. So, here's kind of an ID rating it in, great we now have it within scope the actual number 15,000 something. Same for categories, so now we have three and author. We have the 1100 and here and then here's our mean. Our mean months since epic. It's going to be 573. So, now we're going to determine our CSV and our label columns here. So, we have our non-factor columns, this is our non-user factors and factors from collaborative filtering. This is essentially going to be our labels and our content-based features. So, here's our label next content ID which we set down here as well. Our visitor ID, our content ID, the category, the title of the content, the author of the content and the months since epic that this was interacted with. That's a little bit of context. We also create our factor columns here. Okay, so we're going to have our user factors, all 10 of them and also our 10 item factors. Our CSV columns would be the combination of a non-factor and our factor columns. We also need to set our default values for each CSV column in case we don't have the values for them when read them in. Remember it's a list of lists. So, for each one of these, these are all strings. So, this would be called unknown and mean months since epic we're going to read in from above right here. So, we'll replace it with the mean rather than with zero. Our factor defaults are going to be a list of lists and we're going to create a list comprehension here to do a little fancy condensed way for users and for items. Just like before with the actual keys, we are going to concatenate the non-factor default that list with this factor defaults list to get all features in there. So, we have our standard CSV input function, not much to go through here. So, we have decoding, creating our features dictionary, creating our single label tensor that gets called by first getting our file names, creating a data set that we're then going to map our decode CSV function onto. If the mode is training, you're going to have an indefinite number of epics. We're going to actually use our train steps to determine how many times we train rather than how often we read and of course we want to do a shuffle so that way things are nicely split and we won't end up in any strange area parameter space over tuned in that respect. Elsewhere, and at one epic and no shuffle if we're doing evaluation. We're going to repeat and we're going to batch it up, so that way we can return, and then return our input function. So, here are some new stuff we're going to create our feature columns using our read and features from our input functions. So, we're going to create our column ID or content item ID column here. It's going to be categorical with a hash bucket. Just in case there's any new content IDs that might come about, we want to be able to have some room for there. So, we're going to increase that size. We also might want to embed this. So, remember this is a deep neural network. So, we need to make sure we have dense columns here. We cannot have sparse columns going through there. Not only is it not allow but, too many zeros could end up killing our network since there would be too many zero activations and therefore the gradients would also becomes zero and therefore our model will just not train and that's not good. So, we're going to embed it. We're going to pass in our category column here and we're going to give it the dimensions, basically how many dimensions we have that we want to set a command line. We're going also create our category feature column here. It's going to be categorical. It's going to use our vocabulary file from before. So, it's going to read in the full vocabulary and we're going to have an auto-vocabulary bucket here just in case new vocabulary come along, they can end up in that bucket. We have to convert it once again into a dense column. So that's what we're going to do with the indicator column here. So, it's going to pass the categorical we created and call indicator to create our indicator column. Next feature is title. So, title's going to be using TF hub. So, we're going to call TF hub. It could be a text embedding column here. We're trying to embed the title text. The key is going to tireless, our raw data key, the module spec here, I'm pulling in this model here it's going to be using German because this are German titles. Otherwise it'd be EN for English or other things ES for Spanish such stuff. Used to be a 50-dimensional embeddings, there's also 128 dimensional one. Don't want to go too big, you might over fit but don't go too small because it might be under fit. Say to find what works best for you. I'm going to have with normalization here to try to get better training. We're going to set this to false trainable so that way these will be fixed and we're going to use these embeddings to convert our titles into numbers rather than trying to fine tune these at all. But if you wanted to and enough data, you can set this to true to try to get some better results. We're going to take our author columns, it's going to be category column with a hash bucket and you use a author as our key from our row and our number of authors and we're going to add one and hearing case author is none, in case we had a missing author. But remember that's categorical, so we need to embed it into a lower dimensional representation that's specifically dense that can go into a DNA. We're going to do that using embedding column here with a number of author embedding dimensions that you create at command line. We're also going to create some boundaries for a month since epic boundaries. So, we're going to go 400 as our min, 700 as our max and we're going to move in steps of 20 for our buckets. So, we're going to pull it in raw. It's going to be numeric column. It's a float so we're going to use numeric column. But we're going to bucketize that using our boundaries that we created. So, here's our source feature column now and here's our boundaries you created using that list. We also might want to cross some of these. So, we want to across our category column with our bucketize months because perhaps certain categories were more used during certain months because certain things were happening in the news. So, it might be a useful feature. So, we'll use a cross column on these two categorical features. They both have to be sparse and we're going to have a hash bucket size of the number of boundaries possible with a number of categories plus one in case there's any out of vocabulary. But remember, this right here is sparse, so we have to create a dense layer. So, we're going to use an indicator column to basically create dense out of this crossing. Last but not least, we can't forget about our train walls model that created our user and item embeddings. So, we're going to read this ionist as numeric columns, we're going to use them as floats and that's as much as to it. Finally we're going to create our total feature column lists, all these features combine together as well as our user items factors and our item factors and we'll return that from a function. Now that we have that, we're going to create our model function. So, from here the to do that you're supposed to have done, you need to create an input layer to your neural network, create hidden layers of your neural network and also create your output layer of your neural network all using the correct things and linking to our labels correctly which will be next Content ID. To do that simply would pass in as a parameter here. Remember in custom estimators you want to use the parameters as essentially your window, your portal into the model function. So, it's a dictionary so I can pass in many things. So, for instance feature columns that we created above, I'll pass in and I'll use that in my input layer. These were my features. To create my hidden units, I'm going to loop through the hidden units that are passed by command line and create a loop and don't worry about renaming, these will automatically orienting themselves to net zero, net one, net two. So that way we don't have to worry about any collisions here. It'll pull the units out of the list that I've provided and of course we want non linearity so user Rilu. Use other things if you want incase you're having problems like you lose et cetera. Lastly, our final layer is going to be the dense layer. Where we are going to take the final hidden layer as our input. The number of classes since we're trying to predict which next content you should read. So, it's really the number of pieces of content from that vocabulary and we'd have no activation here because there are all logits. We'll get to predictions in a moment. Now that we have our logits, were going to find the predicted class indices based on the highest logit which you can think of if I squash it down with a Sigmoid or softmax, it's going to be the same value regardless if I use logics or not. So, I find the arg max of my longits along the final access. We don't want to X equals zero because that's going to be batch size and we don't want to choose that. So, this will find us our predicted classes for each example in the batch. Then we are going to read in our content ID vocab to get our content ID names. We're going to gather the names with indices. So, this TFI gather using the parameters of the column ID names, we're essentially just creating a mapping from predicted class index to the name. Therefore, we can return the names which might be more useful to a user rather than some random enumerated content ID. Okay. If in prediction mode, we're going to create a predictions dictionary returning the class IDs, the class names, the probabilities which come from a softmax, that way they're normalized probabilities, so the sum of them all should equal one, and the logits. From there, we create also expert outputs so that way in case we want to actually create an exporter and a certain input function to send it out which we will be doing soon, we can actually serve predictions to those model and have predictions get passed back to us. All right, if we're all done with predictions, we can return the estimator spec with the current mode which to be predict, the dictionary we created here for predictions, there should be no loss, no training, no eval, remember this a prediction mode here, okay. Then our export outputs. All right. So that was an early return, if you have survived this far, you are not in prediction mode, you're either training or evaluation. So let's see what we do from here. So, we're going to create a lookup table using our content ID vocabulary that we pulled in from before. Okay, so we're first going to pull in our content ID vocabulary and create our vocabulary file to be used in our index table. Okay, from here, we can quickly look up the mapping between the indices and the actual names of the content, the content ID which is that long integer. Okay. To do that we use table.lookup and the keys are going to be our labels. All right? So, now we're going to compute the loss on this using the sparse softmax cross entropy since this it's classification and our labels which are the content ID indices and our probabilities are mutually exclusive. So now that we have this, we can use sparse softmax cross entropy because we are going to choose our labels which is going to be mutually exclusive, so it has to be either I read this content or I read this one. You couldn't have read a mix of the two, so that's why we can use sparse here because this label is just an index of the ID which came from our table lookup here. All right, we're going to create some accuracy metrics here, accuracy and also the top k accuracy so that way we can see how accurate the top 10 predictions were, some email metrics, some scalars, right? This a custom estimator, so if you want to be able to see those things in tensor board, you can always look at custom scalars, so we'll write these out. All right, and if we're in evaluation mode, we're going to write out an estimator spec with the mode, no predictions, a loss, no train, this is evaluation mode, the eval metrics and no exports in this case. That's another early return. So, if you're still with me at this point, we're now going to be doing training because that's all that's left, we've already filtered out the prediction mode and the eval modes, now were in training mode. So just to make sure we're going to have an assert here to make sure we're in training mode before we do any changing, so we don't want to change accidentally the weights that we've trained of our neural network by accident by being in the wrong mode. So, we're going to create an optimizer here. Deep neural networks are at a ground works great, you try Adam or other things and we'll pass it our learning rate. Finally, now that we have our train up created, we're going to minimize our loss that we said above and set our global step. We're going to return the training estimator spec or a pass mode, no predictions, the loss, train op. Since we're in training, there's no eval metrics and there is no export outputs. There we are, that's our custom model function. Now, if we do want to be able to serve for those model on Cloud ML Engine, we're going to need a serving input function then we can put in our exporter. To do that, we're going to have our feature placeholders, okay, using our column name and strings and we're going to go through our non-factor columns. Remember, we're going to skip the zeroeth index here because that's our label, our next content ID, and at prediction time, you shouldn't have that because that's the whole point of predicting, we're trying to predict what that value is. We go to a negative one here because we're not going to include the months since epoch because that's a float, not a string. To add that in, we're going to create a new key called months since epoch and that one will be a float and that will finish off our feature placeholders for our non-factor columns. For our factor columns, they're all floats, so we're going to have a simple list to add those columns, those features to our feature placeholder dictionary. For the actual features, we want to expand dims so that way rather than being scalars, they are going to be now vectors. If you were to print out the shape here, I'd be question mark, comma, so they're vectors of that size. We're going to return our input receiver and we're done with our serving input function. Now that all the pieces are assembled, let's create and run our train and evaluate loop. To do that, let's set our login verbosity to info so we can see some interesting things printed out as it trains. We're going to have your train and evaluate, where we had the number, the argument dictionary coming in here which gets passed from our task.py file from command line. We're going to create our estimator here, it is going to be a custom estimator, so estimator.Estimator. There are customer model function, our output directory is going to be passed here and then our parameters are going to be passed through that window, that portal that we created, feature columns, hidden units, number of classes which will be the number of content IDs, learning rate, the top k, and then the bucket. Okay, that's going to be our Google Cloud storage bucket. All right, we also create our training spec, right here, press in our training input function. So therefore we're going to have our train data pass, the train key, okay, and then our max number of steps which will also come from the command line. Our eval specs are going to be using our eval input function, so make sure you have your eval data paths here. And I have no eval steps, so it will just do the entire file. Since number of epochs was sent to one, we're going to have these pulled in from our command line and our exporter is going to be here. Okay, so it's about our latest exports or every time I make a check point, we're going to export a saved model. There we go, we call our train and evaluate loop using our estimator, our train spec our eval spec as usual. Great. Now we can run the train and evaluate function. So, we're going to first set our output directory where you want to train it locally and we are going to remove that file tree so that way we don't have any collisions or any problems and I'm going to create my arguments dictionary. So enormous would come in from our tasks.py from command line but in this case we're doing it locally before the module. So we're just going to create this so it's nice and all collected together, and we're going to call our function and it will train. Now that that has worked, you can run the module locally. So in red are our requirements, we want to make sure that TensorFlow hub is installed. Otherwise we're getting an error when it tries using it and then we're going to call our module here our task.py and our model.py. Okay, so I'm going to point to our module, our folder and I'm going to point to our preProcess features for training and evaluation and set all of our hyperparameters as we had above in addition. Once that has worked and you got no errors, now you can take full advantage or run on Google Cloud ML Engine to get the full power of cloud, scale out as many workers as you want. As before, we're going to call an output directory, it can be our small trained model here, so we will do a bigger one later. We're going to make sure you start off fresh. I'm going to submit a cloud training job. We're going to have our region, our module name where our module save that so can branch to cloud, the package path where our scale tier. So how big do you want to make this? And then the current TF version and then all of our normal hyperparameters as before. So, if you want to add some hyperparameter tuning, you can write a yaml file. So for instance, I might want to maximize my accuracy for instance here, you can change the number of trials and parallel trials and the parameters here for instance batch size or might do a linear scale here for instance. Maybe my learning rate is every floats doubles. So I have a linear scale, categorical, I want to try different sets, different permutations of hidden units for my DNN portion of the model. Likewise with my content ID embedding dimensions and author embedding dimensions. You might have thought, oh, maybe perhaps 10 was a great idea but maybe actually it was 17. So therefore by using Cloud ML Engine with a hyperparameter training, we can actually find the much better values for that. Now, okay. Moving on, you call off your hyperparameter tuning job. Don't forget to include your hyperparameter yaml that we have here, okay, and it will reset all those values and be able to figure out the best ones. Now that you know your best hyperparameters, now we can run the big trained model and that will run our module and pull it in and usually you want to use a bigger number of steps using the best hyperparameters that were found before. That is pretty much it. That is how you will do a hybrid recommendation system using content-based and collaborative filtering embeddings that we've been trained using walls, and put it all together you get even a more powerful model that each one separately.