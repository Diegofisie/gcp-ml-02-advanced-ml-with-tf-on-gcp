Great. We've narrowed down our original huge dataset of videos, to only hundreds of candidates. Now, let's deep dive into the ranking neural network. Now that we have are hundreds of candidate videos, that have been optimized for high precision to ensure high relevance, the ranking network uses more tailored features to make sure that user will like the videos. The first step, is to take the video suggest to the user. These get combined with the videos watched by the user, these both get used as individual video embeddings, and as an average embedding. Both of which are used as input features to our ranking neural network. We then add hundreds of features, including language embeddings and user behavior. Why use a language embedding of both user language and video language? The reason to use this language embedding, is to consider language pairs. For instance, Germans tend to watch English videos, but not vice versa. By having this embedding, we're able to capture this difference. These input features all feed through the ranking neural networks layers, the output of the DNN classifier is the ranking. During training time, the output is a weighted logistic function, whereas during serving, it as just a logistic. Serving uses logistic regression for scoring the videos and is optimized for expected watch time from the training labels. This is used instead of expected click, because then the recommendations could favor clickbait videos over actual good recommendations. Remember, it is important to not only use the right model architecture, but also use the right data, which includes the labels. For training because we are using the expected watch time, we use the weighted logistic instead. The watch time is used as the weight for positive interactions, and negative interactions just get a unit weight. Because the number of positive impressions compared to the total is small, this works pretty well. Now that we have gone through the second half, the ranking neural network, let's test your knowledge. For the ranking neural network, why do we use a weighted logistic regression? For training. Is that the loss function is more numerically stable than a normal logistic? Is it that positive and negative impressions need to be weighted differently? Maybe the probability is going to be formatted in special ways, maybe it reduces user noise better, or maybe it's just none of the above. The correct answer is B. Because the labels are expected watch time, we want to wait videos based on their watch time and also based on their impressions. For positive impressions, we weigh the output probabilities by watch time, and for negative impressions, we weigh the output probabilities by unit weight. We don't do this during serving, because our model has already been trained unexpected watch time. Furthermore, because these are new videos, the user hasn't watched them. So, we don't have the actual watch time to weight the probabilities with.