There are three main types of context-aware recommendation systems or CARS algorithms, there's: contextual prefiltering, contextual postfiltering, and contextual modeling. Let's go into more detail for each type. For contextual prefiltering, we start with our user by item, by context tensor that contains ratings. In traditional collaborative filtering recommendation systems, we have just a user-item interaction matrix that contains ratings. We then apply a filter on our context before the traditional user-item interaction matrix recommendation system, which is why it's called prefiltering. As an example of this, imagine we are looking at movie theater tickets online, and we hope to go Thursday after work. Therefore, the algorithm will filter out all movies that don't have the time dimension of context equaling Thursday. However, we need to make sure things don't get too specific, for example, this user wanting to go to this exact theater on Thursday with a friend from work, where there will be special effects glasses to wear during the movie. That is a really specific contexts, and there are probably going to be sparsity issues. We can also use context generalization, where instead of saying the user wants to see a movie on exactly Thursday, it's a blend of Thursday and weekday for instance. After the data has been filtered by context, the data is once again our usual user-item interaction matrix, which we can use in our traditional collaborative filtering recommendation systems. Know that a contextualized data is filtered by certain dimensions of context, so it's actually a subset of the original data. Our contextualized data now goes through a traditional two-dimensional collaborative filtering recommendation system. This is a big plus because there is no need to develop and implement new algorithms to handle this multidimensional contexts in our input tensor. As we've learned, our recommendation system will simultaneously learn the user, and item embeddings that we can then use to make recommendations. Next, as we learned in module three, we now apply our user vector to the embeddings learned by the recommender to get a predicted rating for each item the user hasn't already interacted with. Lastly, for contextual prefiltering, we return the top k item recommendations to the user. Many different contextual prefiltering algorithms have been developed. Because this is contextual prefiltering, they all are different methods of representing and splitting the data, so that can be used in a traditional two-dimensional recommendation system. We'll take a deeper look at item splitting, user splitting, and the combination of the two, with user-item splitting. We've talked about how context can change our perception, and our sentiment of the same item. Therefore, it isn't unreasonable to come up with a data representation model where we split items into item contexts pairs. Here's an example of a user-item context table with the associated ratings. This example, only has one dimension of context, time. Let's briefly analyze it with inspection. The first thing we might notice is that there are four users and they all watch the same movie twice, but at different times. Looking at the ratings, we can see that there's a big block of very high ratings at the top. We can also see a large block of low ratings right below, this also looks really strange. What can be causing this? Let's check the context. Well, there's the difference. All the high ratings are when the movies were watched on weekends, and all the low ratings are when the movies were watched on weekdays. Because there is such a significant difference of radians between the two contexts with everything else being equal, let's split the item into two item contexts entities. There we go. As we can now see our items have been stratified across context, they're blended together. This functions as if the item was actually multiple items. This is easy to see when we have a small toy data set like this, but how would we do it for much larger and more complex data sets? We simply can use a t-test on two chunks of ratings, and choose what gives the maximum t value, and thus the smallest p-value. There's a simple splitting, when splitting across one dimension of context, and complex splitting was putting over multiple dimensions of contexts. Complex splitting can have sparsity issues and can have overfitting problems. So, single splitting is often used to avoid these issues. There's also user splitting which it's extremely similar to item splitting, except now we split along user rather than item. If we arrange our data now to be sorted by user our user-item context table looks like this. With user splitting, we're now splitting along users as if they are separate users interacting with the item. Users are basically now, user context pairs. As the name suggests, user-item splitting is splitting along both dimensions and blending context into users and items. There's a new dimension of context location added to help make this example more clear. To see what we have done with splitting, the context columns are still remaining. Here we did two simple splits; a user split along the time context dimension, and in item split, along the location context dimension. Removing the context columns, we split along simplifies this table to this, where we have user time contexts pairs as individual users, and item location context pairs as individual items. We can now send this contextually prefiltered data to our traditional two-dimensional recommendation system.