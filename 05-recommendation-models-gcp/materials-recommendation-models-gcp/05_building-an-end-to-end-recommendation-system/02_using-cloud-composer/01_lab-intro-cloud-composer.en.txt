This is the first of two labs in this module. This first one is optional, I strongly recommend it for those who are looking to build event-driven triggers into their workflows. This is especially useful for data engineers who want to automate ETL workflows into BigQuery from flat files and program and health checks using SQL. Our goal is to automatically ingest a CSV file into BigQuery as soon as it's uploaded to a GCS bucket that we specify. The use case is that we have some new ML data that's outside of GCP and outside of BigQuery like this set of CSV files here. So, here's the ideal workflow that you're going to create. Once a CSV file is uploaded to a predefined GCS bucket, it'll then trigger a cloud function, which will trigger an airflow deg which will then start up a dataflow job. Then the deg file tells dataflow where the CSV file is located and also provides the location of the actual dataflow preprocessing code to be run as part of a data pipeline. Once the dataflow job is run, it'll process through that CSV file and then move it to an archival GCS bucket for auditing purposes. Then, as the last part of the dataflow job, it'll write out the records from GCS into BigQuery, using the normal dataflow Apache Beam based functions like BigQuery sink, like you see here. Once it's in BigQuery, we could have a Cloud ML Engine job retraining an ML model, but we'll save that for the last lab in this course. So, go ahead and work your way through the event-driven workflow. Keep in mind your multiple attempts at the lab and then we'll cover the solution together afterward.