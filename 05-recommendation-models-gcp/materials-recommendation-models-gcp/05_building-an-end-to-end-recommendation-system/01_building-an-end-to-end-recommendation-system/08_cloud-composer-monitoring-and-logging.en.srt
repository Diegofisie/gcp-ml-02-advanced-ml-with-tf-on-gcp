1
00:00:00,300 --> 00:00:03,660
By this point, we've got our environment
set up with our DAGs running at

2
00:00:03,660 --> 00:00:06,260
a predefined schedule, or
with triggered events.

3
00:00:06,260 --> 00:00:09,880
The last topic we'll cover before you
practice what you've learned in your labs

4
00:00:09,880 --> 00:00:12,970
is how to monitor and
troubleshoot your cloud functions and

5
00:00:12,970 --> 00:00:14,040
your Airflow workflows.

6
00:00:14,040 --> 00:00:18,060
One of the most common reasons you'll want
to investigate the historical runs of your

7
00:00:18,060 --> 00:00:22,360
DAGs is in the event that your
workflows simply stop working.

8
00:00:22,360 --> 00:00:25,554
Note that you can't have it auto-retry for
a number of attempts,

9
00:00:25,554 --> 00:00:27,047
in case it's a transient bug.

10
00:00:27,047 --> 00:00:30,730
But sometimes you just can't get your
workflow to run at all in the first place.

11
00:00:32,260 --> 00:00:35,440
In the Dag Runs,
you can monitor when your pipelines ran,

12
00:00:35,440 --> 00:00:39,880
and in what state, like success,
running, or failure.

13
00:00:39,880 --> 00:00:43,554
The quickest way to get to this page
is by clicking on the schedule for

14
00:00:43,554 --> 00:00:45,759
any of your DAGs from the main DAGs page.

15
00:00:45,759 --> 00:00:49,579
Here we have five successful runs
over five days for this DAG, so

16
00:00:49,579 --> 00:00:51,930
this one seems to be running just fine.

17
00:00:53,255 --> 00:00:55,565
Now back on the main page for our DAGs,

18
00:00:55,565 --> 00:01:00,605
we see some red, which indicates trouble
with some of our more recent DAG runs.

19
00:01:00,605 --> 00:01:05,098
Speaking of DAG runs, you'll notice the
three circles below, which indicate how

20
00:01:05,098 --> 00:01:09,481
many runs have passed, how many are
currently active, or how many have failed.

21
00:01:09,481 --> 00:01:13,142
It certainly doesn't look good for
268 runs failed and

22
00:01:13,142 --> 00:01:15,630
0 passed for that first DAG.

23
00:01:15,630 --> 00:01:17,343
Let's see what happened,

24
00:01:17,343 --> 00:01:21,822
we'll click on the name of the DAG
to get to the visual representation.

25
00:01:21,822 --> 00:01:25,994
It looks like the first task is
succeeding, judging by the green border,

26
00:01:25,994 --> 00:01:29,920
but the next task,
success-move-to-completion, is failing.

27
00:01:29,920 --> 00:01:34,406
Note that the lighter pink color for the
failure-move-to-completion node beneath

28
00:01:34,406 --> 00:01:36,500
it, means that that node is skipped.

29
00:01:36,500 --> 00:01:37,905
So reading into this a little bit,

30
00:01:37,905 --> 00:01:42,730
the CSV file was correctly processed
by data flow in the first task.

31
00:01:42,730 --> 00:01:47,430
But there's some issue moving this
CSV file to a different GCS bucket

32
00:01:47,430 --> 00:01:49,600
as part of task number two.

33
00:01:49,600 --> 00:01:55,700
Now, to troubleshoot, click on the node of
a particular task, and then click on Logs.

34
00:01:55,700 --> 00:01:58,990
Here you'll find the logs for
that specific Airflow run.

35
00:01:58,990 --> 00:02:04,095
I generally search my browser for the word
error, then start my diagnosis there.

36
00:02:04,095 --> 00:02:05,682
Here, this was a pretty simple error,

37
00:02:05,682 --> 00:02:09,720
where it we was trying to copy a file
from an input bucket to an output bucket,

38
00:02:09,720 --> 00:02:14,011
and the output bucket simply didn't exist,
or is named incorrectly.

39
00:02:15,530 --> 00:02:16,980
Another tool in your toolkit for

40
00:02:16,980 --> 00:02:21,040
diagnosing Airflow failures
is your general GCP logging.

41
00:02:21,040 --> 00:02:25,580
Since Airflow launches other GCP services
through tasks, you can see and filter for

42
00:02:25,580 --> 00:02:26,210
errors for

43
00:02:26,210 --> 00:02:30,790
services in Stackdriver, as you would
debugging any other normal application.

44
00:02:30,790 --> 00:02:36,010
Here I filter for Dataflow step errors, to
troubleshoot why my workflow is failing.

45
00:02:36,010 --> 00:02:39,845
It turns out that I had not changed
the name of the output bucket for

46
00:02:39,845 --> 00:02:40,692
the CSV file.

47
00:02:40,692 --> 00:02:44,419
So after the file is processed
by Dataflow as part of step one,

48
00:02:44,419 --> 00:02:48,420
it dumped the completed file
back into the input bucket.

49
00:02:48,420 --> 00:02:52,438
Which you can begin to see why that's
terrible, because that triggered another

50
00:02:52,438 --> 00:02:56,352
data flow job for processing, and so
on, and so on, and so on, and so on.

51
00:02:56,352 --> 00:03:01,509
As you can see, workflows can be really
powerful tools if we set them up properly,

52
00:03:01,509 --> 00:03:06,173
or really resource intensive,
if not scheduled or architected wisely.

53
00:03:06,173 --> 00:03:09,160
One of the essential steps I
recommend is pretty simple.

54
00:03:09,160 --> 00:03:10,550
Before really scheduling or

55
00:03:10,550 --> 00:03:14,390
triggering your workflow,
try running it manually yourself first.

56
00:03:14,390 --> 00:03:17,810
And ensuring that you can get it to
complete successfully before tying it to

57
00:03:17,810 --> 00:03:20,710
a more aggressive or automated schedule.

58
00:03:20,710 --> 00:03:23,370
There's nothing worst than waking
up to a new workflow that ran for

59
00:03:23,370 --> 00:03:26,000
the past eight hours and
failed each time, and

60
00:03:26,000 --> 00:03:29,610
spammed you team with an email each time
it failed, I've definitely been there.

61
00:03:31,050 --> 00:03:35,040
You might be wondering, if there's an
error with my cloud function, my Airflow

62
00:03:35,040 --> 00:03:39,080
instance would never have been triggered
or issued any Airflow logs at all.

63
00:03:39,080 --> 00:03:39,680
Since, of course, it

64
00:03:39,680 --> 00:03:42,390
was unaware that we were trying to trigger
it if the trigger didn't fire properly.

65
00:03:42,390 --> 00:03:46,240
And you're exactly right if you're
using cloud functions, be sure to check

66
00:03:46,240 --> 00:03:50,720
the normal GCP logs for errors and
warnings in addition to your Airflow logs.

67
00:03:50,720 --> 00:03:55,010
In this example, each time I uploaded
a CVS file to my GCS bucket,

68
00:03:55,010 --> 00:03:56,730
hoping to trigger my cloud function and

69
00:03:56,730 --> 00:04:00,930
then my DAG, I got an error expecting
to find my function triggered that.

70
00:04:01,990 --> 00:04:05,400
Remember way back when I said cloud
functions were case sensitive?

71
00:04:05,400 --> 00:04:09,670
Looking for a function with capital
DAG doesn't exist, if it's looking for

72
00:04:09,670 --> 00:04:12,640
a capital D, lowercase a, and lowercase g.

73
00:04:12,640 --> 00:04:15,860
So be sure to be mindful when setting up
your cloud function for the first time.

74
00:04:17,480 --> 00:04:19,210
You made it to the end of the lectures.

75
00:04:19,210 --> 00:04:22,360
Before I release you to start on the labs,
let's do a quick recap of what we've

76
00:04:22,360 --> 00:04:25,320
covered and
the pitfalls that you can avoid.

77
00:04:25,320 --> 00:04:29,880
Firstly, Cloud Composer is managed Apache
Airflow, and generally all environmental

78
00:04:29,880 --> 00:04:33,670
variables should be edited inside
the Airflow UI, not within Cloud Composer.

79
00:04:34,860 --> 00:04:40,930
DAGs are simply a set of tasks which use
operators to talk to other GCP services.

80
00:04:40,930 --> 00:04:45,240
DAGs are written in Python and are stored
in the auto-created DAGs folder for

81
00:04:45,240 --> 00:04:48,620
each Airflow instance, again,
that's just a GCS bucket.

82
00:04:48,620 --> 00:04:52,730
Workflows can be scheduled at a set
interval, or triggered by an event.

83
00:04:53,850 --> 00:04:58,423
Lastly be sure to monitor your logs in
both airflow and in GCP for warnings and

84
00:04:58,423 --> 00:04:59,590
errors.

85
00:04:59,590 --> 00:05:03,210
Keep in mind, you can set up email
notification nodes as part of your

86
00:05:03,210 --> 00:05:06,968
workflow to alert you and your team
in the event of workflow failures.