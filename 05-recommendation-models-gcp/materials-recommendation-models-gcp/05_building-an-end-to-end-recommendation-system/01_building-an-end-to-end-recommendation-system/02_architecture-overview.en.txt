First and foremost, building a complex recommendation system involves significant data engineering as well as all the model code when you learn to build the accurate model in the first place. Building and serving product ML models is a core skill of any ML engineer. Remember this diagram where you learned about the code that in ML model is only about five percent of the whole ML system? Now, let's apply that concept and see what our actual architecture is going to look like for news recommendations. As review, here are all the pieces that we're going to review and build for our recommendation system. Recall that walls that batch predictions. So, we need to train and run the model periodically to stay fresh. Batch prediction, by the way, is perfectly fine for this type of application. Say you wait until the end of the business day after all your online news articles have been created, and then group them all together as a batch of new recommendations into our training dataset. Online prediction is hard to scale, is unnecessary for this type of problem. If you wanted to get even more granularity than once per day, you could pull a batch once per hour, flex say that top 10 recommendations for each user, and then serve them out of a database. Here's our challenge, new articles and new user behavior from Google Analytics just came in our source datasets. What do we do? You know that you first need to bring that fresh data into our ML training dataset, this involves sending tasks to BigQuery and Google Cloud Storage. Where are these tasks managed and scheduled? Through a workflow service like Cloud composer. At the end of each day, we can have Cloud composer refresh our training dataset by sending a task to BigQuery or the latest Google Analytics data lives and then have BigQuery run an export job to Google Cloud Storage. We can then trigger a new training job to Cloud ML Engine to retrain our recommendation model. Finally, that retrain model is deployed to App Engine to be available as an API endpoint to whichever service needs to make those calls. This could be our front end web site at the poles recommendation system, and displays the top five news articles for each user when they visit. Some of you may be wondering is this all works great if your dataset has already loaded into BigQuery like the Google Analytics sample dataset that you're working with. But what about if my dataset isn't even in Google Cloud Platform at all yet? Say it's in a CSV file that I just have locally, and also I don't want to run a static workflow job once a night because my ML data could come in all at once, or one hour of the day, or periodically a little bit differently, and I wanted the latest reflected as soon as possible for the model that train from. In your first lab, you'll be triggering an event whenever a new CSV file is uploaded to Google Cloud Storage bucket that you specify. What happens is you have a cloud function watch for any new files that are added to your bucket, and then immediately trigger your cloud composer workflow to continue with the rest of the processing and ingestion of that data into BigQuery, and then finally have it available in the cloud ML Engine for retraining. So, by the end of this course, you'll have two methods for running automated workflows on ML datasets. First is the regular ended the day schedule and the second is the triggered workflow. A way to learn how to work with instead of an orchestration layer through cloud composer, because that's what we're going to demo and work with next.