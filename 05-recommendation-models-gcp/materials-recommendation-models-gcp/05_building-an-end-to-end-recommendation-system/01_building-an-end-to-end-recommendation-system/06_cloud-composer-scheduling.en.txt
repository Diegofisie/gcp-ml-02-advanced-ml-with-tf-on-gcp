Now that you're familiar with the cloud composer and apache airflow environments and the basics of building a list of tasks for GCP services inside of your DAG, it's time to discuss a really important topic, workflow scheduling. As we hinted at earlier, there are two different ways your workflow can be ran without you manually sitting there and clicking and run DAG. I've done it before, it doesn't work. The first and most common is to set a schedule or a periodic run of the workflow, like once a day at 6:00 AM or weekly on Saturday nights. The second way is trigger based, like if you wanted to run your workflow whenever a new CSV data file was loaded into a GCS bucket or if new data came in from a pub sub topic you've subscribed to. To view the schedules for your DAGs, you first launch the airflow web server from whizzing cloud composer. This is a great link to bookmark. Navigate to the DAGs tab to view the existing workflows that you have Python DAG files flow. Here, you can see that we have two DAGs. The bottom one, composer samples simple greeting has a daily schedule, but why is this top DAG missing a schedule at all? How does it ever get ran? The answer is the fact that it's not on a set schedule at all, it's event-driven. The driver of when this workflow runs is a cloud function that we're going to create. In the next lesson, we'll actually create our own cloud function that watches a GCS bucket that we specify for new CSV files that are dropped in. If you wanted to go the regular schedule route, you simply specify this schedule interval and your DAG code like what you see here. By the way, clicking on this schedule of one day here in the UI won't allow you to edit the schedule but instead, it actually takes you to the history of all the runs for that particular workflow.