1
00:00:00,590 --> 00:00:02,546
Let's remind ourselves
what our first goal is.

2
00:00:02,546 --> 00:00:06,250
We need to feed our recommendation model
the freshest data that we have available.

3
00:00:06,250 --> 00:00:09,160
Ideally neither you nor I are sitting
by a computer manually hitting

4
00:00:09,160 --> 00:00:12,510
the refresh button in GCS,
a repeat low job in BigQuery.

5
00:00:12,510 --> 00:00:13,750
That's no fun.

6
00:00:13,750 --> 00:00:18,210
As you saw, Cloud Composer will command
the GCP services that we need to run.

7
00:00:18,210 --> 00:00:21,150
But Cloud Composer is simply
a serverless environment

8
00:00:21,150 --> 00:00:24,020
on which an open source
work flow tool runs.

9
00:00:24,020 --> 00:00:26,250
That work flow tool is
called Apache Airflow.

10
00:00:26,250 --> 00:00:30,450
I feel like between Tensor Flow, Dialog
Flow, Data Flow and Airflow, if you're

11
00:00:30,450 --> 00:00:35,040
going to be working on the ML package,
it's likely to end in something flow.

12
00:00:35,040 --> 00:00:36,690
Anyways, bad joke.

13
00:00:36,690 --> 00:00:39,850
How Airflow works is you write
up an ordered set of tasks

14
00:00:39,850 --> 00:00:41,860
as part of what's called a DAG.

15
00:00:41,860 --> 00:00:42,610
More on that in a minute.

16
00:00:42,610 --> 00:00:46,808
And then you watch as Airflow invokes
those other GCP services like

17
00:00:46,808 --> 00:00:49,810
the BigQuery GCS export job here.

18
00:00:49,810 --> 00:00:54,840
Or a new cloud ML engine training job has
kicked off here or deployment of a newly

19
00:00:54,840 --> 00:00:59,310
retrained or trained model, the app
engine, for serving as the API endpoint.

20
00:00:59,310 --> 00:01:02,410
Just a few of the great features of
these airflow DAGs that you're going to

21
00:01:02,410 --> 00:01:05,960
build are the tolerances for failure,
automatic retrying of failed jobs,

22
00:01:05,960 --> 00:01:09,060
and those data health checks that
I alluded to a little bit earlier

23
00:01:09,060 --> 00:01:11,330
that you can program
directly into your workflow.

24
00:01:12,420 --> 00:01:15,320
Now the heart of any workflow,
and by the way I use workflow,

25
00:01:15,320 --> 00:01:19,379
pipeline and DAG a little bit
interchangeably, is that DAG.

26
00:01:19,379 --> 00:01:24,100
A DAG, or directed acyclic graph,
is a set of tasks and

27
00:01:24,100 --> 00:01:27,270
special operators that allow
you to programmatically and

28
00:01:27,270 --> 00:01:30,840
periodically schedule
cool stuff to happen.

29
00:01:30,840 --> 00:01:34,480
Directed means that there's
a specified dependency flow.

30
00:01:34,480 --> 00:01:38,070
First do this task,
then task B, then task C.

31
00:01:38,070 --> 00:01:42,670
And acyclic means it can't be a feedback
loop into itself like a circle.

32
00:01:42,670 --> 00:01:45,240
Acyclic means non circular.

33
00:01:45,240 --> 00:01:49,130
The stuff that's happening in this
particular DAG are four tasks that

34
00:01:49,130 --> 00:01:53,920
update our training data, export it,
retrain our model and then redeploy it.

35
00:01:53,920 --> 00:01:56,760
I'm purposely being ambiguous
by using the word stuff.

36
00:01:56,760 --> 00:02:00,370
Because you can tell your DAG to pretty
much do anything that you need it to do.

37
00:02:00,370 --> 00:02:04,190
Here it's sending tasks to a Big Query,
GCS, and Cloud MLM engine.

38
00:02:04,190 --> 00:02:07,900
But another DAG could send email
notification when it launches a cloud

39
00:02:07,900 --> 00:02:11,390
dataflow job, or
spin up a Google Kubernetes cluster,

40
00:02:11,390 --> 00:02:15,000
or run multiple ad hoc queries
against a Big Query data set.

41
00:02:15,000 --> 00:02:17,820
And you could even trigger
a completely separate DAG

42
00:02:17,820 --> 00:02:20,340
based on the results of the first
one running and completing.

43
00:02:21,460 --> 00:02:24,240
Before we go too far into
Cloud Composer specifics,

44
00:02:24,240 --> 00:02:26,290
I wanted to share this useful analogy.

45
00:02:26,290 --> 00:02:29,270
Cloud Composer is to Apache Airflow

46
00:02:29,270 --> 00:02:32,160
as Cloud Machine Learning Engine
is to TensorFlow.

47
00:02:32,160 --> 00:02:35,220
Both cloud composer and
CMLU are serverless and

48
00:02:35,220 --> 00:02:37,930
fully managed integration layers
to the Google Cloud Platform.

49
00:02:37,930 --> 00:02:41,305
So, you don't have to worry about
what hardware expects your Airflow or

50
00:02:41,305 --> 00:02:42,830
TensorFlow code is running on.

51
00:02:43,870 --> 00:02:48,270
Building any workflow in Cloud Composer
consists of these four steps and

52
00:02:48,270 --> 00:02:51,970
building an ML workflow for
recommendation engine is no different.

53
00:02:51,970 --> 00:02:54,950
We'll start by previewing the actual
Cloud Composer environment.

54
00:02:56,740 --> 00:03:00,760
Once you use the command line or GCP web
UI to launch a Cloud Composer instance,

55
00:03:00,760 --> 00:03:02,960
you'll be met with a screen like this.

56
00:03:02,960 --> 00:03:06,440
Keep in mind that you can have multiple
Cloud Composer environments and

57
00:03:06,440 --> 00:03:09,940
within each environment, you can have
a separate Apache Airflow instance

58
00:03:09,940 --> 00:03:12,420
that can have zero to
many DAGs inside of it.

59
00:03:12,420 --> 00:03:15,620
An important note here is that
sometimes you'll be required to

60
00:03:15,620 --> 00:03:17,590
edit environmental variables for

61
00:03:17,590 --> 00:03:22,070
your workflows, like specifying
your specific GCP project account.

62
00:03:22,070 --> 00:03:26,600
Normally, you would not do that at the
cloud composer level, but instead, do it

63
00:03:26,600 --> 00:03:31,210
at the actual Apache Airflow level, which
is what we'll show you in the next demo.

64
00:03:31,210 --> 00:03:35,300
Again, generally I'm only on
the Cloud Composer page here to create new

65
00:03:35,300 --> 00:03:40,100
environments before I launch directly
into the Apache Airflow webserver.

66
00:03:40,100 --> 00:03:43,540
To access the Airflow admin UI where
you can monitor and interact with your

67
00:03:43,540 --> 00:03:47,340
workflows, you'll click on the link
underneath the Airflow webserver.

68
00:03:47,340 --> 00:03:50,390
The second box you see
here is the DAGs folder

69
00:03:50,390 --> 00:03:53,920
which is where the code of your
actual workflows will be stored.

70
00:03:53,920 --> 00:03:58,040
The DAGs folder for each airflow
instance is simply a GCS bucket that's

71
00:03:58,040 --> 00:04:01,440
automatically created for you when you
launch your cloud composer instance.

72
00:04:01,440 --> 00:04:04,590
Here's where you're going to upload your
DAG files, which you're going to write in

73
00:04:04,590 --> 00:04:08,780
Python, and bring your first workflow
to life visually inside of Airflow.

74
00:04:08,780 --> 00:04:11,660
Let's take a quick look at
a demo of what the simplest DAG

75
00:04:11,660 --> 00:04:14,180
looks like inside of the Airflow UI.

76
00:04:14,180 --> 00:04:16,750
So now I get to see what
Cloud Composer looks like.

77
00:04:16,750 --> 00:04:19,620
And we'll run a very simple DAG that's
going to do a little bit of a work for

78
00:04:19,620 --> 00:04:20,330
us inside of GCP.

79
00:04:20,330 --> 00:04:23,400
So, this is the Cloud Composer
environment.

80
00:04:23,400 --> 00:04:26,570
How you actually get here is
through the Navigation menu.

81
00:04:26,570 --> 00:04:28,300
All the way at the bottom.

82
00:04:28,300 --> 00:04:30,460
You're already familiar with
all the big data products.

83
00:04:30,460 --> 00:04:32,040
Maybe Cloud Composer is new to you.

84
00:04:32,040 --> 00:04:35,330
And you can see the little icon there is
just putting all the pieces together.

85
00:04:35,330 --> 00:04:38,220
I thought for a while, like what
is the T suppose to represent, but

86
00:04:38,220 --> 00:04:40,210
it's a different puzzle pieces.

87
00:04:40,210 --> 00:04:41,500
That'll take you here.

88
00:04:41,500 --> 00:04:45,132
In your lab, you're not going to have
any of these managed Apaches Airflow

89
00:04:45,132 --> 00:04:46,850
environments created yet.

90
00:04:46,850 --> 00:04:49,510
So the first thing as you'd imagine
you want to do is go to Create.

91
00:04:49,510 --> 00:04:54,182
And of course there's a corresponding
shell command line you can actually run

92
00:04:54,182 --> 00:04:58,300
using the CLI to actually create this,
but you can use the UI as well.

93
00:05:03,521 --> 00:05:06,480
So here we just give
the Composer environment a name.

94
00:05:06,480 --> 00:05:07,850
You can specify the number of nodes,

95
00:05:07,850 --> 00:05:10,460
again this is all running
Kubernetes behind the scenes.

96
00:05:10,460 --> 00:05:12,210
Mandatory that you specify a location.

97
00:05:12,210 --> 00:05:14,540
You want closest to you,
optionally a zone.

98
00:05:15,740 --> 00:05:18,360
And I leave the rest of
the infrastructure blank here.

99
00:05:19,510 --> 00:05:22,840
Now you can specify environmental
variables or other labels.

100
00:05:22,840 --> 00:05:25,190
I also leave those the default values.

101
00:05:25,190 --> 00:05:29,830
Now the big, aha moment is currently,
at the time of this

102
00:05:29,830 --> 00:05:34,910
recording each Composer instance takes
about 10 or 15 minutes to spin up.

103
00:05:36,420 --> 00:05:38,500
Here is where you can look
through the rest of the lab,

104
00:05:38,500 --> 00:05:39,740
the rest of the documentation,

105
00:05:39,740 --> 00:05:44,370
that's why I have a few of these other
environments available for us to demo in.

106
00:05:44,370 --> 00:05:48,010
So I am actually going to look at the demo
Composer environment I created a little

107
00:05:48,010 --> 00:05:49,400
bit earlier.

108
00:05:49,400 --> 00:05:50,820
As you saw before,

109
00:05:50,820 --> 00:05:54,170
you've got the actual web server
to take you into the Airflow UI.

110
00:05:54,170 --> 00:05:56,130
You've got the folder to the DAGs.

111
00:05:56,130 --> 00:05:59,189
So the first thing that we want to do
is actually take a look at the DAGs.

112
00:06:00,880 --> 00:06:03,895
Which again, is just a GCS bucket,
there's no objects in there already.

113
00:06:03,895 --> 00:06:08,230
And the DAG, what we're actually going to
be creating is part of our workflow today,

114
00:06:08,230 --> 00:06:13,010
is just going to be a BigQuery related DAG
for the top 100 Stack Overflow posts for

115
00:06:13,010 --> 00:06:14,790
a certain time period.

116
00:06:14,790 --> 00:06:17,787
And that's actually just an example
I took from the documentation for

117
00:06:17,787 --> 00:06:19,220
a cloud data flow.

118
00:06:19,220 --> 00:06:22,760
So here, you can see it's
just a simple bq notify.py.

119
00:06:22,760 --> 00:06:29,220
And it is actually this one is set up to
send an email summary based on the results

120
00:06:29,220 --> 00:06:34,100
of the BigQuery query showing what's
the most popular Stack Overflow post.

121
00:06:34,100 --> 00:06:39,170
So we have our bucket,
again this is auto-generated GCS bucket,

122
00:06:39,170 --> 00:06:40,338
uploading the files.

123
00:06:40,338 --> 00:06:44,630
I've got samples_dag.py.

124
00:06:44,630 --> 00:06:49,550
Once that's uploaded,
we head back into our environment.

125
00:06:49,550 --> 00:06:53,010
You can click on the web server or
click on the actual name.

126
00:06:53,010 --> 00:06:53,620
If you click on the name,

127
00:06:53,620 --> 00:06:56,620
it will give you a little bit more of
the metadata about the environment.

128
00:06:56,620 --> 00:07:01,670
So, you can see this is
the Cloud Composer's name, demo-composer.

129
00:07:01,670 --> 00:07:04,620
And you can see that here is
the actual GKE cluster ID.

130
00:07:04,620 --> 00:07:07,680
But the two links that were also on
the previous page that are really,

131
00:07:07,680 --> 00:07:10,460
really important is
the link to the GCS bucket,

132
00:07:10,460 --> 00:07:12,590
where we can start uploading
those Python DAG files.

133
00:07:12,590 --> 00:07:16,340
And we'll go into much more great detail
on how to create those DAG files.

134
00:07:16,340 --> 00:07:18,878
But the one you want to really
bookmark is the Airflow web UI.

135
00:07:23,924 --> 00:07:29,184
And after we authenticate, you'll
see it's thinking, and maybe refresh

136
00:07:29,184 --> 00:07:35,460
a few times and it will realize that hey,
there's DAGs inside of the DAG folder.

137
00:07:35,460 --> 00:07:38,980
Now, here's the interesting part, as I
mentioned before, you might be putting

138
00:07:38,980 --> 00:07:42,370
environmental variables that
are referenced by your code.

139
00:07:42,370 --> 00:07:48,190
For example, this says sample_dag.py is
expecting something called a gcs_bucket.

140
00:07:48,190 --> 00:07:52,990
And it's essentially a parameter for
whatever you're gcs_bucket is going to be.

141
00:07:52,990 --> 00:07:55,820
So in order to satisfy this and
get the DAG to show up,

142
00:07:55,820 --> 00:07:57,852
we're going to go to Admin > Variables.

143
00:07:57,852 --> 00:08:00,670
And here's where you can create
environmental variables,

144
00:08:00,670 --> 00:08:03,460
that can be referenced by one or
all of your DAG's.

145
00:08:05,380 --> 00:08:07,160
Here we specify gcs_bucket.

146
00:08:08,210 --> 00:08:10,559
I'm going to go ahead and
just create a very simple one.

147
00:08:16,908 --> 00:08:17,779
Create a new bucket.

148
00:08:23,969 --> 00:08:26,340
Call this one just something unique.

149
00:08:27,810 --> 00:08:28,530
Create the bucket.

150
00:08:29,970 --> 00:08:30,730
Bucket's created.

151
00:08:31,860 --> 00:08:33,850
Now we go back in here, specify that.

152
00:08:35,120 --> 00:08:39,330
Now it's vital to understand
depending upon, this is again,

153
00:08:39,330 --> 00:08:40,410
this is a variable name.

154
00:08:40,410 --> 00:08:45,830
Depending upon your code, sometimes
you're going to have to put the gs

155
00:08:45,830 --> 00:08:47,850
google storage slash slash there.

156
00:08:47,850 --> 00:08:51,340
But I just know offhand that
this one already cacatonates in

157
00:08:51,340 --> 00:08:52,230
that as part of the dag.

158
00:08:52,230 --> 00:08:53,560
So, I'm going to leave that alone.

159
00:08:53,560 --> 00:08:55,800
And we're going to save that.

160
00:08:55,800 --> 00:08:59,020
And we're going to see what other
environmental variables are required by

161
00:08:59,020 --> 00:08:59,890
going to dags.

162
00:08:59,890 --> 00:09:03,150
And then we have another one that
says e-mail does not exists,

163
00:09:03,150 --> 00:09:05,302
maybe this is the e-mail that
has sent in the summary to.

164
00:09:05,302 --> 00:09:10,306
So, we again and go to Admin > Variables,

165
00:09:10,306 --> 00:09:16,051
create, e-mail and
then we can just do whatever

166
00:09:16,051 --> 00:09:20,930
your e-mail is when you're
actually creating e-mails.

167
00:09:20,930 --> 00:09:23,820
And I'll tell you why I'm just doing
a bogus email when we get into

168
00:09:23,820 --> 00:09:24,710
actually running the DAG.

169
00:09:24,710 --> 00:09:28,930
Now let's go back in the DAG,
see if there's any other parameters.

170
00:09:28,930 --> 00:09:30,089
We need the GCS bucket.

171
00:09:34,700 --> 00:09:40,240
Our project, GCS project, of course,
we can just get from our home screen.

172
00:09:40,240 --> 00:09:43,393
For your Qwiklabs accounts is going to
be your Qwiklabs projects Id.

173
00:09:46,713 --> 00:09:50,570
And that's where we're actually going to
be running this work low within.

174
00:09:50,570 --> 00:09:54,675
Boom, after all three of those you will
see that your DAG will now appear.

175
00:09:54,675 --> 00:09:57,506
This is the composer_sample_bq_notify.

176
00:09:57,506 --> 00:10:01,202
A couple of different things about
the UI you can plus your DAG

177
00:10:01,202 --> 00:10:03,100
from automatically running.

178
00:10:03,100 --> 00:10:05,970
If you have an automatic scheduler set
up we'll go into the different types of

179
00:10:05,970 --> 00:10:08,430
schedules you can automatically set up.

180
00:10:08,430 --> 00:10:10,750
And then what I do is you
give it about a minute or

181
00:10:10,750 --> 00:10:16,660
30 seconds then when you refresh this,
you'll see the acute notification message.

182
00:10:16,660 --> 00:10:18,880
But then you'll also see all
the links that are available to you.

183
00:10:18,880 --> 00:10:21,792
So you can say this looks like
it's running once a month.

184
00:10:21,792 --> 00:10:24,278
We can take a look, what I'd like to
do is take a look at the code first.

185
00:10:24,278 --> 00:10:31,000
So you can click into the DAG, different
ways of representing it, the graph view.

186
00:10:31,000 --> 00:10:33,320
This is the directed basic graph.

187
00:10:33,320 --> 00:10:35,620
So, you can see that it's
doing a lot of things.

188
00:10:35,620 --> 00:10:39,835
Making a BigQuery dataset, querying it,
exporting the questions that GCS.

189
00:10:39,835 --> 00:10:44,808
E-mailing out the summary while
in tandem it's querying BigQuery,

190
00:10:44,808 --> 00:10:48,020
getting the most popular question queried.

191
00:10:48,020 --> 00:10:51,620
And then reading that out and including
that in the email summary as well.

192
00:10:51,620 --> 00:10:53,240
So that the DAG view here is great.

193
00:10:53,240 --> 00:10:54,650
Also, you can just check
out the code view.

194
00:10:55,730 --> 00:10:59,590
So you can see that at the very
top of this code, it highlights.

195
00:10:59,590 --> 00:11:01,510
Hey, by the way, you're going to
have to specify these three

196
00:11:01,510 --> 00:11:02,470
environmental variables.

197
00:11:02,470 --> 00:11:05,720
So if you wanted to avoid the attack
of the pop up notifications.

198
00:11:05,720 --> 00:11:10,470
You could have just read here and
see that it actually relied on these three

199
00:11:10,470 --> 00:11:12,930
environmental variables
instead of air flow.

200
00:11:12,930 --> 00:11:17,990
This is the DAG, again you can see just
a little bit of what's going on here.

201
00:11:17,990 --> 00:11:21,490
Where it's actually different parts
of the different tasks are running.

202
00:11:21,490 --> 00:11:26,310
This is the actual query, which queries
the public Stack Overflow posts.

203
00:11:26,310 --> 00:11:28,130
Don't worry about too much
of the DAG structure.

204
00:11:28,130 --> 00:11:29,890
I'm going to that in much greater detail.

205
00:11:29,890 --> 00:11:34,980
But this is just essentially a Python file
with a list of dependencies at the end,

206
00:11:34,980 --> 00:11:36,830
for what tasks get run first.

207
00:11:38,140 --> 00:11:40,340
So now if actually wanted to run this,

208
00:11:40,340 --> 00:11:44,380
one of the things we can do is go back
to our DAGs, all the way on the right.

209
00:11:44,380 --> 00:11:46,720
If we don't want to wait for
it to run within 28 days.

210
00:11:46,720 --> 00:11:50,640
We're going to trigger the DAG
with this little icon here.

211
00:11:50,640 --> 00:11:51,260
So trigger it.

212
00:11:51,260 --> 00:11:52,250
Are you sure you want to run it?

213
00:11:52,250 --> 00:11:53,540
Yes, let's go ahead and run it.

214
00:11:53,540 --> 00:11:57,680
And then quickly, what I like to
do is I'll go back into the DAG.

215
00:11:57,680 --> 00:11:58,720
I'll go into the Graph View.

216
00:11:58,720 --> 00:12:00,900
And I'll start refreshing like crazy.

217
00:12:00,900 --> 00:12:06,650
So you want to look for the colored
border in the status or the states.

218
00:12:06,650 --> 00:12:08,250
You're hovering over it and
you see the state in queue.

219
00:12:08,250 --> 00:12:09,430
It doesn't refresh automatically.

220
00:12:09,430 --> 00:12:11,420
So when you're refreshing this,
you can see, immediately,

221
00:12:11,420 --> 00:12:12,720
lighter green means it's running.

222
00:12:12,720 --> 00:12:14,210
So it's making a BigQuery data set.

223
00:12:14,210 --> 00:12:16,520
And if you're following
along very quickly,

224
00:12:16,520 --> 00:12:19,670
I just have BigQuery opened here for
my particular project.

225
00:12:19,670 --> 00:12:22,870
I'm going to refresh that and
see if it actually created it.

226
00:12:22,870 --> 00:12:25,340
Boom, there's the BigQuery
data set it just created.

227
00:12:25,340 --> 00:12:28,660
So we've got no tables in there yet, or
else you'd have a little arrow here.

228
00:12:28,660 --> 00:12:32,550
This is the airflow_bq_notify_dataset
with this suffix of a particular date.

229
00:12:32,550 --> 00:12:35,500
So let's go back, let's refresh this,
let's see where it is now.

230
00:12:35,500 --> 00:12:38,440
So now the BigQuery data set
has been created as we saw.

231
00:12:38,440 --> 00:12:42,140
It's running the bq,
one of those SQL queries that looks for

232
00:12:42,140 --> 00:12:44,050
the recent Stack Overflow posts.

233
00:12:44,050 --> 00:12:47,860
And it's going to be populating those
into two separate tables into BigQuery.

234
00:12:47,860 --> 00:12:50,510
So as you can see that just
finished because again,

235
00:12:50,510 --> 00:12:53,590
it's just a simple call to BigQuery and
BigQuery is going to run it really fast.

236
00:12:53,590 --> 00:12:55,580
Now we're going to go ahead and
refresh BigQuery.

237
00:12:55,580 --> 00:12:59,990
We're going to see how many tables do we
have, actually, in there stored, if any.

238
00:12:59,990 --> 00:13:03,780
Boom, just two queries have ran and
now have results.

239
00:13:03,780 --> 00:13:06,260
The most popular post for
Stack Overflow for

240
00:13:06,260 --> 00:13:10,110
that time period is this article
title with over 340,000 views.

241
00:13:10,110 --> 00:13:13,360
And then here are some
of the recent questions.

242
00:13:13,360 --> 00:13:17,270
As you can see, the most popular
it just takes the top one.

243
00:13:17,270 --> 00:13:19,340
That's what we have our same title here,

244
00:13:19,340 --> 00:13:21,790
and the rest as you can see
in that view count there.

245
00:13:21,790 --> 00:13:25,652
So immediately we have BigQuery data
set that didn't exist before and

246
00:13:25,652 --> 00:13:28,210
we just ran some ad hoc
SQL queries against it.

247
00:13:28,210 --> 00:13:29,840
So let's go back into Airflow and
see what we're doing.

248
00:13:29,840 --> 00:13:33,840
It's export recent questions to GTS, so
it's dumping them to the bucket as well.

249
00:13:35,150 --> 00:13:38,389
And as you can see, we have green,
green, green, green.

250
00:13:38,389 --> 00:13:41,150
Dark green means all of
those nodes have completed.

251
00:13:41,150 --> 00:13:44,740
And then, if you notice, there's a yellow
retry all the way at the end for

252
00:13:44,740 --> 00:13:45,980
the e-mail summary.

253
00:13:45,980 --> 00:13:49,050
So it looks like maybe it failed or
something that's going on with it.

254
00:13:49,050 --> 00:13:50,420
And I'll tell you.

255
00:13:50,420 --> 00:13:53,950
And the reason why I did tests, you might
be thinking that this is failing because

256
00:13:53,950 --> 00:13:56,890
you did an improper e-mail
test at test dot com.

257
00:13:56,890 --> 00:13:58,100
And that's not actually the case.

258
00:13:58,100 --> 00:13:59,462
You can expect these things.

259
00:13:59,462 --> 00:14:01,450
And clicking on a particular node,

260
00:14:01,450 --> 00:14:06,920
I'm clicking on View Log and
you can actually see the error.

261
00:14:06,920 --> 00:14:11,210
So it's saying hey there's no module or
utility named sendgrid or

262
00:14:11,210 --> 00:14:14,070
it's going to give you
an unauthorized email error.

263
00:14:14,070 --> 00:14:16,070
And that's because you
need to actually set up,

264
00:14:16,070 --> 00:14:18,010
in order to send e-mails out of GCP.

265
00:14:18,010 --> 00:14:20,910
This is something new that happened
in the last year to prevent

266
00:14:20,910 --> 00:14:23,530
spam from folks creating these bots.

267
00:14:23,530 --> 00:14:26,710
Is you need to create
an API key with SendGrid,

268
00:14:26,710 --> 00:14:28,060
which is one of the partners that we have.

269
00:14:28,060 --> 00:14:31,870
So unfortunately, we're not going to
be getting any e-mails, which is fine.

270
00:14:31,870 --> 00:14:33,960
The e-mail body would have
just simply been this.

271
00:14:33,960 --> 00:14:34,500
I'll show you.

272
00:14:37,090 --> 00:14:40,740
All the way at the bottom you can see,
hey we analysed Stack Overflow posts

273
00:14:40,740 --> 00:14:44,720
from this date which is parameterized from
this date which and has this view count.

274
00:14:44,720 --> 00:14:46,845
And then the most popular
question was this.

275
00:14:46,845 --> 00:14:52,890
It's pulling this from the GCS bucket,
the file that was dumped into there.

276
00:14:52,890 --> 00:14:56,178
But again if you wanted to actually get
that e-mail, you sign up to SendGrid and

277
00:14:56,178 --> 00:14:57,700
put in the API there as well.

278
00:14:57,700 --> 00:15:01,609
But the big success as you can see is
it it very quickly created this data

279
00:15:01,609 --> 00:15:05,660
set inside of BigQuery, with the two
tables as you see populated there.

280
00:15:05,660 --> 00:15:08,430
So it's got this BigQuery operators going.

281
00:15:08,430 --> 00:15:12,560
And then the cool part is if
we exit out of the code here,

282
00:15:12,560 --> 00:15:15,040
you can go back to the schedule and
you can see.

283
00:15:15,040 --> 00:15:20,380
All right, well every 28 days or
about every month, this

284
00:15:20,380 --> 00:15:24,870
work flow is just going to run, and
it's going to dump in a table inside

285
00:15:24,870 --> 00:15:28,340
a big query with the most popular posts,
and you don't have to do anything else.

286
00:15:28,340 --> 00:15:30,090
It will just run automatically
behind the scenes.

287
00:15:30,090 --> 00:15:31,980
And then, if you put in an e-mail,

288
00:15:31,980 --> 00:15:35,610
if the work flow every fails you can
actually have it send a failure e-mail.

289
00:15:35,610 --> 00:15:38,690
We're going to go through how to
actually create those DAG files.

290
00:15:38,690 --> 00:15:42,750
How do you set up recurring schedule or
an event-triggered schedule.

291
00:15:42,750 --> 00:15:47,830
But all you need to know of the basics of
this so far is just upload a Python file.

292
00:15:47,830 --> 00:15:49,130
Deal with any dependencies for

293
00:15:49,130 --> 00:15:52,800
those environmental variables that'll
give you those notifications.

294
00:15:52,800 --> 00:15:56,639
And then you actually get the name of your
DAG showing up here, one DAG per line.

295
00:15:56,639 --> 00:15:59,628
And then you can interact with it
with some of the quick links here.

296
00:15:59,628 --> 00:16:02,657
All right that's the Cloud Composer
of the very fast demo.

297
00:16:02,657 --> 00:16:06,260
Stay tuned for just much more granularity
for everything that you see here