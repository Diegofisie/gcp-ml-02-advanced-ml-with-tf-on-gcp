So now that we have preprocess our data and save that out to TF Records, we had the data in the correct format to be read into the needed Sparse Tensor format. If you want to be able to eventually predict items for our users, we will need a key which will be the user index. We'll also need the items that each user interacted with which will be our indices, and we'll also need the ratings of these interactions and we'll set as our values. But just how does this get into our estimator? What even are the pieces needed to get into the WALS Matrix Factorization Estimator to work? Let's look. Here's the code to run the WALS Matrix Factorization Estimator. At the moment, WALS is a contrib estimator. So, I'm using experiment and we'll be using learn runner. We need to obviously tell WALS how big our matrix is, how many users and how many items in our interaction matrix. We need to get the number of dimensions or latent factors we want to compress our interaction matrix down into for our embeddings. Of course, we also need to tell the estimator where to write out the model files. We'll need a train input function that takes our preprocess TF records, and likewise an eval input function for those TF records. Also, we'll need the number of steps or batches we are going to train for. One evaluation step, we'll want to set the minimum evaluation frequency so that we don't get our training bogged down by evaluating too often. Lastly, we'll set an export strategy with our serving input function for inference serving. Let's first take a deeper dive into our input functions so that we can see how to use our newly created preprocess TF records. So, now that we preprocess our data and save that to our TF records, our input function is going to need to read them. Therefore, let's define a function that will parse our TF records. First, our function parse TF records receives a file name and the vocab_size, which is required as we saw in the sparse merge signature earlier. For items_for_user, the vocabulary is the items, so vocab_size equals nitems. For users_for_item, the vocabulary is the users, so vocab_size equals nusers. Now that we're in the function, we first create a list of all the files at the input path with a past filename signature, including wildcards using tf.gfile.Glob. When we have our file list, we will create a TF record data set. Next, we take our TF record data set and apply a map to it, where we will decode each serialize example using a custom function we made and the corresponding vocab_size. In this case, the vocab_size for input rows is nitems and input columns is nusers. Of course, we don't want to only go through the files one time. Therefore, we will apply a repeat on our data set a number of epic times. We also know that batching is important in machine learning, so instead of using only one example at a time, we want to use a batch size number of examples. After batching is complete, we need to remap the keys that we talked about earlier in order to fix the first dimension of the rank two indices tensor of our Sparse Tensor because batching overwrote it with the index within the batch. Lastly, now that our Sparse Tensors are fixed, we will return the next batch of Sparse Tensors from our data set using a one-shot iterator. This is all wrapped by the input function which will be called by the WALS Matrix Factorization Estimator. We call parse TF records for our rows, which will be items_for_user, and for our columns, which will be users per item. This is saved in a features dictionary. We could have other features in the dictionary such as priority weights, et cetera. We returned features only from the input function because our labels are within our features as we alternate back and forth between solving rows and columns while keeping the other fixed. Now that we've written our input function and TF records helper function, let's test your knowledge. The WALS Measures Factorization Estimator takes the input rows and input columns as features. If we have our items_for_user and users_for_item TF records, which filename and vocab_size should we use for each of these features, respectively? The correct answer is C. Remember, our user-item interaction matrix has shape, number of users by number of items. Therefore, there's a row for each unique user and a column for each unique item. Thus, when looking at the input rows features it will be a batch of rows from our data set. This means we should be using the items_for_user TF record, since there'll be multiple items for each unique user. The vocab_size which is used for the indices of our Sparse Tensor for the input rows feature will be nitems because each unique user will have interacted with a subset of items from the entire item inventory or vocabulary. However, when assigning the input columns feature, we should use the users_for_item TF record because now we're looking at a unique item that has multiple users that interacted with it. The vocab_size should be end users since when making our input columns Sparse Tensor, we will use this vocab_size for our indices tensor since each item has interacted with a subset of the entire user base or vocabulary.