Great. We've now had an overview of our walls, matrix factorization estimator, input function starting from our TF record files, created our sparse central features of rows for users and columns for items. But, now, let's dive even deeper into the input function starting with an examination of our custom decode example function. We've seen the overview of our TF record parsing function. Let's dive deeper into some of its components, namely the custom functions starting with the decode example function. We call our decode example using a data set map where x is our example protos and our vocab size is either the number of items or users depending on which phase of walls we're in. We first want to specify the schema on the TF record files using a features dictionary into either fixed length or variable length features. Let's say that we're decoding the item TF record and the key will be the item index, which is just one value, therefore it is fixed length. The indices will be the user indices and there are variable number of these because some items might have very few ratings while other items may have many ratings. Lastly, the values will be the ratings which will be the same variable length as the indices. Using the feature map dictionary, we will now parse the features from the example protos one example at a time. Now, we want to use tf.sparse_merge to easily convert our indices n values into a SparseTensor using the vocab size of the number of items or users depending on which phase wall it's in. Having this SparseTensor format representation of our data is a great choice since our matrix is extremely sparse due to a very high number of users and items. This way using the vocab size, we only need to keep track of the interaction pairs that happened and that all of the interaction pairs that didn't through the indices tensor that corresponds with the values tensor that we set up when writing our TF records during pre-processing. Remember how we mentioned earlier that the batching process will replace, for instance, the item ID in the first dimension of the sparse tensors indices where the index within the batch. Well, here we want to save the key into a tensor so that we can use it to remap the indices after bashing is complete. Because this is a map, we are mapping the example protos in our dataset to SparseTensors. But, we need some way to not lose our key, so that we can fix the indices later, some place to store the key. A solution is to take the SparseTensor created by the sparse merge operation and create a new Sparse Tensor. But, concatenate the key to the end of the indices tensor within. Because the indices and values tensors need to be the same length, we also concatenate zero to the end of the values tensor as a dummy placeholder. This way the key is splice into our mapping, which we can then extract later to fix the indexing. We are finally done designing our SparseTensor and return it to complete the dataset mapping operation. Now, that we've gone through the decode example function, let's see what we have learned. In our decode example function we use VarLenFeature indices and values of blank. We perform a sparse_merge because of blank and we concatenate the key because of blank. Choose the answer that best fills in the blanks. The correct answer is A and our decode example function we use VarLenFeature indices and values because of having many ratings per row column. For instance, when predicting items for users, each user will probably have a different number of items they've interacted with. The indices and corresponding ratings, the values. We perform a sparse_merge because the walls matrix factorizations input function needs the inputs to be SparseTensors. We concatenate the keys to the indices tensor because batching overwrites the first dimension of the indices with the batch index. So, we use this trick and the remap keys function later to correct it.