So, now that we've learned a lot of details of collaborative filtering, it sure seems powerful. However, it is not without its drawbacks. It's great that no domain knowledge, is really required. We are just using the explicit or implicit feedback of the interactions between users and items. There's also serendipity. Imagine that you are on a movie site, and only watch Sci-Fi movies, because you like them. Instead of always recommending Sci-Fi movies do you, which might get a bit repetitive, by looking at what other users who watch Sci-Fi also watch, the site might recommend a couple of fantasy movies or maybe some action movies, because those other users also like those types of movies. Collaborative filtering is also a great starting point, to get a baseline model and then iterate, add more interaction types and move on to hybrid recommendation systems. However, collaborative filtering can have a problem, when recommending items to new users or recommending users to brand new items. This is the cold start problem. If there is an interaction data through other users with that item or other items with that user, is hard to make similarity measures between them. Also, basic collaborative filtering only uses interaction data to find latent factors, for users and items within it, that we can use to make recommendations based on similarity. However, we are not able to add contextual features, that we might know are important or at our expert knowledge. This would lead us to need hybrid recommendation systems, that use all three. The cold-start problem, users, or items with a low number of or no interactions, is a major drawback of collaborative filtering. To fix this, we can do a hybrid of content-based and collaborative filtering, depending on statistics for the data, as seen in this flowchart. For instance, if the total number of user ratings is less than or equal to 100, we don't have enough interaction data to make good collaborative filtering recommendations. So, we will use content-based recommendations using data about the items that we hopefully do have. If there are enough tot user ratings, we can move on to our next decision point. In this case, does this current user have more than ten ratings. If not, then we should use users similar to our user for the items we are trying to score. Otherwise, if our user does have enough ratings, we can use them for these items. This is a very simplified approach, and we'll see in the next module, how we can use neural networks combining content-based, collaborative filtering, and knowledge based, all into one powerful hybrid model. So far, we've seen most of the walls matrix factorization estimator. Let's now take a look at the serving input function in more detail. Our solution and serving input function to give back user factors for all items, given a user ID. We read one integer into the user ID placeholder. We then create an enumerated range of all items, which will be the item indices. We then tile the past user ID across all of the item indices. We create our ratings tensor with a rating for every item. These are just dummy values, and will be ignored. We just need to have the right format for the input SparseTensor. We stack the users and items tensors along columns. So, that will be a rank two matrix, with our input user ID, being propagated for every item ID. We then create our Sparse Tensor using the rows as indices, and our dummy ratings as our values. Notice that our tensor shape is number of users by number of items. Our input rows then, is the SparseTensor we just made, and the input columns will be ignored. Remember, we're serving predictions here, not training. How does our wall solution handle giving back item factors given an item ID. It checks whether user ID is less than zero. If it is, it uses item ID. Otherwise, uses user ID. It does this using a conditional, tf.code. Check out the code again to see for yourself, and there we have it. We've explored most of the walls matrix factorization estimator. Obviously, there is still a lot left we could talk about, such as passing the estimator row and column weight tensors. But for now, we already have seen how it can power collaborative filtering recommendation systems. Walls is thus a way to get user and item embeddings, that are trained simultaneously, and then can be used for inference. These embeddings are created solely from user behavior. We didn't create any special features or expert knowledge, we just use what users interact with. However, it would be nice to also use knowledge about the item, like properties of its content, and knowledge for the user, which is knowledge based. How would we combine multiple predictors? Well, we will see the next module that we can use a neural network, which is a much better solution than the flowchart we saw previously. Now, let's test your knowledge. If we want a recommendation system that uses collaborative filtering, what are some strategies to get around the cold-start problem of fresh users and items? The correct answer is E. If we have a fresh user, we could use averages of all the items to show the most popular, until that user interacts with items and generate some data. If we have a fresh item, you could use averages of all the other users, to give the item and estimated rating, until users eventually interact with the item and generate some data. If we have a fresh user and a fresh item, we could use the global average rating between all users and items. We can also use item information to create a content based model and or use information to create a knowledge based model, which can be combined with a collaborator filtering model to create a hybrid recommendation system. Additionally, we could use other user item interaction data. Many systems have multiple ways users can interact with items. So, if one way has missing data, we can use the other slices of data to fill in some of the gaps.