Now, that we have seen WALS in action, let's now see what a more productionized version of WALS looks like in this demo. In our later module, we'll be building an end-to-end productionized and automated solution using a larger Google Cloud platform ecosystem. For this demo, we'll be using the WALS TFT notebook. Let's get started. All right. Now, we'll be doing collaborative filtering on Google Cloud Analytics data, but this time operationalized. We'll be using TensorFlow transform to create transform functions that we're going to apply to our data instead of doing it the other way. All right. This notebook is going to use the WALS measure spec or refactorization approach to do collaborative filtering. However, we'll be doing TensorFlow transform to carry out the pre-processing. This way, these steps are automated. We're going to do a map, visitor ID, a string to a user ID, enumeration of indices, we're going to map content ID a string to item IDs also an enumeration of indices, we're going to remove already viewed items from the batch prediction output and we're going to replace the user ID and visitor ID in batch production output by a visitor ID and content ID. Therefore, we'll now be able to know how they fit back to our problem. First thing's first, Apache Beam currently only works in Python 2, so we're going to switch to Python 2 kernel in the above menu, you can click the drop-down and select Python2. Okay. Then we're going to make sure that we have the right environment here because Apache Beam is only currently for Python2. We're going to activate our Python2 environment and install Cloud Dataflow, install a couple other packages like Apache Beam and TensorFlow transform. Only certain combinations work at the moment, so we're going to choose this one and after that we're going to just check to see what the versions we have are and we're good to go. Before we get started though, make sure you reset the notebook session kernel, so that way all that cell can be pulled into this session. All right, moving on. As usual, we're going to import your OS and set our project bucket in region and we're going to put those into our environment variables in Bash. Make sure you set your project in your compute region, so this can work, and import TensorFlow since that's what we'll be using. The first thing we're going do is going to create our data set for WALS using TF transform. For collaborative filtering, we don't need to know anything about either the users of the content. Essentially, all you need to know is user ID, item ID and the ratings that the particular user gave that particular item. This is our user item interaction matrix. In this case, we're working with the newspaper articles from career.at. The company doesn't ask their users to rate their articles, however, we can use the time spent at session duration on each page as a proxy for rating. Normally, you'd also add a time filter in this, for instance, the latest seven days or something, but our data set itself is limited to only a few days so we don't have to. So, just as before, we create our query here to pull all of our data in. Okay. We have our visitor ID, our latest content ID, it's just the last piece of content they watched or read I mean. The session duration, how long they were on that content and we're pulling it out of this nested table here. Okay. We then get the full visitor ID and then we sum up over all the durations, their total session duration ID and we do a little bit of filtering here where I make sure that the latest content ID is not null because we can't be using null content. I'm going to do a group by full visitor ID and that content ID to get the full duration for that piece of content in case they've seen it multiple times. Also, you want to make sure that session duration is greater than zero because zero or less than zero would make no sense and we're going to order by latest content ID, so that way we have it in order of use. All right. We can pull that into a dataframe here. As you can see here, we have visitor ID, Content ID, and the session duration. Okay visitor ID, really long, integer here and content ID also a fairly long integer. Not going to be very useful for our matrix since they have to be matrix indices, but these are the human readable way of values. Also session duration here, we're probably going to want to scale that, probably something between zero and one. Okay. We want to write out our requirements, so that way when we're training locally, we're going to be using the correct package of TensorFlow Transform version 0.8. Now, we're going to create a Apache Beam job in Cloud Dataflow that will use TensorFlow transform operation that will create a transform function and transform our raw data and metadata into the ones that we're looking for. So, input our package as before, we're going to have a pre-process TFT in this case, rather than without TFT. It's going to be pulling from a query and as we know big query returns a table row and table rows are basically row dictionaries where the keys are the names of the columns and the values are whatever they were in those cell elements. We're going to set the median here that we found by using quantiles. To save us a little time, we just hard coded it for now. The results are going to be user ID, an item ID and the rating of course. Here, we're scaling the ratings so that way it's between zero and one using the media. Here's where we cap it, so any ratings that go above one will be capped down to one. Great thing about TensoFlow transform is you can use TensorFlow operations in the Beam code. So, we can mix Beam and TensorFlow, get the power of both to get us the best results. Right here, we're just making sure that if anything is greater than one, we're going to set it to one in our rating and then we return our result. All right. This all gets called by the pre-process function here that takes in my query and whether I'm in test mode are not, test mode's going to be a local run using direct Runner, and a non-test mode, when I want to do the full thing, I'll be using Dataflow runner, so I can have a full Cloud Dataflow job kickoff. Import some important libraries here and we're going to have a function to write the count out. This will be helping to figure out vocabularies for instance, vocabulary counts, any other aggregate stuff I can use TensorFlow transform to do. To create this pipeline, I first initialize the pipeline here called A and then I add pieces to pipeline trying to build this pipe out to feed my data from the initial source to eventually the sync to the bunch of P transforms in between. You can see here the first app I'm mapping my inputs to creating one one here, then I'm combining the counts by key and then I'm taking the key values out of that and I'm mapping it to just the values to return that and then lastly, I'll write it out to my file here with the filename that's provided. Okay. I'm writing TF records out just like we did before, but now I'm using TensorFlow transform. So, I also want to get all these extra assets like my transform function. Essentially, all that it's going to be is a key value list I'm getting with my index columns here so I'm making a tuple, and I return my key, my indices and my values. This is my feature map essentially of how I'm going to have the data arrayed in my TensorFlow record. I create a job name here obviously, since I want to be able to track this. I use Stackdriver or any other tools within Google Cloud to see this all. If it's in test mode, I'm going to launch a local job whereas if I'm in non-test modes, I want to do the full thing, I'm going to have my output directory not be local, I'm going to have it be on Google Cloud Storage at this path with my bucket that I'm passing in. Okay. So, I set some options here in max number of workers, so that way I can cap it off. The great thing about Cloud Dataflow is it's elastic so it can scale to whatever my problem needs. But if you don't want it to scale forever and ever, you can always put a max there so that way it won't. As I mentioned, if it's in test mode, we're going to use Direct Runner so that way it'll be a local run whereas if it's non-test mode, I'll have Dataflow Runner to actually create a cloud dataflow job and I can browse and see all the great graph it makes and everything and see what's going on. All right. I do have to set up my metadata schema here. So, this is TensorFlow transform. To do that, I'm going to essentially create column names from my list here, visitor ID and content ID as I loop through these in creating this dictionary. My values here give me my value schema. They're all strings, visitor ID and content ID are strings here. Okay. It's going to be a fixed column representation because there's a fixed number, it's one string per column, so I don't have to have any variable length in. I do want to update my schema here, it's a great way how you can add things to this dictionary through a schema I'm creating because session duration is not a string, it's actually a float. Therefore, we can't do it all in one fancy loop if we have to have two. So, we're going to loop through here and create the session duration. Once again, it's one session duration per column so it's going to be a fixed column representation here as you can see. Lastly, now that it's all done, I have my data schema created, I'm going to call dataset_metadata.DatasetMetadata to create my schema and I'm going to pass it as raw data schema here. That's the raw data, I'm going to use this to create the transformed data schema which will be different slightly. Okay. Now that I have that, I can run my Beam job. Creating the pipeline and inside I can pull my query and from above on a couple of cells above. If I'm in test mode, obviously, I probably don't want to have locally on my VM pull millions and millions of rows in, so I'm going to add a little limit, I'm going to concatenate the end of that string at limit 100. From there, if test mode or not test mode, I continue on as you can see the indentation changes. Raw data is going to be my pipeline P that I started with here. I'm going to add a piece of pipe to it, I'm going to read from my BigQuery source using the query that I have in standard SQL. From there, now, I have my raw data. Remember it's a pipeline, it's a Beam pipeline, so I'm going to take the tuple of that with my metadata. Remember that comes from my schema and I'm going to put that altogether into my raw data set. So, it's my data combined with the metadata about that data. Okay, the types and the length of the features et cetera. Now, is the cool part. Here's where I'm actually going to create my transformed data set. From there, I'm going to take that piece of pipe that I've left off on raw data set and now I'm going to add in a new piece of pipe where I'm going to analyze and transform the data set using my pre-processed TFT function up above. Now, remember, when we're doing training, we do analyze and transform that data set whereas an evaluation and prediction, we're going to have transformed data set. Because we're going to use the metrics and vocabularies that we find from the training data set in the evaluation impression, we don't want to recalculate those. Okay, and that will create this tuple of my transformed data set and a transform function that I can then take that function and apply it to the other data sets without having to recalculate all this up again. All right. To then take that, I have my transformed data and my transformed metadata which comes from my transformed data set. I'm expanding that tuple. Also, now that I had that transform function, I'm going to write it out so that I can use it in my TensorFlow code pull the function in. To do that, I'm going to name it this so I can find it nicely on my graph. I essentially just call transform_fn_io.WriteTransformFn and I point to where I want to put it in my output path. Okay. I'm going to name it transform function. Great. Now that I've got that, I'm going to do a group-by to create users_for_item and items_for_user. Okay. So, remember, each item might have multiple users and therefore multiple ratings and likewise each user might have multiple items and multiple ratings for those items. So, I create a pipeline for each using my transform data. To do this, I take my transform data and I'm going to create a map for items node into my cloud dataflow graph, where I'm essentially just taking the item and I'm mapping it to the item ID. Then I group by that on that key and then I'm going to take these and create my user list and write that to a TF record here of my user list for each item. Likewise, I do the same for items _for_user except this time rather than trying to map the item IDs and create a list of those, I'm doing it for user IDs. So, for each item ID we'll get a user list for that item. Lastly, I 'll transfer my dataset, I'm going to have to have an output schema. What's that going to be in my TF record? Have they had that feature map? We created something for my input, now I'm going to do it for my output. So, here I have my key. So, my key here it's going to be an integer, okay? It's either my user ID or my item ID depending on which piece of the pipe I'm in, and it's going to be fixed column representation, I only have one per row. Now, however, the indices and the values, the indices are also going to be int 64 and these will be in case the key is users, it will be items in this case, and if the key is item, it'll be users in this case. As you might see, it's going to be a list column representations, same for value since these have to be in sync with each other. The reason being, like I said, each user can have multiple items that they've interacted with, and likewise for items. Values of course and we float 32 we've scaled the session duration into floating-point integers between zero and one, so now we can use this and output that. Now that I've created this, I have to actually write it to the records. So, to do that, I'm going to use my users for item section of pipe that I have here, and I'm going to write it to a TF record at the output path, with this file name, with this coder here which is going to be example proto coder, using the output schema that I defined, likewise, same as for items for user. All right, I'm also going to write out my count for the users for items so that way I can get the number of items and the number of users, so I don't have to recalculate those later. Okay. This is basically just the number of items total in all my data and the number of users. That's it, I call my preprocess on that query, I'm saying test_mode false I went to fold asset please, it's going to write all my TF records. Of course you should run the next cell once it's all done to make sure all the records have been written out, and you can check out what's inside your transform function and all the assets it's created. So, to summarize, what we should have here is you should have a users_for_item it's going to contain all the users and their ratings for each item in a TFExample format. The items_for_users here are integers; they are not strings, they were strings, but now we've converted them. Item ID not content ID, for instance, and user ID rather than a visitor ID. The rating is also scaled. Likewise, we would have an items_for_user, where it's going to contain all the items and ratings for each user now, and it's going to be in a TFExample format. The items and users here are integers, they used to be strings, but they're not anymore. So, for instance, we converted content ID to item ID and visitor ID to user ID, and the rating has also been scaled. Lastly, from that right count, we have the vocab items and the vocab users so that way we can have for both, for the mappings. All right. Now that we have that, with our transform function written and all of our data process in TF records, we can now actually train with WALS. This is fairly similar to what it was in the non-operationalize WALS. So, I'm not going to go through all of the code here, but we'll just look for the parts that are different now, all right? So, we have our protos, we have our decode example as before, we have our remap keys so that way it can fix some of what batching does with the indices, and with my parse TF records here, okay? I'm going to read in my file, I'm going to get the file path all possible files in case there are shards, create a dataset out of that. I'm going to map my decode example with my vocab size passing each serialize example, and a repeated number of epoch times depending on if I'm in training or evaluation mode. I'm going to have a batch of those, batch of records and lastly fix my keys and then create it and return. That gets pulled from my input function here, where I'm parsing my TF records, items_for_users and users_for_item and protects row so that way I can either have a recommendation model that I'm recommending items for users or retargeting model, where I'm recommending users for items and that's it; I return features and none because my labels, as we talked about before, they switch. I keep my rows fixed and solve for my columns, and then I keep my columns fixed and solve for my rows. So, I'm going vice versa, features and labels between the two. Then lastly, I return my input function since that's what my customer estimator wants. All right, find_top_k as before, I am going to try to figure out this using the matrix multiplication, I take the top k items out of there, I'm going to cast those back as integer since they are indices in fact. But we have a little bit of changes here as you might see. In batch prediction, I'm going to create my lookup. In here, I'm actually pulling in from my assets here for my transform function that we've learned through the Beam, TensorFlow transform job we made, okay? This way I can do a simple mapping to use my vocab or my items and my users to create my original item IDs and the original user IDs. These are the things that are human readable; it's a visitor ID and content ID. So, that way, I can type directly back to my original problem and that the enumerations we created to be able to use WALS. Okay. To do next, I create a session, pull my trained model back in and I can pull my user factors and my item factors back out using my estimator.get_rows_factors and get_column_factors. They should be respectively number of users by the number of embedding dimensions we used and the number of items by the number of embedding items we used. Okay. So with those matrices back, and I'm going to take this with our top k, we're going to call our function in a map functions so we're going to keep doing this for each user for instance since this is going to be a recommendation model here. We're looping through users and we'll get the top k per each user that we send in our batch. All right. Then here's where we do our lookups that we pulled in from our transform function, it's a simple mapping because thankfully user ID and item ID are enumerated. There are already indices so therefore we can just point to that value that index in the list, and we'll get our originals back, we don't have to do any fancy HashMap or anything like that. All right. Training as usual, nothing's really changed since we did it for the normal WALS, all the same, create train_steps, create experiment function since this is contrib. All right. Create our experiment. Inside of that, we have our WALSMatrixFactorization estimator, we pass it to a number of rows or columns, the number of embedding dimensions we want to use where we want to write out the model files, our input function for training, our input function for evaluation, number of train_steps we want to do, the number of evaluation steps, and also how often and we want to evaluate, okay? We want to do it often enough that we have good metrics to maybe plot or check, but not too often that we're always on evaluation mode and rarely our training. From there, we call learn_runner and run it out and train it up and get our batch prediction. So, from there what we can do is call off our job, we're calling train_and_evaluate function above. Make sure that you remove all prior files so that way you don't have any collisions or we don't pick up where we left off, we want to start fresh. Once that's all done, you'll see in your WALS_trained local folder we made here, you'll see your batch prediction.text here, and this now will have the correct visitor IDs and content IDs here rather than the enumerations that we would've gotten before without that nice mapping, our checkpoints or our events file for Tensor Board, our graph, all that stuff. As you can see here, here are user IDs. There are those large integers again, that's great as we wanted and k in this instance we had it set to three. So, here are the top three pieces of content with their content IDs. Notice these are not user ID and item ID anymore, they're enumerations, these are back to the raw values that we can now tie back to our problem. Now that you have that done, you can run as a Python module the same code, copy and paste it all into module format into a test.py, model.py, call that off, and then if that all works perfectly and no errors or anything like that, now you can run it on Cloud, go for a large number of epoch series, so you go through the dataset many many times, very fast scale it all out, it's great. This took about 10 minutes for me, of course if you increase the number of epochs it'll take a little longer, but you can get really good training set. You get the row and column factors, you can pull them out by creating a session like we had above, and then you can return those matrices and you can print them, and again you can plot and see how they look in the embedding space. That's it. I hope you enjoyed using Hash for transform, it's really powerful, it helps operationalize this model, and make it much more scalable and do a lot more powerful things with it.