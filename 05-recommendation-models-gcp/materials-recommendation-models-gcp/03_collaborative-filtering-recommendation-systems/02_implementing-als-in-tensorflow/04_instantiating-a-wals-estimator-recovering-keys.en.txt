We now know how to decode our TFRecord example protols into SparseTensors, but we're not done yet. After we repeat and batch our decoded examples, we then have to remap the stored keys to undo the batch re-indexing. Let's take a look at how that works. Remember several steps ago when we concatenate the keys to the indices tensor within our SparseTensors, well, now we finally get to use them to fix the re-indexing that happens during batching, where the item ID or user ID gets replaced by the example index within the batch. We first want to map our current dataset that just finished batching to a new dataset that has the fixed indices. So, we call it dataset.map and pass our batches SparseTensors to our custom remap keys function. Once in the function, we want to store the incorrect indices and values from the batches SparseTensors into their own tensors. There is a lot of tensor manipulation in this function and we don't want to overwrite the wrong thing and/or we might need some buffer space. So, we will use these two tensors later in the function. Next, we want to groupby the batch indices and get the count for each. This way we will know how many instances there were per batch example. We do this by creating a tensor of all ones for each batch example as the segment data and a segment sum, and using the same slice of bad indices tensor as a segment IDs to group into which are just the batch indices. Effectively, it's just a groupby operation of batch index with the aggregate count of number of users or items for each. Don't forget to subtract one from the segment sum because we don't want to count our concatenated keys and the totals. We also want to get the number of batch indices. Normally, this would just be the batch size, but in the event of a partially filled batch, it will be smaller and we need to know this number. This length is essentially just the first dimension of the shape of our grouped by size tensor. We need to know the offsets of our actual data from the keys with a key inserted after every chunk of real data. Which if you remember, our variable length. So, there's no simple way of priority to know where to slice or skip. To aid us in this, we can find the cumulative sum of the size tensor which can help us with a global re-indexing of the batch. Additionally, to know the assets between each example in the batch due to our storage solution of concatenating the keys in decode example, we will need to create a tensor that has a range from zero inclusive to the number of batch examples exclusive. Tf.range makes us easy and we can use our length tensor which is just the number of batch indices to create this range. Lastly, in this segment of the function, add in the length range to the cumulative sum will give us the cumulative range, which are the indices of the fake data we added to the SparseTensor by concatenating the keys in decode example. We want to get the key values we stored within the indices tensor. To do that, we first use a tf.gather with a bad indices tensor we made at the beginning of the function, from the input SparseTensor as our params and using the cumulative range we calculated earlier as the gathers indices. We slice the tensor only to keep the second dimension because the first dimension is the batch index, which we definitely don't want and in fact are trying to get rid of, and the second dimension has our key. We then squeeze the tensor to get into the correct shape of a vector. For us to be able to remove our concatenated into rows, we first need to know how many rows we have in the batch total, and then enumerate the range of row indices. We do this first by finding the shape of our indices tensor and taking the first dimension. We then use tf.range with that scalar we just found as our limit to get the enumeration. After all that work, we now have all the pieces to find the row indices of the actual data and can drop the key rows from the indices tensor. We have created two sets of indices and in our Venn diagram, we want the opposite of the intersection. First, we're going to copy some old tensors into new ones that we can play with called x, which will be the sparse indices enumerated range we just made in the last step, and s which is the cumulative sum range which are the indices of the keys. We want the indices of the not keys. We will be doing some tiling. First, we need to calculate the number of multiples we are going to tile x, our sparse indices range. We do this by first finding the shape of x and finding the shape of that will essentially be the number of dimensions of x, and then creating a tensor of ones of that shape. This is concatenated with the shape of s, our cumulative sum range which is just the number of keys. Now that we know how many times we want to tile x, let's tile it. First, we're going to expand dims x into a rank two tensor by adding a dimension to the end. We then are going to tile that tensor a tile multiple number of times calculated from above. This then gives us a rank two tensor with enumerated indices titled multiple times one for each key. TensorFlow is great for doing a vectorized math. Now that we have this tiling, we're going to perform our each tile slice for each key simultaneously. First, we create a Boolean tensor with each vector gets a true at the index of the key and I'll false elsewhere. We then use a tf.reduce any operation on the last axis, which essentially acts as a logical or merging all the Boolean vectors into one where any element that is true corresponds to a key index. Remember, we want the not key indices. So, we need to negate the following tensor, so that the true elements will be the elements that are not the keys. We can perform this negation by adding the till day unary compliment operator which will flip all the zeros to ones and vice versa. What to do with these Boolean mask that give the not key indices? We can apply them to one of the original tensors we created, the bad indices tensor, which has the original indices tensor that got messed up during batching. We can simply use tf.boolean mask to apply it to our tensor, which will return the indices from our actual data and drop the keys that we can cut into it in. Do we do all of this work to just throw the keys out anyway? Not at all. We'll be using them in just a moment but first we had to extract the actual indices data. Of course, this is all being put back into a SparseTensor, where the length of the indices and values tensors have to match because they correspond with each other. So, we will apply the same Boolean mask except this time to the original values tensor instead of to the indices. This will return our actual value data and drop the dummy zeros we added to the value's tensor back in the decode example function. We're getting close to the end now but we essentially have only returned to the state we would have been in after batching if we never concatenated the keys. So, now it's time to make all of this effort worth it by inserting the keys into the right spot and dropping the batch indices that overrode them. We will need to replace the first column of the slotted indices tensor we just created from our Boolean mask with the keys over the batch indices. This will not be done through tiling, however, it is not as simple as the last time we used tiling. This is due to the variable length of the data. Some items might have one user interaction, some might have five, others 10. The same is true for users who have item interactions. This jigging nature doesn't quite work well with TensorFlow or rectangular tensor structure. So, we'll need to have to improvise. We need to know what we're going to tile and how many multiples we're going to make it. First, we expand the first element of the gathered indices tensor, which are our key indices, by adding dimension to the end of that. The multiples will then be the first element of the size tensor, which is the groupby batch index counts. This is just the very first tiling to see the while loop we're going to do later. We had to repeatedly apply this tiling to each example in the batch. However, because there is a variable length of users or items, it is jagged and we cannot use tf.map function due to how it internally does the stack in the tensor array. So instead, we're going to create our own version of tf.map function by creating a loop body that we will use in a while loop in the next step. First, we define a loop body function that takes the loop index as the first parameter and the tensor that we are going to be growing as a second parameter that eventually will contain the tiled item IDs or user IDs from our stored keys. We will return two values, the loop variable incremented and the tensor we are eternally growing by concatenating the previous tensor with the next tiling by sliding along the key indices and batch index counts tensors. The loop body we just created won't run by itself, so we need to call it using tf.while loop. We call with our conditions, so that it loops over the full length of our tensors using the loop body function we created and using our initial loop virus of one for our loop variable i and our initial one shot tiling we did to the spec. After the loop completes, it will return the tensor we grew through repeated tiling as our new tensor named result, which will be our key values, item or user indices expanded out for the correct number of examples that correspond with. We're near the end. We have two correct but separate tensors. One contains our keys, one contains our data that is associated with each key. This is the step in which we finally fix the overriding of the item ID or user ID by the batch index that happened during batching. To do this, we will concatenate the tiled keys we just made with the second column of the indices that we fix earlier of the actual data. We had to expand the dimensions of each tensor by one at the end of each sensor and then we concatenate along columns. Awesome. All we need to do now is put everything back in SparseTensor format, so that the walls input function can use it correctly. The indices will be the indices we just fixed in the last step by concatenating the tiled keys with our fixed indices. The values will be the values that we fixed by removing the dummy zeros we added in the decode example function. The dense shape will be the same dense shape of the input SparseTensor to the remap keys function. We're done. Return the remap SparseTensor and we will return to the parse TFRecords function scope, where we will call the one-shot iterator and then go back up and scope to the walls input function for input rows and input columns. This was a lot of work to remap the keys back into the input data SparseTensors but it was completely essential. Without it, the walls matrix factorization estimator, will not perform the necessary sweeps because the batch index keeps resetting back to zero every batch and the model will not train. With this, the sweep perform correctly and the model will learn from the data. Wow, we now know how to remap the key is to fix the re-indexing by the batching process. So, let's see how much we've learned. Here is sample data for items for user with the following schema, user index, item indices, ratings. Here are the first two batches of SparseTensors pass to remap keys with batch size equals two. Match the tensor elements to what they represent based on color. The correct answer is D. The red highlighted elements is the indices tensor are the batch indices, which are the first dimension of the indices tensor. Notice that the values range from zero to one because our batch size is two. So, there should only be two values. The green highlighted elements are the item indices and the indices tensor, that is what we wrote and read in our indices with the TFRecords. The blue highlighted elements are the user indices in the indices tensor. This is what we wrote and read in as a key with TFRecords and concatenated these to the indices tensor in the decode example function. The yellow highlighted elements are their ratings. This is what we wrote and read in as values with the TFRecords. The purple highlighted elements are the dummy values. We can cut into this to the values tensor in the decode example function to ensure that the indices and value sensors had the same length. The remap keys function wants to replace the batch indices with the corresponding user indices and drop the added elements of the user indices and dummy values. Here at the bottom is what the SparseTensors will look like after the remap keys function fixes them. Notice that the batch indices have been replaced with the correct user indices and we've eliminated extraneous rows with a user indices and dummy values.