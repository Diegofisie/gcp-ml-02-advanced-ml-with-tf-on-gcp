Excellent. We have our sparse tensors fixed now after our remap keys function. There are a few items of business left to take care of before we are done with our WALSMatrixFactorization input function. Namely, how many times we should repeat and if we should add additional features. At the top of our parsed_tfrecords function, we need to determine the number of epochs we're going to go through, because we are using num_epochs in dataset.repeat. As usual, if we are in training, we want to cycle through the dataset over and over until the estimator, which is the number of train steps. For evaluation, we want to run through the dataset only once. Usually, when we think about recommendation systems, we think of a model that recommends items to users, whether that is items to buy, places to visit, or movies to watch. But why should we always choose a user and return items? Couldn't we also provide the transpose since WALS serves both users and items simultaneously? We could be developing a targeting model, in such models we might be trying to decide which users to send coupons for an item to or who bust a call for pulling, et cetera. To flip our recommendations from users to items, we can add a feature to our input functions feature dictionary called WALSMatrixFactorization.PROJECT_ROW. This is a boolean tensor that if set to true, will protect rows and if set to false, it will protect columns during inference. So, now that we've gone through the entire input function, let's see how it fits in with the estimator. Here's our train input function where we pass the train estimator ModeKeys, so that will go an indefinite number of epochs through the dataset until we reach our number of train steps calculated above, and here's our eval input function, where we would go through the dataset only once. Remember at the moment WALS is a contrib Estimator, so I'm using to experiment and learn_runner. The current implementation of WALS handles batching, but not distribution. When it is updated to also handle distribution, it will move to core. The goal is that all core estimators distribute to multiple machines without any code changes. Here's the call with an experiment to see the setup for WALSMatrixFactorization estimators. We still need to create a certain input function for making recommendations, but we'll get to that later. Lastly, going to kick out the action with learn_runner running our experiment function. Using the WALSMatrixfactorization model, it is possible to predict the ratings between all items and users, if given the row and column factors. However typically, we just store the top K items per user or users per item to save space and computation by not filling out the entire radio matrix, which can be very large. Let's define a function called find_top_K that takes the user factors particular user U, and the item factors of all items V that the WALSMatrixFactorization model solve for. We also want to pass to the function K, so that it knows how many users or items to return. Instead of multiplying a vector from each factor matrix to get a specific user item rating prediction as we did earlier in our user movie example, we can matrix multiply instead to get all of the ratings for the user. Because this is just one user, user factors is a vector specific to that user. Before we can do the matrix multiply, we need to expand dims of user factors to make it a rank two tensor. Thus, where matrix multiplying a one by latent factors matrix with a latent factors by items matrix, which will result in a one by items matrix. Now that we have a tensor of all item ratings for a user, we can use tf.nn.top_k to get a tensor object of the top K values and their associated indices, which would be the item IDs in this case. Lastly, we want to return the top K items for our user. So, we'll return the indices member tensor of our top K object. Because these are indices will cast them to integers before we exit the function. If we want to find the top K items for all users we can create a batch prediction job. Note the session creation, this is a complete TensorFlow function, it is not part of any other graph. We define our estimator which will load the model from our output directory we wrote to during training. We extract the user factors from our estimator by getting the row factors which will be a rank two tensor, we shaped number of users by number of embedding dimensions. Likewise, we extract the item factors from our estimator by getting the column factor which will be a rank two tensor which shape number of items by number of embedding dimensions. We can reuse the fine top K function we just made and put it in a map function, we'll use our entire user factors matrix as the elements to our map function where it will pass a row or user from the matrix by the lambda function to find our top K as a user. This will create a stack of matrices, so we will squeeze it to get her down to a rank two tensor, we shape users by top K. We'll create a file IO stream to our batch prediction output file, and we'll create a loop where we evaluate the top K within the session we created, each row in the file in this case will be a comma delimited string of the top K best items for that user. Let's now test your knowledge, we saw how to recommend the top K items for users, but what if we wanted to instead recommend the top K users for items, which would be the correct change in our batch prediction function? Choose the answer that best fills in the blanks. The correct answer is B, are changing argument through the Lambda function to our find_top_K function will be an item for both. We will be matrix multiplying this item vector which will expand dams into a matrix by the user factors matrix. Lastly, the elements we will mapping from will be from our items factors matrix of shape number of items by number of embedding dimensions where it'll be pull a row per map from.