1
00:00:00,000 --> 00:00:04,365
We now know how to decode our TFRecord example protols into SparseTensors,

2
00:00:04,365 --> 00:00:06,210
but we're not done yet.

3
00:00:06,210 --> 00:00:09,375
After we repeat and batch our decoded examples,

4
00:00:09,375 --> 00:00:13,380
we then have to remap the stored keys to undo the batch re-indexing.

5
00:00:13,380 --> 00:00:15,555
Let's take a look at how that works.

6
00:00:15,555 --> 00:00:18,750
Remember several steps ago when we concatenate the keys to

7
00:00:18,750 --> 00:00:21,855
the indices tensor within our SparseTensors, well,

8
00:00:21,855 --> 00:00:26,025
now we finally get to use them to fix the re-indexing that happens during batching,

9
00:00:26,025 --> 00:00:30,735
where the item ID or user ID gets replaced by the example index within the batch.

10
00:00:30,735 --> 00:00:34,230
We first want to map our current dataset that just

11
00:00:34,230 --> 00:00:37,505
finished batching to a new dataset that has the fixed indices.

12
00:00:37,505 --> 00:00:39,650
So, we call it dataset.map and pass

13
00:00:39,650 --> 00:00:43,300
our batches SparseTensors to our custom remap keys function.

14
00:00:43,300 --> 00:00:46,790
Once in the function, we want to store the incorrect indices and

15
00:00:46,790 --> 00:00:50,315
values from the batches SparseTensors into their own tensors.

16
00:00:50,315 --> 00:00:54,200
There is a lot of tensor manipulation in this function and we don't want to overwrite

17
00:00:54,200 --> 00:00:57,965
the wrong thing and/or we might need some buffer space.

18
00:00:57,965 --> 00:01:01,195
So, we will use these two tensors later in the function.

19
00:01:01,195 --> 00:01:05,660
Next, we want to groupby the batch indices and get the count for each.

20
00:01:05,660 --> 00:01:09,865
This way we will know how many instances there were per batch example.

21
00:01:09,865 --> 00:01:12,770
We do this by creating a tensor of all ones for

22
00:01:12,770 --> 00:01:15,815
each batch example as the segment data and a segment sum,

23
00:01:15,815 --> 00:01:18,830
and using the same slice of bad indices tensor as

24
00:01:18,830 --> 00:01:23,200
a segment IDs to group into which are just the batch indices.

25
00:01:23,200 --> 00:01:25,700
Effectively, it's just a groupby operation of

26
00:01:25,700 --> 00:01:29,780
batch index with the aggregate count of number of users or items for each.

27
00:01:29,780 --> 00:01:32,990
Don't forget to subtract one from the segment sum because

28
00:01:32,990 --> 00:01:36,560
we don't want to count our concatenated keys and the totals.

29
00:01:36,560 --> 00:01:39,875
We also want to get the number of batch indices.

30
00:01:39,875 --> 00:01:42,155
Normally, this would just be the batch size,

31
00:01:42,155 --> 00:01:44,870
but in the event of a partially filled batch,

32
00:01:44,870 --> 00:01:47,405
it will be smaller and we need to know this number.

33
00:01:47,405 --> 00:01:48,860
This length is essentially

34
00:01:48,860 --> 00:01:52,075
just the first dimension of the shape of our grouped by size tensor.

35
00:01:52,075 --> 00:01:54,860
We need to know the offsets of our actual data from

36
00:01:54,860 --> 00:01:58,550
the keys with a key inserted after every chunk of real data.

37
00:01:58,550 --> 00:02:00,890
Which if you remember, our variable length.

38
00:02:00,890 --> 00:02:05,020
So, there's no simple way of priority to know where to slice or skip.

39
00:02:05,020 --> 00:02:07,850
To aid us in this, we can find the cumulative sum of

40
00:02:07,850 --> 00:02:12,275
the size tensor which can help us with a global re-indexing of the batch.

41
00:02:12,275 --> 00:02:16,250
Additionally, to know the assets between each example in the batch due

42
00:02:16,250 --> 00:02:19,955
to our storage solution of concatenating the keys in decode example,

43
00:02:19,955 --> 00:02:22,580
we will need to create a tensor that has a range from zero

44
00:02:22,580 --> 00:02:25,625
inclusive to the number of batch examples exclusive.

45
00:02:25,625 --> 00:02:28,490
Tf.range makes us easy and we can use

46
00:02:28,490 --> 00:02:32,805
our length tensor which is just the number of batch indices to create this range.

47
00:02:32,805 --> 00:02:35,515
Lastly, in this segment of the function,

48
00:02:35,515 --> 00:02:39,650
add in the length range to the cumulative sum will give us the cumulative range,

49
00:02:39,650 --> 00:02:41,780
which are the indices of the fake data we added to

50
00:02:41,780 --> 00:02:45,485
the SparseTensor by concatenating the keys in decode example.

51
00:02:45,485 --> 00:02:49,250
We want to get the key values we stored within the indices tensor.

52
00:02:49,250 --> 00:02:51,230
To do that, we first use a

53
00:02:51,230 --> 00:02:55,595
tf.gather with a bad indices tensor we made at the beginning of the function,

54
00:02:55,595 --> 00:02:59,180
from the input SparseTensor as our params and using

55
00:02:59,180 --> 00:03:03,115
the cumulative range we calculated earlier as the gathers indices.

56
00:03:03,115 --> 00:03:05,090
We slice the tensor only to keep

57
00:03:05,090 --> 00:03:08,750
the second dimension because the first dimension is the batch index,

58
00:03:08,750 --> 00:03:12,320
which we definitely don't want and in fact are trying to get rid of,

59
00:03:12,320 --> 00:03:14,245
and the second dimension has our key.

60
00:03:14,245 --> 00:03:18,635
We then squeeze the tensor to get into the correct shape of a vector.

61
00:03:18,635 --> 00:03:21,725
For us to be able to remove our concatenated into rows,

62
00:03:21,725 --> 00:03:25,370
we first need to know how many rows we have in the batch total,

63
00:03:25,370 --> 00:03:27,715
and then enumerate the range of row indices.

64
00:03:27,715 --> 00:03:29,570
We do this first by finding the shape of

65
00:03:29,570 --> 00:03:32,255
our indices tensor and taking the first dimension.

66
00:03:32,255 --> 00:03:38,875
We then use tf.range with that scalar we just found as our limit to get the enumeration.

67
00:03:38,875 --> 00:03:40,840
After all that work,

68
00:03:40,840 --> 00:03:43,580
we now have all the pieces to find the row indices of

69
00:03:43,580 --> 00:03:47,945
the actual data and can drop the key rows from the indices tensor.

70
00:03:47,945 --> 00:03:51,560
We have created two sets of indices and in our Venn diagram,

71
00:03:51,560 --> 00:03:53,905
we want the opposite of the intersection.

72
00:03:53,905 --> 00:03:59,810
First, we're going to copy some old tensors into new ones that we can play with called x,

73
00:03:59,810 --> 00:04:03,875
which will be the sparse indices enumerated range we just made in the last step,

74
00:04:03,875 --> 00:04:08,680
and s which is the cumulative sum range which are the indices of the keys.

75
00:04:08,680 --> 00:04:12,375
We want the indices of the not keys.

76
00:04:12,375 --> 00:04:14,669
We will be doing some tiling.

77
00:04:14,669 --> 00:04:18,595
First, we need to calculate the number of multiples we are going to tile x,

78
00:04:18,595 --> 00:04:20,390
our sparse indices range.

79
00:04:20,390 --> 00:04:23,210
We do this by first finding the shape of x and finding

80
00:04:23,210 --> 00:04:26,660
the shape of that will essentially be the number of dimensions of x,

81
00:04:26,660 --> 00:04:29,740
and then creating a tensor of ones of that shape.

82
00:04:29,740 --> 00:04:32,030
This is concatenated with the shape of s,

83
00:04:32,030 --> 00:04:35,600
our cumulative sum range which is just the number of keys.

84
00:04:35,600 --> 00:04:40,355
Now that we know how many times we want to tile x, let's tile it.

85
00:04:40,355 --> 00:04:43,190
First, we're going to expand dims x into

86
00:04:43,190 --> 00:04:46,310
a rank two tensor by adding a dimension to the end.

87
00:04:46,310 --> 00:04:48,545
We then are going to tile that tensor

88
00:04:48,545 --> 00:04:52,205
a tile multiple number of times calculated from above.

89
00:04:52,205 --> 00:04:54,860
This then gives us a rank two tensor with

90
00:04:54,860 --> 00:04:58,740
enumerated indices titled multiple times one for each key.

91
00:04:58,740 --> 00:05:01,735
TensorFlow is great for doing a vectorized math.

92
00:05:01,735 --> 00:05:03,205
Now that we have this tiling,

93
00:05:03,205 --> 00:05:07,375
we're going to perform our each tile slice for each key simultaneously.

94
00:05:07,375 --> 00:05:11,180
First, we create a Boolean tensor with each vector

95
00:05:11,180 --> 00:05:15,155
gets a true at the index of the key and I'll false elsewhere.

96
00:05:15,155 --> 00:05:19,679
We then use a tf.reduce any operation on the last axis,

97
00:05:19,679 --> 00:05:23,420
which essentially acts as a logical or merging all the Boolean

98
00:05:23,420 --> 00:05:27,785
vectors into one where any element that is true corresponds to a key index.

99
00:05:27,785 --> 00:05:31,040
Remember, we want the not key indices.

100
00:05:31,040 --> 00:05:33,095
So, we need to negate the following tensor,

101
00:05:33,095 --> 00:05:36,325
so that the true elements will be the elements that are not the keys.

102
00:05:36,325 --> 00:05:38,210
We can perform this negation by adding

103
00:05:38,210 --> 00:05:40,940
the till day unary compliment operator

104
00:05:40,940 --> 00:05:44,720
which will flip all the zeros to ones and vice versa.

105
00:05:44,720 --> 00:05:49,495
What to do with these Boolean mask that give the not key indices?

106
00:05:49,495 --> 00:05:52,400
We can apply them to one of the original tensors we created,

107
00:05:52,400 --> 00:05:53,990
the bad indices tensor,

108
00:05:53,990 --> 00:05:57,350
which has the original indices tensor that got messed up during batching.

109
00:05:57,350 --> 00:06:01,715
We can simply use tf.boolean mask to apply it to our tensor,

110
00:06:01,715 --> 00:06:03,290
which will return the indices from

111
00:06:03,290 --> 00:06:07,175
our actual data and drop the keys that we can cut into it in.

112
00:06:07,175 --> 00:06:11,695
Do we do all of this work to just throw the keys out anyway? Not at all.

113
00:06:11,695 --> 00:06:16,880
We'll be using them in just a moment but first we had to extract the actual indices data.

114
00:06:16,880 --> 00:06:20,945
Of course, this is all being put back into a SparseTensor,

115
00:06:20,945 --> 00:06:23,390
where the length of the indices and values tensors

116
00:06:23,390 --> 00:06:26,395
have to match because they correspond with each other.

117
00:06:26,395 --> 00:06:29,510
So, we will apply the same Boolean mask except

118
00:06:29,510 --> 00:06:33,815
this time to the original values tensor instead of to the indices.

119
00:06:33,815 --> 00:06:36,710
This will return our actual value data and drop

120
00:06:36,710 --> 00:06:41,395
the dummy zeros we added to the value's tensor back in the decode example function.

121
00:06:41,395 --> 00:06:45,530
We're getting close to the end now but we essentially have only returned to the state

122
00:06:45,530 --> 00:06:49,160
we would have been in after batching if we never concatenated the keys.

123
00:06:49,160 --> 00:06:52,820
So, now it's time to make all of this effort worth it by inserting

124
00:06:52,820 --> 00:06:57,770
the keys into the right spot and dropping the batch indices that overrode them.

125
00:06:57,770 --> 00:07:01,670
We will need to replace the first column of the slotted indices tensor we just

126
00:07:01,670 --> 00:07:05,665
created from our Boolean mask with the keys over the batch indices.

127
00:07:05,665 --> 00:07:08,185
This will not be done through tiling, however,

128
00:07:08,185 --> 00:07:11,540
it is not as simple as the last time we used tiling.

129
00:07:11,540 --> 00:07:14,045
This is due to the variable length of the data.

130
00:07:14,045 --> 00:07:16,325
Some items might have one user interaction,

131
00:07:16,325 --> 00:07:18,755
some might have five, others 10.

132
00:07:18,755 --> 00:07:21,800
The same is true for users who have item interactions.

133
00:07:21,800 --> 00:07:24,260
This jigging nature doesn't quite work

134
00:07:24,260 --> 00:07:26,690
well with TensorFlow or rectangular tensor structure.

135
00:07:26,690 --> 00:07:29,495
So, we'll need to have to improvise.

136
00:07:29,495 --> 00:07:34,205
We need to know what we're going to tile and how many multiples we're going to make it.

137
00:07:34,205 --> 00:07:37,925
First, we expand the first element of the gathered indices tensor,

138
00:07:37,925 --> 00:07:39,440
which are our key indices,

139
00:07:39,440 --> 00:07:41,530
by adding dimension to the end of that.

140
00:07:41,530 --> 00:07:45,065
The multiples will then be the first element of the size tensor,

141
00:07:45,065 --> 00:07:47,540
which is the groupby batch index counts.

142
00:07:47,540 --> 00:07:52,765
This is just the very first tiling to see the while loop we're going to do later.

143
00:07:52,765 --> 00:07:56,720
We had to repeatedly apply this tiling to each example in the batch.

144
00:07:56,720 --> 00:07:59,750
However, because there is a variable length of users or items,

145
00:07:59,750 --> 00:08:02,210
it is jagged and we cannot use tf.map

146
00:08:02,210 --> 00:08:06,250
function due to how it internally does the stack in the tensor array.

147
00:08:06,250 --> 00:08:10,100
So instead, we're going to create our own version of tf.map

148
00:08:10,100 --> 00:08:14,585
function by creating a loop body that we will use in a while loop in the next step.

149
00:08:14,585 --> 00:08:17,510
First, we define a loop body function that takes

150
00:08:17,510 --> 00:08:20,750
the loop index as the first parameter and the tensor that we are going to be

151
00:08:20,750 --> 00:08:24,200
growing as a second parameter that eventually will contain

152
00:08:24,200 --> 00:08:28,040
the tiled item IDs or user IDs from our stored keys.

153
00:08:28,040 --> 00:08:30,355
We will return two values,

154
00:08:30,355 --> 00:08:35,180
the loop variable incremented and the tensor we are eternally growing by concatenating

155
00:08:35,180 --> 00:08:37,790
the previous tensor with the next tiling by sliding

156
00:08:37,790 --> 00:08:41,195
along the key indices and batch index counts tensors.

157
00:08:41,195 --> 00:08:44,525
The loop body we just created won't run by itself,

158
00:08:44,525 --> 00:08:47,195
so we need to call it using tf.while loop.

159
00:08:47,195 --> 00:08:48,905
We call with our conditions,

160
00:08:48,905 --> 00:08:51,335
so that it loops over the full length of our tensors

161
00:08:51,335 --> 00:08:54,080
using the loop body function we created and using

162
00:08:54,080 --> 00:08:57,050
our initial loop virus of one for our loop

163
00:08:57,050 --> 00:09:01,990
variable i and our initial one shot tiling we did to the spec.

164
00:09:01,990 --> 00:09:03,475
After the loop completes,

165
00:09:03,475 --> 00:09:07,985
it will return the tensor we grew through repeated tiling as our new tensor named result,

166
00:09:07,985 --> 00:09:09,665
which will be our key values,

167
00:09:09,665 --> 00:09:12,290
item or user indices expanded out

168
00:09:12,290 --> 00:09:15,515
for the correct number of examples that correspond with.

169
00:09:15,515 --> 00:09:17,300
We're near the end.

170
00:09:17,300 --> 00:09:19,730
We have two correct but separate tensors.

171
00:09:19,730 --> 00:09:21,530
One contains our keys,

172
00:09:21,530 --> 00:09:24,275
one contains our data that is associated with each key.

173
00:09:24,275 --> 00:09:27,410
This is the step in which we finally fix the overriding of

174
00:09:27,410 --> 00:09:31,460
the item ID or user ID by the batch index that happened during batching.

175
00:09:31,460 --> 00:09:35,075
To do this, we will concatenate the tiled keys we just made

176
00:09:35,075 --> 00:09:39,260
with the second column of the indices that we fix earlier of the actual data.

177
00:09:39,260 --> 00:09:42,860
We had to expand the dimensions of each tensor by one at the end

178
00:09:42,860 --> 00:09:47,105
of each sensor and then we concatenate along columns.

179
00:09:47,105 --> 00:09:52,025
Awesome. All we need to do now is put everything back in SparseTensor format,

180
00:09:52,025 --> 00:09:55,055
so that the walls input function can use it correctly.

181
00:09:55,055 --> 00:09:57,470
The indices will be the indices we just fixed in

182
00:09:57,470 --> 00:10:01,505
the last step by concatenating the tiled keys with our fixed indices.

183
00:10:01,505 --> 00:10:04,070
The values will be the values that we fixed by removing

184
00:10:04,070 --> 00:10:07,445
the dummy zeros we added in the decode example function.

185
00:10:07,445 --> 00:10:11,300
The dense shape will be the same dense shape of the input SparseTensor to

186
00:10:11,300 --> 00:10:15,640
the remap keys function. We're done.

187
00:10:15,640 --> 00:10:20,670
Return the remap SparseTensor and we will return to the parse TFRecords function scope,

188
00:10:20,670 --> 00:10:23,810
where we will call the one-shot iterator and then go back up and scope to

189
00:10:23,810 --> 00:10:27,860
the walls input function for input rows and input columns.

190
00:10:27,860 --> 00:10:30,470
This was a lot of work to remap the keys back into

191
00:10:30,470 --> 00:10:34,070
the input data SparseTensors but it was completely essential.

192
00:10:34,070 --> 00:10:37,115
Without it, the walls matrix factorization estimator,

193
00:10:37,115 --> 00:10:40,790
will not perform the necessary sweeps because the batch index keeps

194
00:10:40,790 --> 00:10:45,095
resetting back to zero every batch and the model will not train.

195
00:10:45,095 --> 00:10:49,925
With this, the sweep perform correctly and the model will learn from the data.

196
00:10:49,925 --> 00:10:55,460
Wow, we now know how to remap the key is to fix the re-indexing by the batching process.

197
00:10:55,460 --> 00:10:57,370
So, let's see how much we've learned.

198
00:10:57,370 --> 00:11:00,980
Here is sample data for items for user with the following schema,

199
00:11:00,980 --> 00:11:05,210
user index, item indices, ratings.

200
00:11:05,210 --> 00:11:08,330
Here are the first two batches of SparseTensors

201
00:11:08,330 --> 00:11:11,410
pass to remap keys with batch size equals two.

202
00:11:11,410 --> 00:11:15,320
Match the tensor elements to what they represent based on color.

203
00:11:15,320 --> 00:11:17,570
The correct answer is D.

204
00:11:17,570 --> 00:11:22,445
The red highlighted elements is the indices tensor are the batch indices,

205
00:11:22,445 --> 00:11:24,860
which are the first dimension of the indices tensor.

206
00:11:24,860 --> 00:11:28,955
Notice that the values range from zero to one because our batch size is two.

207
00:11:28,955 --> 00:11:30,875
So, there should only be two values.

208
00:11:30,875 --> 00:11:35,120
The green highlighted elements are the item indices and the indices tensor,

209
00:11:35,120 --> 00:11:38,875
that is what we wrote and read in our indices with the TFRecords.

210
00:11:38,875 --> 00:11:43,310
The blue highlighted elements are the user indices in the indices tensor.

211
00:11:43,310 --> 00:11:45,890
This is what we wrote and read in as a key with

212
00:11:45,890 --> 00:11:50,935
TFRecords and concatenated these to the indices tensor in the decode example function.

213
00:11:50,935 --> 00:11:54,315
The yellow highlighted elements are their ratings.

214
00:11:54,315 --> 00:11:58,345
This is what we wrote and read in as values with the TFRecords.

215
00:11:58,345 --> 00:12:01,575
The purple highlighted elements are the dummy values.

216
00:12:01,575 --> 00:12:03,830
We can cut into this to the values tensor in

217
00:12:03,830 --> 00:12:05,930
the decode example function to ensure that

218
00:12:05,930 --> 00:12:09,520
the indices and value sensors had the same length.

219
00:12:09,520 --> 00:12:13,220
The remap keys function wants to replace the batch indices with

220
00:12:13,220 --> 00:12:15,560
the corresponding user indices and drop

221
00:12:15,560 --> 00:12:18,980
the added elements of the user indices and dummy values.

222
00:12:18,980 --> 00:12:21,050
Here at the bottom is what the SparseTensors will look

223
00:12:21,050 --> 00:12:23,465
like after the remap keys function fixes them.

224
00:12:23,465 --> 00:12:27,410
Notice that the batch indices have been replaced with the correct user indices

225
00:12:27,410 --> 00:12:32,420
and we've eliminated extraneous rows with a user indices and dummy values.