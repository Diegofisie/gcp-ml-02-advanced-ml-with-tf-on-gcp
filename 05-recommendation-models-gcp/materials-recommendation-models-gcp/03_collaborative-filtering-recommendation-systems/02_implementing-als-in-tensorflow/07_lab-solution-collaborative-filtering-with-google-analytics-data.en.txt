All right. We're going to look at the WALSMatrixFactorization notebook. We're going to go through collaborative filtering using Google Analytics Data. We're going to see how we can do this to work. First what we're going to do is we're going to import our OS library here and we're going to set our project, our bucket, and our region into our environment variables. Replace these with whatever your projectID is and your bucket name. We're going to put these in our environment variables right here. We're using TensorFlow version 1.8, in this lab and then we're going to set our configs, for our project and our compute region, using those environment variables by calling bash. From there, we're going to import TensorFlow, because we'll be using that for this lab. I'm going to print the TF version, just to make sure that we're using the right one, 1.8. All right. Now, we're going to create our raw dataset. For collaborative filtering, we don't need to know anything about either the users or the content. Essentially, all we need to know is the userID, the itemID, and the rating that the particular user gave to a particular item. This is what makes the user item interaction matrix. In this case, we are working with newspaper articles. The company doesn't ask our users to write the articles, however we can use the time spent on the page as a proxy for rating, which is going to be their session duration. Normally, we'd also add a time filter to this, for instance like the last seven days, but our dataset is itself limited to a few days, so we don't have to worry about that too much. All right. First, what we're going to do is, we're going to import the BigQuery library from datalab, then we're going to write our query. In our query, I'm using standard SQL and we're going to create a CTE table here with this width, visitor page content, and we're going to select from the table right here, GA session sample, the fullVisitorID and then we're also going to select the max from this index, from this nested table on the custom dimensions, which these are going to become our contentIDs, so the latest contentID, then I'll hit the lead of the hit's time around the partition over, fullVisitorID and order it by the hit's time, in ascending order. I want to subtract out the hit's time and this will be our session duration. Basically, each session that a user interacting with that content. What we're going to do is we're going to make sure that we're only including hits on pages. So, that's how we're going to have this, where clause here, hits.type equals page and then we're going to group by the fullVisitorID, a latest contentID, and the hit's time. From there, using the CTE table, we're going to select the fullVisitorID as the visitorId, the latestcontentId as our contentId, and SUM with other session_durations as our session_duration, for that particular visitorId and contentId. Okay? We're going to filter out things where the latestcontentId is not null, because we don't really care about content that doesn't exist and we're going to group by the fullVisitorID and the latestcontentId, and we're going to make sure that session_durations are greater than zero, because anything is zero or less than, is either bad data or not useful for us. I want to order by the latestcontentId, so, that's repeatable. All right. Now that we've had that SQL query written, we're going to write that into a dataframe. We're going to do this using the BigQuery library, where we call it bg.query, the query pass, which is our SQL query we created up here called SQL. I'm going to execute it, I'm going to take that result, and send it to a dataframe. From there, we're going to print out the head of the dataframe, which will show the first five rows. As you can see in our little table here, for this row, we have this visitorId, which is a long integer, a contentId here, the thing that they viewed, it's another long integer, and the session_duration right here, which is going to be how long that that visitor interacted with this piece of content. Let's see some of the stats. To do the stats, we're going to do a df.describe, so this will describe the statistics of the dataframe and I'm going to print it out. As you can see here, we have our accounts and our session durations. We have on average it looks like session durations of 1.2 times 10_5 seconds and there is 2.7 times 10_5 number of interactions. Since standard deviation here is pretty high, so that way you can see there's lot of variants amongst users. The minimum looks like it's one second. Why would it be one second? Now remember we filtered out anything that was zero or less, so therefore we shouldn't have anything less than one. The maximum here, that's our longtime. Someone's really reading a lot of content, it looks like 7.6 million seconds. All right. But let's try to get that in a more reasonable ranges. To do that, we're going to create a rating and the rating is going to be the session_duration scaled to be in the range of zero to one, this will help during training. To do this, we're first going to take our session_duration and we're going to subtract out the stats of the median, for that session_duration. I'm going to divide by the median to denormalize it. From there, we're going to add one and multiply it by 0.3, to get our new rating. From there, we're going to say if any ratings are greater than one, we're going to camp at one. Therefore nothing can be higher than one. Now if we hit df.describe, you'll see, we still have the same number of counts, however, our ratings all had the correct ranges. To the minimum here is almost zero and the maximum here is one, where we've kept things off that nothing can ever go above that. Now we don't need session_duration anymore. So, we've created our own rating column, so we can delete it from the dataframe. Next, we'll call bash and we'll make sure that anything in our folder called data is deleted and they'll make a new directory called data, that will put all these pieces of data in. To do that, we're going to call the method to the dataframe to CSV, pass in our CSV, named create collab raw, so basically our collaborative raw data, because there's no index and no header. Then we're going to call bash again and we're going to write it out and as you can see there it is. As you can see here's our visitorIds, our contentIds and our ratings that we just created, all going from zero to one. Great. So, now we need to do is we need to create our dataset for WALS, the raw dataset above won't work for WALS. First, the userId and itemId, have to be zero, one, two. So, we need to create a mapping from visitorId and the raw data to userId, and contentId and the raw data to itemId. Why do we do this? Well, remember this is a user item interaction matrix and we need to have the indices of the matrix to use to be able to link up later on. We also need to save the above mapping to a file because at prediction time we need to know how to map, the contentId in the table above to the itemId and same goes for the users. We don't know who user zero is going to be, but all itemId number 462, but if we have the reverse mapping we can also go backwards and say, is actually user really long integer that goes to contentId also long integer. But that fits our business case. Lastly, we'll need to have two files; a rows dataset, where all the items from a particular user are listed and a column dataset where the users for a particular item are listed. Remember, we're using Alternating Least Squares here. So, it will back and forth between rows and columns, fixing one while solving for the other and vice versa. All right. So, to create this mapping, I'm going to import pandas and numpy. I'm going to create a mapping function here where it takes in values and the filename. I'm going to open the file name and I'm going to take for each value and position. I'm going to loop through and enumerate all the unique values from that file. For the value and position in each one of those, I'm going to write out the value and location for that, the index. I'm going to return the value ID out of this function. So, let's see how that works. So, I'm going to take my data frame. I'm going to read in the file we just created above with our raw data. No header, I'm going to give these names to our columns visitor ID, content ID, and rating. We're going to give the data types, we're going to say this is the string, kind of is a string and radius is going to be a float between zero and one. Next what we're going to do, is we're going to say two CSV. Now we're going to be writing it back out. Alright. So, we're going to take our data frame that we just created, I'm going to call it the two CSV method on it and write back out to make sure that it has the correct things. Next, we're going to take the user mapping and we're going to create a mapping from that using our function above right here and we're going to map visitor ID to our users. Likewise, we're going to create an item mapping and we're going to take our data frame with content ID and then create our items. All right, let's see what that looks like. So, we're going to take the head, the top three rows from each of our data sets that we created. So, here's our raw data set as you can see here is our long integers for each visitor ID, a long integer for each content ID and our ratings that go from zero to one. If we look at our items file, you can see here is our content ID and the associated item ID that we created from that. So, Content ID, item ID, Content ID, item ID, Content ID, item ID. This is our mapping from Content ID to item ID. Let's look at our user's file. You can see our user, our visitor ID right here and then this gets mapped to a user ID. Visitor ID, user ID. Visitor ID, user ID. So, this is our mapping from visitor ID the user ID. Great. So, now what we're going to do is we're going to get a data frame where we're going to take this map and we're going to do the conversion. So, we'll take visitor ID then we map it using the mapping we just created and that will become our user ID in our dataframe. Likewise we'll do the same with content ID where we map using our item map we created to get our dataframe column of item ID. We're going to create a mapped dataframe now that's going to be the concatentation of these dataframes using user ID, Item ID and a rating. We're now going to write this out to a file using the method two CSV of our mapped dataframe to the colab mapped dot CSV. Let's look at what some of that looks like. Here we go. As you can see in this table, we have our user ID's or item ID's and our ratings. You can see these look like matrices. They start from zero and they enumerate out to N minus one rows and start from zero and enumerate out to N minus one columns and all their ratings are nicely between zero and one. All right. Let's see what we'll do next. Creating rows and columns datasets. All right. So, now we have our user mappings and our item mappings. Let see how we can create our actual data to go to the estimator. So, we're going to import pandas and numpy again and we're going to read in our mapped dataframe. I'm going to do that from our colab mapped dot CSV file and these will be the names of our columns. User ID, Item ID, and rating. As you can see above, this is what matches, so great. We need to figure out how many items and users we're going to have. So, to do that since everything's enumerated nicely, we can just do a max. So, to do that we're going to take their map dataframe item id, take the numpy max of that and just add one because remember, it's N minus one same as their indices. Same goes for users. We'll take the maps dataframes user ID column, find the numpy max of that, add one and that's the number of users. What we'll do then is we're going to round the map dataframe rating to values of two significant figures and we'll set that back into our rating. So, that way we don't have long superlinear floats. Next we'll print the items and users and interactions and we'll see what that number is. So, we had in this data set 5,668 items, 82,802 users, and between all of those there was 278,913 interactions. Wow! That's a lot. But still if you use the Cartesian product between items and users, this is a really small percentage of that. So, as you can see, our user item interaction data is very sparse like most recommendation collaborative filtering systems have. So, now we're going to group by items. To do that, we're going to take our map dataframe we're going to group by the item ID field. Okay. We are then going to create a loop here so we can just see what's inside and for each item and group inside a group by items, we're going to print it out. We're going to do that five times. So, here we go. We have the very first one, here is our item ID, an item ID has zero has this user ID and it has this rating. Item ID one has user ID one going with it and it has one rating. Now, here's where it gets a little different. Item ID two has actually two users that have interacted with it, user two and user three and they each had their corresponding ratings right here. So, 0.17 goes with this user and 0.25 goes with this user. All right. Moving on we have item three with user four with a rating of this, 0.05, and so on. All right. So, now that we have our maps, we need to put this into TF records remember. So, let's see how this goes. So, we're going to import TensorFlow in case we haven't already done it. We'll do our groupby items were we're taking our map by dataframe and we're going to groupby item ID. Okay. We're then going to call TF record writer for Python IOstream and it's going to be users for item. So, it's going to be the singular item, unique item and all the users that went with that item and all the corresponding ratings that went with those users. So, for each item and grouped entity in there, we're going to loop over and we're going to create an example. So, it will be TF.Train.example. I'm going to set our features, our features are going to be these train features to be a feature dictionary. Now, if you remember from before, we're going to have three things here. We have the key, the key here is going to be the item ID here. Item, the indices here are going to be our user IDs that were groupby. The values here will be the corresponding ratings that went with those users. From there we're going to serialize that all to a string and write it out to the file. Likewise we'll do the same with users. We're going to take our math by dataframe and group by this time user ID. From here we're once again get a TF record writer but now we're going to be writing out to the file items for user. So, this case we have one unique user with multiple items and then the corresponding ratings that go along with those. Just like before, except now we're going to be looping by item, we're going to loop over item or user are going to be grouped by those. We'll once again create an example and this time we're going to have key will be our user ID, the indices will be our grouped by item IDs and our values will be our good by ratings that correspond with those item IDs. Once again we're going to serialize those to string. To make things a little simpler, we're going to create a sample data set here. We're going to do the same thing, items for user that's going to be a subset. Okay. Since we have a lot of users 82,000 remember. So, we're going do this and now it's going to loop over the top 20. Now, let's see what our data looks like. So, you can see we have lots of files we've created now. We have our raw file here, our users and items mappings, our mapped raw data and then our users for item, our items for user and our items for users subset TF records. Great. So, as you can see here to summarize, we've created the following data files from collabraw.CSV. We have collab_mapped.CSV which essentially is the same data as in collab raw except that visitor ID and content ID which are business specific have been mapped now to user ID and item ID which are enumerated in zero to the number of mappings that are stored in items.CSV and users.CSV. So, we can use these during inference. Users for item contains all the user ratings for each item into TF record example format, and items for user contains all the item ratings for each user in TF example format as well. Now that we have our data in the correct format, let's train with walls. Once you have a data set do matrix factorization with walls using the matrix factorization estimator in the contrib directory. It's an estimator model so it should be relatively familiar. As usual we write an input function to provide the data to the model and then create the estimator to do train and evaluate. Because it's contrib and hasn't moved over to TF estimator yet, we use TF.contrib.learn.experiment to handle the training loop and then we'll use learn runner to actually run the experiment. So, first to do this we're going to import all our other libraries. Here you can see we're importing our walls matrix factorization from the contrib library under factorizations. To do this, we're going to do our read data set. We're going to pass in the mode, sources, training or evaluation and then the arguments that are going to be passed. So, here's our decode example function that takes our example protocols from the TF records and our vocabulary science. Here in the lab portion, you had to fill this out. Let's go over this to see if you did it correctly. So, we're going to read on our features dictionary here, coming from our protos. So, we're going to have a key here, which are going to be depending on our user ID or item ID, depending on what TF record it's reading. This will be our fixed length feature. The indices here and the values will both be variable length features and they'll both be the same length. If key is our user ID, the indices will be our item IDs and their corresponding ratings. If key is the other way around it'll be the opposite. Then what we're going to do is we're going to parsed our features using this dictionary we created here from our protos and we'll get our parsed features. From there, we're going to pull our values out by doing a TF dot sparse merge, which will take our indices and our values and using our vocab size it will create a SparseTensor. Remember, to fix the batching re-indexing problem, we're going to save a key to remap after batching. So, we'll take our parsed features key and write that out to this tensor here. Finally, we're going to create our decode and Sparse Tensor which will be first, the indices part of our SparseTensor will be the concatenation of our values, the indices, and then the key. Because remember, this is how we're going to transfer our key through the input function along the axis zero. Our values will be the concatenation of the actual values we've read in, and we're going to add in a dummy value which will be zero. Lastly, our data shapes will be just the datasets we started with. From there, we're going to return our decoded SparseTensor onto the rest of the input function. Now, what we're going to do next is remap the keys from the Sparse Tensor. Now, remember the reason we're doing this is because the input function during batching replace the actual key indices, that are the first dimension of our indices of the SparseTensor with the batch indices, which we don't want because during training, you're not going to be able to sweep correctly and will not actually learn much. Because the indices will keep getting repeated every batch, zero, one, two, however much your batch size is. So, let's see how remap keys works. So, we've already covered this in depth, but let's just go through this quickly. So, first what we're going to do is we're going to take our SparseTensor that comes in. Remember, this has already been batched, so we have a batch of these coming in. So, we're going to have current indices of our SparseTensor we need to fix. We're going to write these out, the bad indices and the bad values which will come from our indices and values members of our SparseTensor. We're going to group by the batch indices and get the count for each. So, for this, all we're doing is basically taking a segment sum. Where our data is going to be ones or where our bad indices are, and the segment ID's of our bad indices and then we'll do minus one. That will be the size. The length of batch indices, it'll basically be batch size unless it was a partially full batch, what should be this? Predict the shape of our size and we'll take just the first dimension of that and that'll be our length. Next, we'll find the cumulative sum which will be used for indexing later, right here. That's the cumulative sum of the size and then we'll find us our cumulative sum, so we can find our offsets. To find the offsets, we're going to first create our length range. So, let's basically enumerate from zero to the length. So, the number of examples in each batch to find this and then their indices will be the cumulative range. We'll just take this cumulative, which are our assets plus this, which will give our actual indices of the items we added during the concatenation in our decode example function. From there, we're going to gather. So, the keys that we've exerted back out of our concatenated Sparse Tensor. Okay. So, we're going to do our TF test squeeze, gather using the bad indices, we're going to pull out of those, we're going to extract from it using this gather from the cumulative range of the second column. Okay. So, the numerator row indices, the Sparse Tensors indices number are all going to get next. From here, I was going to create a range. I'm going to enumerate over the shape. So, that way we have the number of times each example was in the batch. We want to find here the row indices of this sparse tensors indices member, that are the actual data in that concatenated rows. So, we want to find the intersection of the two sets and then take the opposite of that. So, to do that we're just going to create two little helper tensors here X and S, X will be our Sparse indices range and S will be the cumulative range. So, first what we're going to do is going to tile this. So, we need to know the number of times we're going to be tiling. To do that, we're going to concatenate which ones using the shape of our Tensor and then the other shape of our S tensor here, our accumulate range. We're going to concatenate those two things together, and that will the number of multiples that were going to tile X. So, I want to expand X on sparse indices range into a range two tensor and then multiply it by the rows, by one, so there's no copying and the column for the number of examples in the batch. So, to do that we just do a TF tile and then TF's that are expanding, we're going to turn this into a matrix and then we're going to tile the number of times we just found. From there, essentially what we're going to do is a vectorized logical or and then we're going to negate it with our unionary compliment operator here where there is tilly. To do that, when it's a equal where X tile equals S. So, basically here, anywhere that the actual concatenated indices is found will be true and then across all of them we're going to reduce any. So, wherever there's any truce, the overlap and we'll get all the truce wherever there is a one and the rest will be left with zeros. Then we'll take a not here, with us today which basically flips all the ones and zeros and all the zeros to ones. This will be X not and S. Moving on, the SparseTensor's indices, that are actual data by using the Boolean mask we just made above applied to the entire indices member of SparseTensor. So, now we want to pull out our selected indices here. To do that, now that we have this Boolean mask, we can apply a Boolean mask to it, where our tensor will be the bad indices and our mass will be this X not in S or zero axis. That's the indices, but we also want the values are ratings. To do that, we just use the same Boolean mask and now apply it to the bad values tensor. Using biomass we created a long zero x axis. Now, that's great and all but we haven't removed the batch indices yet. So, first to do that we need to replace the first column selected indices with the keys. So, we firstly need to tile our gathered indices. So, first we're going create a tiling. Now, we can't do this as simple as before, because the arranging is going to be Jagged Each item might have different numbers of users and same likewise, each user might have done numbers of items. Right. One user might have seen five things and another user medicine 10. So, there's no nice way to do this without a while loop which we'll get to next. So, first we need to initialize our while loop with initial tiling. To do that, we're shrinking our gathered indices, the zero column is or a batch indices and we're going to do it multiple times using the size here. Okay. From there, when you create a loop body. So that way, we can create our own tf.map function, because the stack in tensor arrays have to be rectangular. In this case, since its jagged, we don't have that. So, we'll create our own. So, to do that loop body, we're going to pass in our loop variable here,I, then our tensor that we're going to grow here called tensor grow. Inside a little body, it will return the next loop variable. So, I plus one and we can concatenate our values here, which are going to be our original tensor grow and then when I add on top of that a tiling of that, or expanded in terms of our gathered indices. How many times we're going to do this? On multiple number of times that we found above based on the size, how many items that user had, for instance. Well, that's great and all, but we need to call this loop. So, what we're going to do here is going to get our tf.while_loop, pass in our Lambda of our loop variable here, and tensor_grow with the condition that i is going to be less than the length because we don't want to go outside of our batch, and then we're going to have our loop body, which we're going to execute, and they're going to pass in our initial variables for i in tensor_grow, which will be one for i and then our initial tiling for tensor_grow. This will get us our result tensor here. So, now what we're going to do is we're going to concatenate our tiled keys with the second column of selected indices, so that we will have the correct keys now. So, selected indices fixed will be the concatenation of expanded dims a result because right now it's a vectors we're going to turn it into a matrix, and then we're going to concatenate that with the expanded as a selected indices of the second column, because remember we don't want the first column because the first column is the batch indices, which is what we're trying to throw out. So, we want to get these, and we're going to concatenate those two together along columns. Now, that's all fixed, we got to put it all back together in a nice SparseTensor, so that our input function can send it off to the estimator. To do that, we're going to create SparseTensor using our indices, which will be our selected indices fixed, and our values are selected values, these are our ratings right here, and the density will be the same as before, and then we return back out to our main parse TF records function. So, now we're willing to parse TF records, we'll be parse a file name and our vocab size. So, if mode equals train, we want to make sure that the number of epochs equals none. So, that way it trains indefinitely and uses the train steps to decide when to stop training. Else if we're not in train, other words, evaluation we're going to have one epic that we just got the one and only one time, and that way we can do our evaluation correctly. We're going to do here is get our files list, so we pick our input path from above using our filename, and we're going to create a list of files and the files are shorted out because it was too big for one file. From there we take our file list and create a dataset. To do that remember these are TF records, we're going to create our TF record file dataset using our files and create our dataset. From there, we got to do our decode example function that we have just went through above, using our example protos as input, and the current vocab size depending if we're doing items or users. Next, we are going to repeat the number of epics that we've set up here. Next we're going to do our batching. So, we have a batch size number of items or users depending on which phase we're in, but here we're going to do our remap keys to fix some of the batching indices being overwritten, and lastly we're going to create our one-shot iterator which will return our batch of examples. So, now this all gets called in our rapt input function right here where we could refuse your dictionary of input rows where we call parse TF records for items for users, so remember this is a single user having multiple items and therefore, multiple ratings, and the vocabulary here is the number of items, and then our input columns for our factorization is going to be using the users for item file, where now our vocabulary's the number of users where we have one unique item and multiple users for each item and therefore also multiple ratings. Lastly, we're going to protect row here, so that we can use this during serving where we can just decide are we predicting rows or columns. So, true will predict rows, so we'll be predicting items for users and false we'll be using predicting columns, so we'll be trying to predict users for items as in like a targeting model. From here we're going to return features and none. Remember this none here is usually for our labels, but because this is an alternating algorithm in law and phase our labels are going to be the columns and the other phase or labels will be the rows because we fixed one while solving for the other and vice versa, and just for developing line by line you don't really need some production but just so you can see. We're going to create input function subset where we just use our small number of users you created here out of the 82,000. All right, and then from there our input functions done, let's try it out and see what it looks like. So, this code is helpful in developing the input function, you don't need in production, but it's a good way to check while you're coding to make sure things are running smoothly. Okay, to do this we create a function called try out, and to do that we're going to create a session. Within that session, we're going a function, that's going to point to our read dataset function above, and here we're going to pass in our mode, a mode is going to be eval in this case, we're going to be reading from the eval data set, and we're going to do our input path our data and our batch size, is going to be eight or number of items is going to be 5668 and our number of users is going to be the total number of users 82, 802. When I write out our features from this function when as we call it and I'm going to print off our features, our input rows using eval. Let's see what that looks like, as you can see we have our SparseTensor, here are indices. Okay. Let me show you these are going to be using our input rows here, so that means that these will be user IDs and then these will be item IDs down here, and then here in our values right these are the associated ratings that go with each of these combinations, these user item combinations, and of course, our dense_shape is going to be the batch size here times the number of items. All right, that's great let's set up our function to find the top K. We don't want every single recommendation for every single user, we just want the top K of them. To do that we're going to pass in the user ID, the item factors that we've been trained and then what K should be, should be five, 10, opens what your problem is. All right, to do this we're going to get all of our items, we'll solve the entire matrix by doing a matrix multiply, of our user expanding dams and to a one by number of user vectors, if it's just one users be one-by-one. Since it's our bad prediction yet and then we're going to do our transpose or item factors, which will be a items by k or d-dimensional matrix. So, we got our top k revenue tf.nn.topk of all of our items where k equals k and then last we're going to return the casting of the indices to integers because remember these must be IDs, so we're going to pass those IDs back of the top k items. But if you want do more than one, at a time when a user at a time should you bet prediction. So, here we're going to pass our arguments inside on a call NumPy and were to create a session. Remember, this session is completely disconnected graph it's its own graph or in its own session. To do that, we have to create an estimator this estimator will read in what we've already trained, from our output directory here from intermodal directory. So, that we can read in the values that were trained and get our user factors in our item factors be able to do our predictions. So, we're going to call our estimator here, which you should have filled in during the lab of Tf contribute factorization walls matrix factorization or number of rows will be of course a number of users, number columns with a number of items, the MBA dimension this is what we decided how many latent features we would solve for, the number of meds and here's Ramon directory. The rest of the lab was already done for you so, now you can kind of just follow along and see what we've done. So, this is how you get the row factors for an outer vocab user data. So, if case there was a new user or a new item you could use these. So, you'd list get protections using that and you convert to a tensor of the row factors, but for vocab data so far users that already exist and items that already exists we can use this instead, the row vectors are already in that checkpoint. So, user factors would just be a tf.convert to tensor, I'm going to call the estimator I'm going to call the method get Rho factors. You're going to get the first dimension of that, which will be the number of users by number of embeds will be the shape. In either case we have to assume the catalog hasn't changed, so you've just seen there's been no new items added to the interaction matrix. So, colon or factors are brought in. So, I don't factor so just be tf.convert to tensor. We're going to call once again, call our estimator using the method get co-factors taken the first dimension and it is going to be number of items by number of imbalance as the shape, for each user and our batch we're going to find the top k items. So, top k it's going to be a squeeze when it call this map function f2 to loop through all these users essentially with this vectorized top k, I'm going to a user or call top k, using user. Where user factors will be r elements that we'll loop over and then our item factors will be what? We multiply with and then our top k will be how many we save back out and that gets read into our top k right here, which will be users by top-k shape. From here we're going to do is create our branch predictions. Okay. So, for the best items for user and top k we're going to evaluate this Tensor right, here this top k tensor. From there runs the graph and then we're going to write out a comma delimited list inside of our filename here for each unique user what their top k items would be. How do we call this? Well, we're going to create our serving input function that we're going to pass arguments to for user embeddings where we pass a user ID. So, all items for this user for the user embeddings, the items will be arranged in enumerated range of all the items possible. The users will be the user ID times a tensor of ones for all the items. That way, essentially, we tile their user ID across all of them. The ratings will just be a dummy right now, 0.1 times the ones like the users. So, this will just be used because we're not actually using the ratings, we're predicting ratings. But we need a placeholder right now to build it right into. Then we'll return our items, our users, our ratings, and our tf.constant true. Does anyone remember what tf.constant true is for? Remember this was our projectile rows. Since we're predicting for users, we want to have this be true. Whereas if we're predicting for items, it would be false as we'll see shortly. If we're doing it for item embeddings where we pass in the item ID, the users now will be the range, the enumerated range of all the users, and the items ID will now be the item ID tiled across all of the ones for each user. The ratings ones are going to be dummies, and we'll return. The items, the user's, the ratings are now false, because now we're projecting columns and not rows. All right. So, now this all gets called in the survey input function. So, we have a feature placeholder, where we read in data from whatever is sending the inputs to our model. We have a placeholder for one integer for a user ID and one integer for an item ID. In here, what we're going to do is we're going to say, well, depending on whats sent, we had to make sure we're either doing users or items for what the user that sent this request wants. To do that, as we mentioned earlier, we're going to use a tf.cond,where we're going to see if the user ID is less than zero, so if they give a negative number, we're going to do item embeddings instead. Else, if it's greater than or equal to zero, we're going to do a user embeddings. To do that, once we're done figuring out if it's for items or for users, we're going to stack the users and items tensors together to get our rows and then we do this transpose of stacking the items and users tensors together to get the columns. From there, our input rows will be a SparseTensor of the rows with the ratings as our values with the dense shape of number of users by number of items. Our input columns will be the transpose by using the columns as our indices now and their ratings as our values, once again with the dent shape of number of users per number of items. Now we have our features dictionary we're going to create from these placeholders using input rows, input columns, and what project rows. Last thing that we'll do, well return tf.contribute.learn.input function ops, our features, no labels, and our feature placeholder, and return as serving input function. Wow, that was a lot, but we finally made it to our train and evaluate function. First, what we're going to do is figure out how many train steps we're going to do. To do this, we do a little bit of math and figure out maybe the number of epics, also times the number of users, times maybe 0.5 and then divide by the batch size. That might be a good number of train steps to perform, and the steps in each epic might be 0.5 plus the number of users divided by the batch size. So, this will tell us that we're going to train for this many steps and evaluating this many times. So, now we're gonna create our experiment function that we'll write out to our output directory. It's going to return an experiment and inside the experiment is going to be our estimator which will be the tf.contribute factorization.walls matrix factorization. Where the argument is going to pass the number of rows via number of users, the number of columns via number of items. The embedding dimension is going to be the number of embedding dimensions we want. So, how many latent features are we shooting for here? Is it 2, 4, 20? And then our model directory, so we're going to write this all out to it to keep our model files. Our training but functional be read data set where we pass in the train keys. So, that way we're going to use this mode to make sure that our number of epics is none, so, it'll indefinitely run until we run out of train steps. Our eval input function will be the read data set except now with the eval. So, we'll run through the dataset only once because number of epics equals one. Our train steps is going to equal our train steps from above. Eval steps is going to equal to one, and then our min-eval frequency will be the number of steps for each epic that we're going to do, that we set up over here. Now, we also want to save this model out, so we're going to define an export strategy here using our saved_model_export_utils. I'm going to make the export strategy our serving input function. Its going to be our create_serving_input_function with our arguments passed to this train and evaluate function. From there, we're going to import TensorFlow.contribute.learned.Python.learn, the learn runner. This learn runner is actually where we run our train and evaluate loop. As you can see here, learn runner it's going to run our experiment function and it's going to write it out to the output directory. Then after we're all done with that, we'll do a batch prediction using our arguments. From there, we're going to import our shut utility here and we're going to call that with the method remove_tree. So, that way wals train will be completely fresh. So, that way when we train into it we won't have any collisions or any problems, it'll be exactly at this run. Now, we are going to pass our arguments. We're going to create a dictionary here. Our output directory will be called wals trained. Our input path will be data. Remember, we had multiple data files in there. Our number of epochs will be 0.5 in this case, number of items it's going to be the number of items that we found, the number users- the number of users we found. Our batch size we chose would be 512. Number of embeddings is going to be 10. So, it'll 10 latent features in this model, and then their topk's going to be three. Of course, you can always change these things down here because they're hyperparameters. All right. So, as we can see, it'll train. In this case it's going to train for eight steps. It's going to evaluate every 161 steps. To do that, it's going to create our training evaluate loop. It's going to create our checkpoints, all of our configurations, and then it's going to start running right here. It starts running. It starts doing it's train. It's just doing it's sweeps. It saving out checkpoints with our models here. Here's our loss function right here. So, we've got 97,000 loss for the first step. It's going to fit the step starting. Going on, saving checkpoints now for step eight which is the last step remember, because there's eight steps for training. Then it writes it all out. Here's our loss for our final step, and then we're done. Then we do an evaluation, and here's for step eight. This our loss and evaluation, and we're done with that part. So, now if we look at what's inside our wals train directory, we can see we have batch prediction.text. our checkpoints, the evaluation folder, the events out. So, we can look at this in tensor board. The export is where our saved model is trained, our graph, and a bunch of checkpoints in it. Let's have a look inside batch predictive.text. As you can see here, we have for each user our top three movies. Isn't that great. However, you might notice that these are not our original content IDs, which we'll talk about later. We'd need to maybe do a reverse mapping to get these back into content IDs we care about. So, you can also do the same learn runner as a Python module. So here, everything is basically the same except it's been copied into a model.py and a task.py, where we pass on by command line the parameters that we care about. You can see here, it trains as it did before except now using the module, and it writes the save model out to where we pointed to, and then we're done. Now, if you want to get row and column factors, you need to do a little bit more. So, once you have a trained wals model, you can get row and column factors user and item embeddings using the serving_input function that we exported in our export folder. We'll look at how to use these in the section on building a recommendation system using deep neural networks in the next module. So, you can see here, we're going to create a input json. I'm going to have a user ID, its going to be four. Index four and item ID index negative one. So, what this means is we're going to be predicting items for a user since this is negative one and user ID is positive. So, our tf.com will say, this is users I'm doing here not Items. To do that, we're going to send in our input Json and it's going to do our production out of these are the items that are the best for the user, their ratings, their perfect ratings. Same goes for the user ID. We're predicting for item four now, so, now I want to target users for that item. So, we're going to send that off to our ML Engine local predict and once again we're going to get the projected rating for those. So, now to run on cloud, we do the same exact thing. But first we're going to send our data to our bucket on Google Cloud Storage and wals data directory. Next, we're going to call the cloud training job. I'll use our our model that we created and send all that over using a basic GPU, and using our number of epochs now at 10, because now we have the power of the cloud behind us. So, we can definitely go a lot bigger and larger in our number of items and our number of users. We send that off, and it takes about 10 minutes or so for me. We'll see how it goes for you. That's that. That's the wals matrix factorization lab.