So, we have our batch predictions and we're ready to continue, right? Well, unfortunately the output is not very human readable. For instance, what is item ID 800 on the third line? Who is user ID two? Which isn't even listed, but we know it's the third line which corresponds to user index two. What's the problem here? We enumerated the users and items but we really need are the visitor ID and Content ID, which we can tie back to original data. We map from visitor ID and content ID, but we need to reverse mapping to get back from our enumerations. What we really need are visitorId and contentId in our prediction files, not the enumerated userId and itemId. Of course, we can do this reverse mapping and post-processing but, can we do it while writing the file instead? Even though we typically will do training on a periodic basis, on a nightly or hourly snapshot based on some time window, the datasets can be too large to be able to take advantage of using in-memory tools, like Pandas. So, we need a way to do this and be able to handle the scale. We should use TensorFlow-Transform to first create the group-by data set. Then you create the mapping vocabulary or visitorId the userId. Then we can use that vocabulary to reverse map when doing predictions. Let's see how that would look in a dataflow graph. TensorFlow Transform uses Cloud Dataflow in the analysis stage to create assets that TensorFlow uses in training and prediction. Here is this example's DataFlow graph. Let's walk through each step. First, since we were using Google Analytics data from BigQuery, we will read that in. Next, we'll use TensorFlow- Transform for the analysis to get the proper statistics like building the vocabularies. Then, we'll create the transform function that TensorFlow will use to do the correct mapping. This will create the vocabularies of visitorId to userId and contentId to itemId, that we can use for the map and reverse map. Now, that we have the vocabularies, we can create the group-by data sets for both users and items. Just as before when we were using Pandas, DataFlow will convert our group-by datasets into TensorFlow records. Finally, we will write our pre-process files to cloud storage so they can be used by our TensorFlow model. The training code remains the same. Instead of Pandas pre-processes in our data, the dataflow job writes up TF records containing itemId and userId. Our batch production code for addition code takes that, looks it up in the vocabulary and writes that are files that contain contentId and visitorId. We just need to make a few additions to the batch prediction code and everything will work perfectly. First, we'll define a new function, create lookup, that uses the transform function assets to read in the vocabularies. We'll call our create lookup function for both item and user vocabularies to use for our reverse mapping. Lastly, we'll look at the vocabulary when writing out the userId and itemId. This is easy and doesn't require a dictionary because we're dealing with indices here and can just return that element based on index location, a vocabulary list. Let's test your knowledge. We want a scalable way to generate predictions that directly tie back to the original data and not just enumerated indices. We should use TensorFlow- Transform to first create blank, then create blank, and lastly create blank. Choose the answer that best fills in the blanks. The correct answer is C. To use the walls matrix factorization estimator, we need to convert from our human readable user and item identifiers to enumerated indices. However, without a reverse mapping the predictions will not be in the correct form for us to easily understand them. Therefore, we want to scalable way to generate predictions that directly tie back to the original data and not just enumerated indices. We should use TensorFlow Transform to first create the group-by datasets, so that each item has a list of issues that interacted with it, with our corresponding ratings. Each user has a list of each item that they interacted with, also with the corresponding ratings. Then, we create the vocabularies to perform the forward mapping from the group-by datasets to enumerated indices, which can then be used in the walls matrix factorization estimator. Lastly, we create the predictions using the TensorFlow Transform assets to create a lookup to do the reverse mapping from our predicted enumerated indices to our original data formatting.