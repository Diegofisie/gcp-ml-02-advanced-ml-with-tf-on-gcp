1
00:00:00,000 --> 00:00:01,995
Now, that we have seen WALS in action,

2
00:00:01,995 --> 00:00:05,715
let's now see what a more productionized version of WALS looks like in this demo.

3
00:00:05,715 --> 00:00:08,715
In our later module, we'll be building an end-to-end productionized and

4
00:00:08,715 --> 00:00:12,150
automated solution using a larger Google Cloud platform ecosystem.

5
00:00:12,150 --> 00:00:15,915
For this demo, we'll be using the WALS TFT notebook.

6
00:00:15,915 --> 00:00:18,330
Let's get started. All right.

7
00:00:18,330 --> 00:00:22,785
Now, we'll be doing collaborative filtering on Google Cloud Analytics data,

8
00:00:22,785 --> 00:00:25,515
but this time operationalized.

9
00:00:25,515 --> 00:00:29,610
We'll be using TensorFlow transform to create transform functions that we're going to

10
00:00:29,610 --> 00:00:33,900
apply to our data instead of doing it the other way. All right.

11
00:00:33,900 --> 00:00:36,255
This notebook is going to use the WALS measure spec or

12
00:00:36,255 --> 00:00:39,030
refactorization approach to do collaborative filtering.

13
00:00:39,030 --> 00:00:42,635
However, we'll be doing TensorFlow transform to carry out the pre-processing.

14
00:00:42,635 --> 00:00:44,810
This way, these steps are automated.

15
00:00:44,810 --> 00:00:46,565
We're going to do a map, visitor ID,

16
00:00:46,565 --> 00:00:50,210
a string to a user ID, enumeration of indices,

17
00:00:50,210 --> 00:00:56,190
we're going to map content ID a string to item IDs also an enumeration of indices,

18
00:00:56,190 --> 00:00:58,130
we're going to remove already viewed items from

19
00:00:58,130 --> 00:01:00,440
the batch prediction output and we're going to replace

20
00:01:00,440 --> 00:01:05,125
the user ID and visitor ID in batch production output by a visitor ID and content ID.

21
00:01:05,125 --> 00:01:09,140
Therefore, we'll now be able to know how they fit back to our problem.

22
00:01:09,140 --> 00:01:13,280
First thing's first, Apache Beam currently only works in Python 2,

23
00:01:13,280 --> 00:01:16,340
so we're going to switch to Python 2 kernel in the above menu,

24
00:01:16,340 --> 00:01:18,805
you can click the drop-down and select Python2.

25
00:01:18,805 --> 00:01:23,570
Okay. Then we're going to make sure that we have

26
00:01:23,570 --> 00:01:28,190
the right environment here because Apache Beam is only currently for Python2.

27
00:01:28,190 --> 00:01:32,630
We're going to activate our Python2 environment and install Cloud Dataflow,

28
00:01:32,630 --> 00:01:36,215
install a couple other packages like Apache Beam and TensorFlow transform.

29
00:01:36,215 --> 00:01:38,540
Only certain combinations work at the moment,

30
00:01:38,540 --> 00:01:42,020
so we're going to choose this one and after that

31
00:01:42,020 --> 00:01:46,610
we're going to just check to see what the versions we have are and we're good to go.

32
00:01:46,610 --> 00:01:48,065
Before we get started though,

33
00:01:48,065 --> 00:01:50,825
make sure you reset the notebook session kernel,

34
00:01:50,825 --> 00:01:54,490
so that way all that cell can be pulled into this session.

35
00:01:54,490 --> 00:01:57,120
All right, moving on.

36
00:01:57,120 --> 00:02:01,640
As usual, we're going to import your OS and set our project bucket in

37
00:02:01,640 --> 00:02:06,335
region and we're going to put those into our environment variables in Bash.

38
00:02:06,335 --> 00:02:10,040
Make sure you set your project in your compute region,

39
00:02:10,040 --> 00:02:11,495
so this can work,

40
00:02:11,495 --> 00:02:15,020
and import TensorFlow since that's what we'll be using.

41
00:02:15,020 --> 00:02:17,330
The first thing we're going do is going to create our data set

42
00:02:17,330 --> 00:02:19,405
for WALS using TF transform.

43
00:02:19,405 --> 00:02:21,560
For collaborative filtering, we don't need to

44
00:02:21,560 --> 00:02:23,645
know anything about either the users of the content.

45
00:02:23,645 --> 00:02:25,160
Essentially, all you need to know is user ID,

46
00:02:25,160 --> 00:02:29,150
item ID and the ratings that the particular user gave that particular item.

47
00:02:29,150 --> 00:02:31,720
This is our user item interaction matrix.

48
00:02:31,720 --> 00:02:35,835
In this case, we're working with the newspaper articles from career.at.

49
00:02:35,835 --> 00:02:39,720
The company doesn't ask their users to rate their articles, however,

50
00:02:39,720 --> 00:02:45,295
we can use the time spent at session duration on each page as a proxy for rating.

51
00:02:45,295 --> 00:02:47,745
Normally, you'd also add a time filter in this, for instance,

52
00:02:47,745 --> 00:02:49,720
the latest seven days or something,

53
00:02:49,720 --> 00:02:53,495
but our data set itself is limited to only a few days so we don't have to.

54
00:02:53,495 --> 00:02:58,630
So, just as before, we create our query here to pull all of our data in.

55
00:02:58,630 --> 00:03:02,625
Okay. We have our visitor ID,

56
00:03:02,625 --> 00:03:03,900
our latest content ID,

57
00:03:03,900 --> 00:03:07,625
it's just the last piece of content they watched or read I mean.

58
00:03:07,625 --> 00:03:09,470
The session duration, how long they were on

59
00:03:09,470 --> 00:03:13,435
that content and we're pulling it out of this nested table here.

60
00:03:13,435 --> 00:03:20,855
Okay. We then get the full visitor ID and then we sum up over all the durations,

61
00:03:20,855 --> 00:03:23,120
their total session duration ID and we do

62
00:03:23,120 --> 00:03:25,100
a little bit of filtering here where I make sure that

63
00:03:25,100 --> 00:03:30,290
the latest content ID is not null because we can't be using null content.

64
00:03:30,290 --> 00:03:33,590
I'm going to do a group by full visitor ID and that content ID to get

65
00:03:33,590 --> 00:03:36,830
the full duration for that piece of content in case they've seen it multiple times.

66
00:03:36,830 --> 00:03:40,630
Also, you want to make sure that session duration is greater than zero because

67
00:03:40,630 --> 00:03:45,130
zero or less than zero would make no sense and we're going to order by latest content ID,

68
00:03:45,130 --> 00:03:48,175
so that way we have it in order of use.

69
00:03:48,175 --> 00:03:50,710
All right. We can pull that into a dataframe here.

70
00:03:50,710 --> 00:03:52,280
As you can see here, we have visitor ID,

71
00:03:52,280 --> 00:03:54,125
Content ID, and the session duration.

72
00:03:54,125 --> 00:03:56,480
Okay visitor ID, really long,

73
00:03:56,480 --> 00:04:01,080
integer here and content ID also a fairly long integer.

74
00:04:01,080 --> 00:04:04,670
Not going to be very useful for our matrix since they have to be matrix indices,

75
00:04:04,670 --> 00:04:07,775
but these are the human readable way of values.

76
00:04:07,775 --> 00:04:10,295
Also session duration here, we're probably going to want to scale that,

77
00:04:10,295 --> 00:04:12,295
probably something between zero and one.

78
00:04:12,295 --> 00:04:14,625
Okay. We want to write out our requirements,

79
00:04:14,625 --> 00:04:16,460
so that way when we're training locally,

80
00:04:16,460 --> 00:04:21,515
we're going to be using the correct package of TensorFlow Transform version 0.8.

81
00:04:21,515 --> 00:04:25,610
Now, we're going to create a Apache Beam job in

82
00:04:25,610 --> 00:04:30,290
Cloud Dataflow that will use TensorFlow transform operation that

83
00:04:30,290 --> 00:04:34,310
will create a transform function and transform

84
00:04:34,310 --> 00:04:39,045
our raw data and metadata into the ones that we're looking for.

85
00:04:39,045 --> 00:04:41,240
So, input our package as before,

86
00:04:41,240 --> 00:04:44,180
we're going to have a pre-process TFT in this case,

87
00:04:44,180 --> 00:04:46,085
rather than without TFT.

88
00:04:46,085 --> 00:04:49,060
It's going to be pulling from a query and as we

89
00:04:49,060 --> 00:04:53,710
know big query returns a table row and table rows are basically

90
00:04:53,710 --> 00:04:57,160
row dictionaries where the keys are the names of the columns

91
00:04:57,160 --> 00:05:01,085
and the values are whatever they were in those cell elements.

92
00:05:01,085 --> 00:05:05,260
We're going to set the median here that we found by using quantiles.

93
00:05:05,260 --> 00:05:07,705
To save us a little time, we just hard coded it for now.

94
00:05:07,705 --> 00:05:09,385
The results are going to be user ID,

95
00:05:09,385 --> 00:05:11,500
an item ID and the rating of course.

96
00:05:11,500 --> 00:05:16,160
Here, we're scaling the ratings so that way it's between zero and one using the media.

97
00:05:16,160 --> 00:05:21,100
Here's where we cap it, so any ratings that go above one will be capped down to one.

98
00:05:21,100 --> 00:05:23,860
Great thing about TensoFlow transform is you can use

99
00:05:23,860 --> 00:05:26,840
TensorFlow operations in the Beam code.

100
00:05:26,840 --> 00:05:30,059
So, we can mix Beam and TensorFlow,

101
00:05:30,059 --> 00:05:33,015
get the power of both to get us the best results.

102
00:05:33,015 --> 00:05:36,340
Right here, we're just making sure that if anything is greater than one,

103
00:05:36,340 --> 00:05:41,880
we're going to set it to one in our rating and then we return our result. All right.

104
00:05:41,880 --> 00:05:44,290
This all gets called by the pre-process function here that takes

105
00:05:44,290 --> 00:05:46,840
in my query and whether I'm in test mode are not,

106
00:05:46,840 --> 00:05:49,315
test mode's going to be a local run using direct Runner,

107
00:05:49,315 --> 00:05:51,340
and a non-test mode,

108
00:05:51,340 --> 00:05:53,545
when I want to do the full thing, I'll be using Dataflow runner,

109
00:05:53,545 --> 00:05:56,975
so I can have a full Cloud Dataflow job kickoff.

110
00:05:56,975 --> 00:06:00,250
Import some important libraries

111
00:06:00,250 --> 00:06:04,445
here and we're going to have a function to write the count out.

112
00:06:04,445 --> 00:06:08,255
This will be helping to figure out vocabularies for instance,

113
00:06:08,255 --> 00:06:12,890
vocabulary counts, any other aggregate stuff I can use TensorFlow transform to do.

114
00:06:12,890 --> 00:06:15,080
To create this pipeline,

115
00:06:15,080 --> 00:06:21,170
I first initialize the pipeline here called A and then I add pieces to pipeline trying to

116
00:06:21,170 --> 00:06:23,510
build this pipe out to feed my data from

117
00:06:23,510 --> 00:06:27,680
the initial source to eventually the sync to the bunch of P transforms in between.

118
00:06:27,680 --> 00:06:34,645
You can see here the first app I'm mapping my inputs to creating one one here,

119
00:06:34,645 --> 00:06:39,200
then I'm combining the counts by key and then I'm taking the key values out of

120
00:06:39,200 --> 00:06:44,120
that and I'm mapping it to just the values to return that and then lastly,

121
00:06:44,120 --> 00:06:48,100
I'll write it out to my file here with the filename that's provided.

122
00:06:48,100 --> 00:06:52,145
Okay. I'm writing TF records out just like we did before,

123
00:06:52,145 --> 00:06:53,700
but now I'm using TensorFlow transform.

124
00:06:53,700 --> 00:06:57,965
So, I also want to get all these extra assets like my transform function.

125
00:06:57,965 --> 00:07:01,070
Essentially, all that it's going to be is a key value list

126
00:07:01,070 --> 00:07:03,920
I'm getting with my index columns here so I'm making a tuple,

127
00:07:03,920 --> 00:07:05,955
and I return my key,

128
00:07:05,955 --> 00:07:07,320
my indices and my values.

129
00:07:07,320 --> 00:07:09,560
This is my feature map essentially of how I'm going to have

130
00:07:09,560 --> 00:07:12,760
the data arrayed in my TensorFlow record.

131
00:07:12,760 --> 00:07:14,730
I create a job name here obviously,

132
00:07:14,730 --> 00:07:16,810
since I want to be able to track this.

133
00:07:16,810 --> 00:07:21,950
I use Stackdriver or any other tools within Google Cloud to see this all.

134
00:07:21,950 --> 00:07:27,590
If it's in test mode, I'm going to launch a local job whereas if I'm in non-test modes,

135
00:07:27,590 --> 00:07:28,745
I want to do the full thing,

136
00:07:28,745 --> 00:07:31,850
I'm going to have my output directory not be local,

137
00:07:31,850 --> 00:07:34,445
I'm going to have it be on Google Cloud Storage

138
00:07:34,445 --> 00:07:37,955
at this path with my bucket that I'm passing in.

139
00:07:37,955 --> 00:07:42,560
Okay. So, I set some options here in max number of workers,

140
00:07:42,560 --> 00:07:44,360
so that way I can cap it off.

141
00:07:44,360 --> 00:07:47,390
The great thing about Cloud Dataflow is it's elastic so it

142
00:07:47,390 --> 00:07:50,835
can scale to whatever my problem needs.

143
00:07:50,835 --> 00:07:53,750
But if you don't want it to scale forever and ever,

144
00:07:53,750 --> 00:07:56,570
you can always put a max there so that way it won't.

145
00:07:56,570 --> 00:07:58,760
As I mentioned, if it's in test mode,

146
00:07:58,760 --> 00:08:00,350
we're going to use Direct Runner so that way it'll be

147
00:08:00,350 --> 00:08:02,715
a local run whereas if it's non-test mode,

148
00:08:02,715 --> 00:08:06,170
I'll have Dataflow Runner to actually create a cloud dataflow job and I can

149
00:08:06,170 --> 00:08:10,730
browse and see all the great graph it makes and everything and see what's going on.

150
00:08:10,730 --> 00:08:13,880
All right. I do have to set up my metadata schema here.

151
00:08:13,880 --> 00:08:16,015
So, this is TensorFlow transform.

152
00:08:16,015 --> 00:08:20,765
To do that, I'm going to essentially create column names from my list here,

153
00:08:20,765 --> 00:08:26,775
visitor ID and content ID as I loop through these in creating this dictionary.

154
00:08:26,775 --> 00:08:28,910
My values here give me my value schema.

155
00:08:28,910 --> 00:08:32,210
They're all strings, visitor ID and content ID are strings here.

156
00:08:32,210 --> 00:08:36,140
Okay. It's going to be a fixed column representation because there's a fixed number,

157
00:08:36,140 --> 00:08:37,430
it's one string per column,

158
00:08:37,430 --> 00:08:39,635
so I don't have to have any variable length in.

159
00:08:39,635 --> 00:08:41,600
I do want to update my schema here,

160
00:08:41,600 --> 00:08:44,570
it's a great way how you can add things to this dictionary through a schema I'm

161
00:08:44,570 --> 00:08:48,910
creating because session duration is not a string, it's actually a float.

162
00:08:48,910 --> 00:08:52,160
Therefore, we can't do it all in one fancy loop if we have to have two.

163
00:08:52,160 --> 00:08:54,380
So, we're going to loop through here and create the session duration.

164
00:08:54,380 --> 00:08:56,510
Once again, it's one session duration

165
00:08:56,510 --> 00:09:00,740
per column so it's going to be a fixed column representation here as you can see.

166
00:09:00,740 --> 00:09:02,320
Lastly, now that it's all done,

167
00:09:02,320 --> 00:09:04,365
I have my data schema created,

168
00:09:04,365 --> 00:09:06,000
I'm going to call

169
00:09:06,000 --> 00:09:10,415
dataset_metadata.DatasetMetadata to create my schema

170
00:09:10,415 --> 00:09:12,740
and I'm going to pass it as raw data schema here.

171
00:09:12,740 --> 00:09:15,500
That's the raw data, I'm going to use this to create

172
00:09:15,500 --> 00:09:20,260
the transformed data schema which will be different slightly.

173
00:09:20,260 --> 00:09:21,900
Okay. Now that I have that,

174
00:09:21,900 --> 00:09:23,640
I can run my Beam job.

175
00:09:23,640 --> 00:09:26,600
Creating the pipeline and inside I

176
00:09:26,600 --> 00:09:30,330
can pull my query and from above on a couple of cells above.

177
00:09:30,330 --> 00:09:32,325
If I'm in test mode, obviously,

178
00:09:32,325 --> 00:09:39,015
I probably don't want to have locally on my VM pull millions and millions of rows in,

179
00:09:39,015 --> 00:09:40,650
so I'm going to add a little limit,

180
00:09:40,650 --> 00:09:43,735
I'm going to concatenate the end of that string at limit 100.

181
00:09:43,735 --> 00:09:47,580
From there, if test mode or not test mode,

182
00:09:47,580 --> 00:09:50,100
I continue on as you can see the indentation changes.

183
00:09:50,100 --> 00:09:53,225
Raw data is going to be my pipeline P that I started with here.

184
00:09:53,225 --> 00:09:54,740
I'm going to add a piece of pipe to it,

185
00:09:54,740 --> 00:09:59,830
I'm going to read from my BigQuery source using the query that I have in standard SQL.

186
00:09:59,830 --> 00:10:02,925
From there, now, I have my raw data.

187
00:10:02,925 --> 00:10:05,040
Remember it's a pipeline, it's a Beam pipeline,

188
00:10:05,040 --> 00:10:09,830
so I'm going to take the tuple of that with my metadata.

189
00:10:09,830 --> 00:10:11,810
Remember that comes from my schema and I'm going

190
00:10:11,810 --> 00:10:14,010
to put that altogether into my raw data set.

191
00:10:14,010 --> 00:10:18,020
So, it's my data combined with the metadata about that data.

192
00:10:18,020 --> 00:10:23,705
Okay, the types and the length of the features et cetera.

193
00:10:23,705 --> 00:10:25,150
Now, is the cool part.

194
00:10:25,150 --> 00:10:29,240
Here's where I'm actually going to create my transformed data set.

195
00:10:29,240 --> 00:10:32,510
From there, I'm going to take that piece of pipe that I've left off

196
00:10:32,510 --> 00:10:35,330
on raw data set and now I'm going to add in a new piece

197
00:10:35,330 --> 00:10:37,960
of pipe where I'm going to analyze and transform the data

198
00:10:37,960 --> 00:10:41,975
set using my pre-processed TFT function up above.

199
00:10:41,975 --> 00:10:44,679
Now, remember, when we're doing training,

200
00:10:44,679 --> 00:10:48,810
we do analyze and transform that data set whereas an evaluation and prediction,

201
00:10:48,810 --> 00:10:50,290
we're going to have transformed data set.

202
00:10:50,290 --> 00:10:53,330
Because we're going to use the metrics and vocabularies that we

203
00:10:53,330 --> 00:10:56,480
find from the training data set in the evaluation impression,

204
00:10:56,480 --> 00:10:58,180
we don't want to recalculate those.

205
00:10:58,180 --> 00:11:00,200
Okay, and that will create this tuple of

206
00:11:00,200 --> 00:11:03,260
my transformed data set and a transform function that I

207
00:11:03,260 --> 00:11:05,180
can then take that function and apply it to

208
00:11:05,180 --> 00:11:08,660
the other data sets without having to recalculate all this up again.

209
00:11:08,660 --> 00:11:11,610
All right. To then take that,

210
00:11:11,610 --> 00:11:13,140
I have my transformed data and

211
00:11:13,140 --> 00:11:16,805
my transformed metadata which comes from my transformed data set.

212
00:11:16,805 --> 00:11:19,875
I'm expanding that tuple.

213
00:11:19,875 --> 00:11:22,915
Also, now that I had that transform function,

214
00:11:22,915 --> 00:11:25,135
I'm going to write it out so that I can use it

215
00:11:25,135 --> 00:11:27,790
in my TensorFlow code pull the function in.

216
00:11:27,790 --> 00:11:31,945
To do that, I'm going to name it this so I can find it nicely on my graph.

217
00:11:31,945 --> 00:11:33,340
I essentially just call

218
00:11:33,340 --> 00:11:39,055
transform_fn_io.WriteTransformFn and I point to where I want to put it in my output path.

219
00:11:39,055 --> 00:11:41,230
Okay. I'm going to name it transform function.

220
00:11:41,230 --> 00:11:43,435
Great. Now that I've got that,

221
00:11:43,435 --> 00:11:46,960
I'm going to do a group-by to create users_for_item and items_for_user.

222
00:11:46,960 --> 00:11:51,400
Okay. So, remember, each item might have multiple users and therefore multiple ratings

223
00:11:51,400 --> 00:11:55,855
and likewise each user might have multiple items and multiple ratings for those items.

224
00:11:55,855 --> 00:11:59,380
So, I create a pipeline for each using my transform data.

225
00:11:59,380 --> 00:12:02,530
To do this, I take my transform data and I'm going to

226
00:12:02,530 --> 00:12:06,460
create a map for items node into my cloud dataflow graph,

227
00:12:06,460 --> 00:12:11,380
where I'm essentially just taking the item and I'm mapping it to the item ID.

228
00:12:11,380 --> 00:12:17,350
Then I group by that on that key and then I'm going to take these

229
00:12:17,350 --> 00:12:23,440
and create my user list and write that to a TF record here of my user list for each item.

230
00:12:23,440 --> 00:12:26,680
Likewise, I do the same for items _for_user except this time

231
00:12:26,680 --> 00:12:29,770
rather than trying to map the item IDs and create a list of those,

232
00:12:29,770 --> 00:12:31,225
I'm doing it for user IDs.

233
00:12:31,225 --> 00:12:35,545
So, for each item ID we'll get a user list for that item.

234
00:12:35,545 --> 00:12:38,215
Lastly, I 'll transfer my dataset,

235
00:12:38,215 --> 00:12:40,120
I'm going to have to have an output schema.

236
00:12:40,120 --> 00:12:41,725
What's that going to be in my TF record?

237
00:12:41,725 --> 00:12:43,330
Have they had that feature map?

238
00:12:43,330 --> 00:12:45,235
We created something for my input,

239
00:12:45,235 --> 00:12:46,525
now I'm going to do it for my output.

240
00:12:46,525 --> 00:12:48,160
So, here I have my key.

241
00:12:48,160 --> 00:12:51,520
So, my key here it's going to be an integer, okay?

242
00:12:51,520 --> 00:12:56,935
It's either my user ID or my item ID depending on which piece of the pipe I'm in,

243
00:12:56,935 --> 00:12:58,869
and it's going to be fixed column representation,

244
00:12:58,869 --> 00:13:01,915
I only have one per row.

245
00:13:01,915 --> 00:13:04,090
Now, however, the indices and the values,

246
00:13:04,090 --> 00:13:08,740
the indices are also going to be int 64 and these will be in case the key is users,

247
00:13:08,740 --> 00:13:10,285
it will be items in this case,

248
00:13:10,285 --> 00:13:12,250
and if the key is item,

249
00:13:12,250 --> 00:13:14,215
it'll be users in this case.

250
00:13:14,215 --> 00:13:17,710
As you might see, it's going to be a list column representations,

251
00:13:17,710 --> 00:13:20,770
same for value since these have to be in sync with each other.

252
00:13:20,770 --> 00:13:22,795
The reason being, like I said,

253
00:13:22,795 --> 00:13:26,290
each user can have multiple items that they've interacted with,

254
00:13:26,290 --> 00:13:29,245
and likewise for items.

255
00:13:29,245 --> 00:13:32,665
Values of course and we float 32 we've scaled

256
00:13:32,665 --> 00:13:37,120
the session duration into floating-point integers between zero and one,

257
00:13:37,120 --> 00:13:39,985
so now we can use this and output that.

258
00:13:39,985 --> 00:13:41,755
Now that I've created this,

259
00:13:41,755 --> 00:13:43,450
I have to actually write it to the records.

260
00:13:43,450 --> 00:13:48,100
So, to do that, I'm going to use my users for item section of pipe that I have here,

261
00:13:48,100 --> 00:13:51,100
and I'm going to write it to a TF record at the output path,

262
00:13:51,100 --> 00:13:52,555
with this file name,

263
00:13:52,555 --> 00:13:56,080
with this coder here which is going to be example proto coder,

264
00:13:56,080 --> 00:13:58,210
using the output schema that I defined,

265
00:13:58,210 --> 00:14:00,895
likewise, same as for items for user.

266
00:14:00,895 --> 00:14:04,090
All right, I'm also going to write out my count for the users for

267
00:14:04,090 --> 00:14:06,925
items so that way I can get the number of items and the number of users,

268
00:14:06,925 --> 00:14:08,845
so I don't have to recalculate those later.

269
00:14:08,845 --> 00:14:11,050
Okay. This is basically just the number of items

270
00:14:11,050 --> 00:14:13,990
total in all my data and the number of users.

271
00:14:13,990 --> 00:14:17,380
That's it, I call my preprocess on that query,

272
00:14:17,380 --> 00:14:20,410
I'm saying test_mode false I went to fold asset please,

273
00:14:20,410 --> 00:14:22,210
it's going to write all my TF records.

274
00:14:22,210 --> 00:14:24,400
Of course you should run the next cell once it's all

275
00:14:24,400 --> 00:14:27,745
done to make sure all the records have been written out,

276
00:14:27,745 --> 00:14:30,790
and you can check out what's inside

277
00:14:30,790 --> 00:14:34,225
your transform function and all the assets it's created.

278
00:14:34,225 --> 00:14:36,400
So, to summarize, what we should have here

279
00:14:36,400 --> 00:14:38,230
is you should have a users_for_item it's going to contain

280
00:14:38,230 --> 00:14:42,565
all the users and their ratings for each item in a TFExample format.

281
00:14:42,565 --> 00:14:44,590
The items_for_users here are integers;

282
00:14:44,590 --> 00:14:46,060
they are not strings, they were strings,

283
00:14:46,060 --> 00:14:47,380
but now we've converted them.

284
00:14:47,380 --> 00:14:49,180
Item ID not content ID,

285
00:14:49,180 --> 00:14:51,895
for instance, and user ID rather than a visitor ID.

286
00:14:51,895 --> 00:14:53,530
The rating is also scaled.

287
00:14:53,530 --> 00:14:55,585
Likewise, we would have an items_for_user,

288
00:14:55,585 --> 00:14:59,140
where it's going to contain all the items and ratings for each user now,

289
00:14:59,140 --> 00:15:01,315
and it's going to be in a TFExample format.

290
00:15:01,315 --> 00:15:03,115
The items and users here are integers,

291
00:15:03,115 --> 00:15:05,050
they used to be strings, but they're not anymore.

292
00:15:05,050 --> 00:15:09,580
So, for instance, we converted content ID to item ID and visitor ID to user ID,

293
00:15:09,580 --> 00:15:11,500
and the rating has also been scaled.

294
00:15:11,500 --> 00:15:13,480
Lastly, from that right count,

295
00:15:13,480 --> 00:15:17,440
we have the vocab items and the vocab users so that

296
00:15:17,440 --> 00:15:22,450
way we can have for both, for the mappings.

297
00:15:22,450 --> 00:15:25,165
All right. Now that we have that,

298
00:15:25,165 --> 00:15:29,005
with our transform function written and all of our data process in TF records,

299
00:15:29,005 --> 00:15:30,865
we can now actually train with WALS.

300
00:15:30,865 --> 00:15:35,395
This is fairly similar to what it was in the non-operationalize WALS.

301
00:15:35,395 --> 00:15:37,660
So, I'm not going to go through all of the code here,

302
00:15:37,660 --> 00:15:41,140
but we'll just look for the parts that are different now, all right?

303
00:15:41,140 --> 00:15:42,250
So, we have our protos,

304
00:15:42,250 --> 00:15:45,500
we have our decode example as before,

305
00:15:48,360 --> 00:15:51,460
we have our remap keys so that way it can fix some

306
00:15:51,460 --> 00:15:54,610
of what batching does with the indices,

307
00:15:54,610 --> 00:15:58,645
and with my parse TF records here, okay?

308
00:15:58,645 --> 00:16:00,324
I'm going to read in my file,

309
00:16:00,324 --> 00:16:04,164
I'm going to get the file path all possible files in case there are shards,

310
00:16:04,164 --> 00:16:05,950
create a dataset out of that.

311
00:16:05,950 --> 00:16:12,985
I'm going to map my decode example with my vocab size passing each serialize example,

312
00:16:12,985 --> 00:16:17,575
and a repeated number of epoch times depending on if I'm in training or evaluation mode.

313
00:16:17,575 --> 00:16:18,880
I'm going to have a batch of those,

314
00:16:18,880 --> 00:16:24,130
batch of records and lastly fix my keys and then create it and return.

315
00:16:24,130 --> 00:16:26,055
That gets pulled from my input function here,

316
00:16:26,055 --> 00:16:27,585
where I'm parsing my TF records,

317
00:16:27,585 --> 00:16:32,250
items_for_users and users_for_item and protects row so that way I can either

318
00:16:32,250 --> 00:16:36,990
have a recommendation model that I'm recommending items for users or retargeting model,

319
00:16:36,990 --> 00:16:40,495
where I'm recommending users for items and that's it;

320
00:16:40,495 --> 00:16:43,645
I return features and none because my labels,

321
00:16:43,645 --> 00:16:46,735
as we talked about before, they switch.

322
00:16:46,735 --> 00:16:51,130
I keep my rows fixed and solve for my columns,

323
00:16:51,130 --> 00:16:54,340
and then I keep my columns fixed and solve for my rows.

324
00:16:54,340 --> 00:16:56,305
So, I'm going vice versa,

325
00:16:56,305 --> 00:16:59,005
features and labels between the two.

326
00:16:59,005 --> 00:17:03,610
Then lastly, I return my input function since that's what my customer estimator wants.

327
00:17:03,610 --> 00:17:07,660
All right, find_top_k as before,

328
00:17:07,660 --> 00:17:10,930
I am going to try to figure out this using the matrix multiplication,

329
00:17:10,930 --> 00:17:12,820
I take the top k items out of there,

330
00:17:12,820 --> 00:17:16,450
I'm going to cast those back as integer since they are indices in fact.

331
00:17:16,450 --> 00:17:18,985
But we have a little bit of changes here as you might see.

332
00:17:18,985 --> 00:17:22,000
In batch prediction, I'm going to create my lookup.

333
00:17:22,000 --> 00:17:25,120
In here, I'm actually pulling in from my assets here

334
00:17:25,120 --> 00:17:28,360
for my transform function that we've learned through the Beam,

335
00:17:28,360 --> 00:17:31,255
TensorFlow transform job we made, okay?

336
00:17:31,255 --> 00:17:35,830
This way I can do a simple mapping to use my vocab or my items and

337
00:17:35,830 --> 00:17:40,435
my users to create my original item IDs and the original user IDs.

338
00:17:40,435 --> 00:17:42,625
These are the things that are human readable;

339
00:17:42,625 --> 00:17:45,490
it's a visitor ID and content ID.

340
00:17:45,490 --> 00:17:48,475
So, that way, I can type directly back to my original problem

341
00:17:48,475 --> 00:17:52,300
and that the enumerations we created to be able to use WALS.

342
00:17:52,300 --> 00:17:54,430
Okay. To do next,

343
00:17:54,430 --> 00:17:55,540
I create a session,

344
00:17:55,540 --> 00:18:00,790
pull my trained model back in and I can pull my user factors and my item factors

345
00:18:00,790 --> 00:18:06,625
back out using my estimator.get_rows_factors and get_column_factors.

346
00:18:06,625 --> 00:18:10,270
They should be respectively number of users by the number of embedding dimensions we

347
00:18:10,270 --> 00:18:14,020
used and the number of items by the number of embedding items we used.

348
00:18:14,020 --> 00:18:16,705
Okay. So with those matrices back,

349
00:18:16,705 --> 00:18:19,480
and I'm going to take this with our top k,

350
00:18:19,480 --> 00:18:21,160
we're going to call our function in

351
00:18:21,160 --> 00:18:23,920
a map functions so we're going to keep doing this for each user for

352
00:18:23,920 --> 00:18:28,435
instance since this is going to be a recommendation model here.

353
00:18:28,435 --> 00:18:31,990
We're looping through users and we'll get the top k per

354
00:18:31,990 --> 00:18:36,385
each user that we send in our batch. All right.

355
00:18:36,385 --> 00:18:39,760
Then here's where we do our lookups that we pulled in from our transform function,

356
00:18:39,760 --> 00:18:43,975
it's a simple mapping because thankfully user ID and item ID are enumerated.

357
00:18:43,975 --> 00:18:46,210
There are already indices so therefore we can

358
00:18:46,210 --> 00:18:48,700
just point to that value that index in the list,

359
00:18:48,700 --> 00:18:50,425
and we'll get our originals back,

360
00:18:50,425 --> 00:18:53,755
we don't have to do any fancy HashMap or anything like that.

361
00:18:53,755 --> 00:18:56,185
All right. Training as usual,

362
00:18:56,185 --> 00:18:59,875
nothing's really changed since we did it for the normal WALS,

363
00:18:59,875 --> 00:19:01,960
all the same, create train_steps,

364
00:19:01,960 --> 00:19:04,420
create experiment function since this is contrib.

365
00:19:04,420 --> 00:19:07,045
All right. Create our experiment.

366
00:19:07,045 --> 00:19:10,630
Inside of that, we have our WALSMatrixFactorization estimator,

367
00:19:10,630 --> 00:19:13,210
we pass it to a number of rows or columns,

368
00:19:13,210 --> 00:19:14,680
the number of embedding dimensions we want to

369
00:19:14,680 --> 00:19:17,935
use where we want to write out the model files,

370
00:19:17,935 --> 00:19:20,050
our input function for training,

371
00:19:20,050 --> 00:19:21,835
our input function for evaluation,

372
00:19:21,835 --> 00:19:23,680
number of train_steps we want to do,

373
00:19:23,680 --> 00:19:25,690
the number of evaluation steps,

374
00:19:25,690 --> 00:19:29,515
and also how often and we want to evaluate, okay?

375
00:19:29,515 --> 00:19:33,925
We want to do it often enough that we have good metrics to maybe plot or check,

376
00:19:33,925 --> 00:19:38,425
but not too often that we're always on evaluation mode and rarely our training.

377
00:19:38,425 --> 00:19:41,530
From there, we call learn_runner and run it

378
00:19:41,530 --> 00:19:45,400
out and train it up and get our batch prediction.

379
00:19:45,400 --> 00:19:48,940
So, from there what we can do is call off our job,

380
00:19:48,940 --> 00:19:51,250
we're calling train_and_evaluate function above.

381
00:19:51,250 --> 00:19:54,880
Make sure that you remove all prior files so that way

382
00:19:54,880 --> 00:19:56,620
you don't have any collisions or we don't

383
00:19:56,620 --> 00:19:58,885
pick up where we left off, we want to start fresh.

384
00:19:58,885 --> 00:20:00,745
Once that's all done,

385
00:20:00,745 --> 00:20:04,330
you'll see in your WALS_trained local folder we made here,

386
00:20:04,330 --> 00:20:06,220
you'll see your batch prediction.text here,

387
00:20:06,220 --> 00:20:11,020
and this now will have the correct visitor IDs and content

388
00:20:11,020 --> 00:20:12,400
IDs here rather than

389
00:20:12,400 --> 00:20:16,030
the enumerations that we would've gotten before without that nice mapping,

390
00:20:16,030 --> 00:20:19,720
our checkpoints or our events file for Tensor Board,

391
00:20:19,720 --> 00:20:22,570
our graph, all that stuff.

392
00:20:22,570 --> 00:20:25,330
As you can see here, here are user IDs.

393
00:20:25,330 --> 00:20:27,130
There are those large integers again,

394
00:20:27,130 --> 00:20:31,960
that's great as we wanted and k in this instance we had it set to three.

395
00:20:31,960 --> 00:20:35,290
So, here are the top three pieces of content with their content IDs.

396
00:20:35,290 --> 00:20:40,390
Notice these are not user ID and item ID anymore, they're enumerations,

397
00:20:40,390 --> 00:20:46,195
these are back to the raw values that we can now tie back to our problem.

398
00:20:46,195 --> 00:20:50,440
Now that you have that done, you can run as a Python module the same code,

399
00:20:50,440 --> 00:20:55,990
copy and paste it all into module format into a test.py, model.py,

400
00:20:55,990 --> 00:21:01,140
call that off, and then if that all works perfectly and no errors or anything like that,

401
00:21:01,140 --> 00:21:03,075
now you can run it on Cloud,

402
00:21:03,075 --> 00:21:04,980
go for a large number of epoch series,

403
00:21:04,980 --> 00:21:07,245
so you go through the dataset many many times,

404
00:21:07,245 --> 00:21:11,160
very fast scale it all out, it's great.

405
00:21:11,160 --> 00:21:13,085
This took about 10 minutes for me,

406
00:21:13,085 --> 00:21:15,640
of course if you increase the number of epochs it'll take a little longer,

407
00:21:15,640 --> 00:21:18,085
but you can get really good training set.

408
00:21:18,085 --> 00:21:20,710
You get the row and column factors,

409
00:21:20,710 --> 00:21:23,110
you can pull them out by creating a session like we had above,

410
00:21:23,110 --> 00:21:27,605
and then you can return those matrices and you can print them,

411
00:21:27,605 --> 00:21:32,265
and again you can plot and see how they look in the embedding space.

412
00:21:32,265 --> 00:21:38,260
That's it. I hope you enjoyed using Hash for transform, it's really powerful,

413
00:21:38,260 --> 00:21:39,970
it helps operationalize this model,

414
00:21:39,970 --> 00:21:44,890
and make it much more scalable and do a lot more powerful things with it.