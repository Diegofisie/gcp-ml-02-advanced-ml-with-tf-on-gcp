Hi. I'm Ryan. I'm a machine-learning scientist at Google, and I love applying math, and machine learning to big data to better make sense to the world. Now that we've learned the basics of recommender systems, and how content based variants work, in this module, we will dive into the powerful recommendation technique of collaborative filtering. In this module, you will learn how to build a collaborative filtering recommendation system using WALS, or Weighted Alternating Least Squares. We will learn this by first understanding how to take data of the interactions between users and items, and find similar items, or users to recommend. This could be what users should watch next, what they should buy, or who they should market to. Then we'll learn how to take this new understanding to correctly prepare our data for Tensorflows WALS matrix factorization estimators input function, that we're going to create. We'll next learn how to use the trained WALS estimator to make batch predictions which are recommendations. Finally, we'll learn how to create a productionized version, and go over some of the strengths, and weaknesses of our model, and ways we can help address some of those issues. We've learned about how content-based recommendation systems can use properties, and features of items to make recommendations. If given items that a user likes, we can search in embedding space for similar items. In other words, items in the local neighborhood of the item factor embedding space, using some distance metric. This can be great because it doesn't need data about other users, and can recommend niche items. However, it usually requires domain knowledge. It only makes safe recommendations, and stays in our local bubble of embedding space, and it doesn't try things off our limited data manifold. For instance, the similarities between the properties of fruit smoothies, color, taste, acidity, texture, et cetera, can all be represented by points for each item in a multidimensional embedding space. What if we don't know the best factors to compare items with? We often think we know the factors that influence certain behaviors, or lead to certain preferences, but sometimes we can be wrong. Let's see an example. Touching the Void was published in 1988, and didn't so many copies. Then, Into Thin Air came out in 1999, and it was the best seller. A bookseller noticed that many users who were buying Into Thin Air also bought Touching the Void, and started recommending it to buyers of Into Thin Air. Soon, sales of Touching the Void started to take off, and now that book outsells Into Thin Air by more than two to one. Know that recommendations were made without knowing anything about the users, other than their buying behavior, and without knowing anything but the content of the two books. This is an example of collaborative filtering, and shows the power of that technique for making recommendations. Instead of requiring lots of domain knowledge, Collaborative Filtering can learn the latent factors contained in the user item interaction data we have. Collaborative filtering can also give recommendations some serendipity, because even though one user's data manifold or neighborhood maybe small, and only in one small bubble of embedding space, other users may have the first users manifold as a subset of there's. This means that the users are similar to each other, and perhaps the first user might actually like to branch out, and see things that the other users also liked. Collaborative filtering solves two problems at the same time between users and items. It uses similarities between items and users simultaneously. Obviously, a hybrid approach of content-based Collaborative filtering, and knowledge based recommendations usually works best, which we will cover in the next module. But for now, let's look at the awesome power of Collaborative filtering. For example, instead of just having items such as drinks represented as points in a multidimensional embedding space, users themselves too can be represented along each dimension, two embeddings within the same space. Collaborative filtering uses the interactions between users, and items to find similarities between them. This can be represented as a matrix, although usually a very sparse one, because usually there are large numbers of users and items, and most have never interacted. Therefore, to get around this sparsity problem, collaborative filtering usually uses matrix factorization. Matrix factorization in collaborative filtering starts from ratings or user interaction-matrix, where the rows are users, and items are columns. In this example, there are four users and five items, which are movies that the users may have seen on our website. Our goal is to recommend movies to each user that they would like to see. So, should user 4 see the movie, Shrek, or not. Well, that depends on the ratings. Sometimes these ratings are explicit. This could be the number of stars, the number of thumbs up, or maybe just a simple like or dislike button click. The main point here is that a user is intentionally explicitly leaving feedback for that item. You'll notice that there are a lot of blank squares, and we'll learn how to handle that later. In this example, the scores for each movie could be a one through five, where one is a strong dislike, three is neutral, and five is a strong like. Explicit feedback is usually difficult to get, but thankfully, there's also implicit feedback, which is still more information than if there was no feedback. Implicit feedback is different from explicit feedback because it is not intentionally given as a means of rating the item the user has interacted with. However, there was some type of interaction, and from that, we can infer whether the user had a positive, or a negative experience. This could be whether someone viewed a video, how long they watched a video, if a user spent a lot of time on a page, if they clicked certain areas or buttons on the page, et cetera. Much of the time we cannot give a real valued score, as in the last table. For instance, in this example, we simply assign a checkbox which will end up numerically as a one to indicate that user i watched movie j. Sometimes, there's both explicit and implicit data for user item interactions, that can be leveraged for recommendations. We think about how a user movie data might be stored in a database. There might be an index column, a timestamp column, a user ID column, a movie ID column, and then a column with the rating. In this implicit feedback example, the implicit rating from user 2 for the movie Memento was a one, meaning that whatever their interaction was, that interaction was flagged as implicit positive feedback. Now it's time to test your knowledge, about the types of feedback. If we were creating a YouTube video recommender system, where we had like and dislike data, and also the duration of the video was watched, which feedback would be considered explicit, and which to be considered implicit? Was the like and dislike data, explicit or implicit feedback? Was the watch duration explicit or implicit feedback? The correct answer is D. The like and dislike data is explicit because a user is purposefully giving either positive feedback by clicking like, or negative feedback by clicking dislike. They are telling us explicitly. Whereas watch duration is more like implicit feedback. The user isn't purposefully telling us whether they liked the video, but we can make an inference. Perhaps users that like the video we'll watch it longer, or even to full completion, whereas users that don't like the video may quickly change to something else, and thus have a short duration.