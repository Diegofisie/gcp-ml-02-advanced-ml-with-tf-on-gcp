In addition to that tensor or matrix operation approach we saw previously, we can also develop a content-based approach using tools from our supervised machine learning toolbox. That is, given a user's features and a movie's features, we can try to model the star rating that a user might give the movie. Or from a classification perspective, perhaps we will try to determine which movie in our database the user will want to watch next. In either case, we would probably want to use features of our user and features of our items in this case movies to make that prediction. For the user features, we could perhaps consider things like the user's age, location, native language, gender, or other demographics that we think might be relevant. For the movie features, it might be helpful to consider the movie genre, and duration, or the director, iterating with the Motion Picture Association or whether it's won any awards. Let's see how this might work with an example. We'll use data collected from kurier.at, a large news provider in Austria. Here's a snapshot of their website. The Center is the front page and the related items recommendations are on the right. We'll build a content-based filtering model to recommend articles to readers. The date of this lab is available on BigQuery as part of the cloud training demos dataset. After some SQL queries, we can clean up the initial table and create the following dataset that we will use for training our recommendation model. Each row of this dataset corresponds to a single visitor interaction with the website. The input features of our recommender model pertained to the features of the current article being read. For example, we'll use the current article content ID. An ID that corresponds to a single article and the category title "Author" and newness of the current article. The newness of the current article is captured by the month_since_epoch column. The visitor ID is actually a browser ID. So, a different device means a different ID. So, we won't use it for our model. The goal of our model is to predict the next article read by a visitor. This is represented by the next_content_id column. Let's see how we can incorporate each of these features in TensorFlow. The content_id is prescribed by kurier and corresponds to the ID of the current article being read. We will make this a categorical column and use hash buckets because the number of content IDs will likely change over time. We'll then use an embedding column to embed this categorical feature before passing it off to the model. Similarly, for the category column, this is also prescribed by the kurier website, though here we have a vocabulary list. So, naturally, we will use the feature column for categorical features with a vocabulary list. For the title, we will use a TensorFlow hub to embed the current article title, and specifying the module spec, we can prescribe which Hub module to use. This will add the corresponding module to TensorFlow graph we are building for our model. Because we have a finite number of authors when training the model, we'll treat the author just as we did the content_id using categorical columns with hash buckets. The months_since_epoch_column represents how new the current article is. We'll use this feature as bucketized values. Of course, now that we have the basic feature columns in place, we can add features using cross columns. For example perhaps it makes sense to cross the months_since_epoch_column with a categorical column because some categories are more or less relevant depending on how new they are. We'll leave you to determine the code for those bucketized features. The final step is to build our model. We'll start with just a single dense layer, but feel free to experiment and try more interesting architectures. Given the features we have constructed in the previous slides, in code it would look like this.