In this lab, we're going to build a content-based recommendation engine using a neural network. The idea is that we'll use features of the current article being read by a user on the career website to model what the next article is that user would want to read. So say another way, what we're going to be doing is creating a classification model with a multi-output label. The input of our model is going to be the features of the current article with a possible output being any of the other articles in the career database. This lab is going to consist of two notebooks. The Create Datasets notebook is going to build up our training and test datasets, and the actual building of the neural network will be in the content-based filtering notebook. Let's get started with the Create Datasets notebook first. It says here, the notebook builds the data that we're going to use for our content-based model, and we have a couple to-dos in this lab. Let's read through the lab and film the to-dos as we go. Our first cell inputs the necessary libraries that we've been using. We'll import OS library, of course we're going to use TensorFlow and NumPy, another one we're going to use is the Google Datalab BigQuery library since we'll be pulling the Google Analytics data from the career website. You'll want to change your project and bucket values to your project ID and bucket. Here we just set those values to be global environment variables. Let's do that now, and we set these for our G Cloud configuration. In this lab, we'll be writing to disk the various information that we're collecting from the BigQuery table. So we're going to start by defining a helper function called write_list_to_disk. It takes a list and a file name, and simply writes that list to specify the file name. In the next block of code, we're going to pull the relevant information from the career dataset in BigQuery. The first part sets up where the SQL Query should be, and to make more sense of this query, I think it's helpful to go back and look at the original table. If we grab the table name here, we'll grab just this part, we can go to BigQuery and we can have a look at what this table actually looks like. We'll compose a query, we'll select everything to start just to get an idea of everything looks like, and we'll limit our query just to be a few rows. Change this to standard SQL, we'll run the query. Then here we can see the data is actually collected from the career websites. One thing I noticed right away is that we have some nested columns. Each one of these rows indicates a single visitor's traffic on the career website. If we scroll a bit further, we can get some information about which article that user was reading, and that's contained in this hits custom dimensions table here. Looking here, we see that we have information about the author, the date, the day, the article title, the category, in this case it's news, and some other information. Index 10 here refers to the content ID of the article, that's a unique identifier for each article in the career database, and looking back to the SQL Query in our data lab, we can see that that's what we're pulling in our first cell. Looking back to the query, we can see that we're taking the index 10 which corresponds to the content_id. That's the article ID from the career database. We also specify they were only interested in hits that are of type page visits and we want to make sure that the content_id exists, so we're using a where clause saying that we want to make sure that that content_id is nominal. The final group by leaves us with a column containing each unique content_id in the database. We'll use the BigQuery library to execute that SQL command, so we're doing that here. We'll execute the SQL command, save the result, write it to a data frame, and pull up the content_id values that we created, and then we're going to save it as a variable called content_ids_list. The next line just writes that list to disk, it's writing that list we create above to a file called content_ids.text. The last two commands just print some of the sample content_ids and then tell us the total number of articles that we're working with here. So we can have a look at that. We have a few content_ids, just a sample, and we see that the total number of articles that we're working with is about 15,000. The next cell has a to-do for us to complete. The idea here is that we can just simply modify the SQL query above to get the category value from the table. Let's see how that might look. We can use a lot of this query, so I'm going to grab a lot of this here, and I'll replace this. So instead of grabbing the index 10, what we want to do here is grab the category which corresponded to index seven. Here, we'll set index to be seven, and here also the index to be seven, and instead of calling it content_id, of course, we'll call it category, and we'll group by category as well. Just as before, we'll want to execute that query, we can use a similar command that we saw for the content_ids over places to do with that. Instead of grabbing the content_id, now, of course, I'm pulling the category. The next two commands write that categories list to a file and then we're going to print some of that categories list. Here, our categories list is pretty small. It really just contains the 'News', 'Stars & Kultur', and 'Lifestyle'. We're going to do the same thing in this next SQL query. Let's just read through it. Here, we're creating our author list. We're pulling information from index two, we're saving it as first author, same kind of group by as before, this BigQuery command is similar to what we saw above, where we're executing the SQL query and pulling up the first author and saving it to a list, which would then write into a file called authors.txt. Again, we can print some of the authors and we can see how many total authors we have in our database that we're working with. Here we see some of the authors and we see that the total number of authors is about 385. Lastly, we're ready to create our train and test splits. This is a pretty big query, let's just look through it briefly to see what's happening. It starts off with a common table expression. Again, we can see this looks like in BigQuery by just grabbing this entire thing and moving over to BigQuery and having a look at it. Really it looks a lot like the table we had before, just cleaned up a bit. Really all we have now are the visitor_id, the content_id, the category, the title of the article, the author for the article. We're keeping here the month and the year in which the article was published, and then again, this next custom dimensions table has a lot of the information relevant to the article like the category and the content_id and the title, things that we're going to be using for our model down the road. So, from that table, we're going to select the necessary columns, these. This last bit is computing the date difference. So, we're looking at the time in which the article was published, given the start of the newspaper, and we're going to be saving that as a variable called months_since_epoch. We use this where clause to select out where the index 10 is not null, and then we come to our TODO. So, here, what we're asked to do is to use a farm fingerprint on the catenated values of visitor ID and content ID to create a training set that contains about 90 percent of the data. So, we've seen farm fingerprints before. Let's fill that out and see how that might look. So, to start, what we want to do is to create a concatenation of the visitor ID and the content ID, and then we'll apply a farm fingerprint to that. So, farm fingerprint, which will take the absolute value of, and then we'll mod this by 10 and ask that we take those values that are less than nine. So, if we're using a mod of 10 and we're taking the values of those modulus that are less than nine, this should give us about 90 percent of the data. The rest of this cell executes that SQL query. It captures the result and saves it to a local data frame, and then we can write it to a file called training_set.csv. We'll also going to print out the head of that data frame just to see what it looks like. So, here, we can see what our training data looks like. We have our visitor ID, our content ID, our category, the title for that article, the author, the months_since_epoch, which remember was just when the article was published more or less, and the next content ID. So, the next content ID indicates the next article read from that one visitor. That's what each row of our training dataset looks like. So, in this next cell, we're going to execute a similar SQL query to create our test set. So, if you look at the two, they're actually exactly the same. If you scroll a bit further, we come to another TODO. So, what we're asked to do here is to pull out the remaining 10 percent that we're going to use as our test set. So, let's get rid of this, and I can cheat a bit and use the previous line that I had up here. Now, instead of asking that I want my modulus to be less than nine, I'll just ask that it's greater than or equal to nine. So, now, we can see what our test set looks like. So, each row corresponds to a single visit to the courier website. We have the visitor ID, we have the content ID, the category, the title before the author, and again what we're trying to do is predict what the next content ID should be. The final two cells of this notebook are really just a way of double-checking the work that we did above. So, first, what we'll do is just do a line count for both of our training set and our test set. So, we wanted to pull out about 90 percent of our total files. If you look at this, we have about 258,000 articles and to get 90 percent, we have about 232 elements in our training set. That seems to work out okay. We can also have a quick look at what the heads of both of these files look like. So, here's the start of our test set CSV file, and here's the start of our training set CSV file. Again, we know what each one of these features relates to, and what we'll then do is use these CSV files to build our model in the next notebook. So, here we are back in the folder which has our notebooks for this lab. We just executed the create datasets notebook, and you can see now we have these extra files. The text files for our authors are categories and our content IDs, but also the CSV files for our training in our test set. It actually build our model, then we'll go to our content-based filtering notebook. So, here we are in the content-based filtering notebook. So, one thing to note is that this needs to be run with the Python 3 kernel, and you can verify that you're in Python 3 kernel here. So, we're all set. Another thing to note is that this lab uses a TensorFlow Hub. So, we need to make sure that it's installed on this VM before we continue, and so we can do that by doing a pip freeze and grabbing out any values that have the word tensor. So, we can see we have TensorBoard, we have TensorFlow, but there's no Tensor Hub. So, if that's the case for you, then you want to uncomment out this neck cells, and do a pip install of TensorFlow Hub. That's installed now, and we can again go back to the previous cell and double-check and make sure that that's there now, it is. In this next cell, we import the usual libraries. So, we have our OS library, our TensorFlow and NumPy libraries. You'll notice we're also importing TensorFlow Hub, since we'll be using that later, and our shell utility library. You'll want to change the project and buckets to your project and bucket, and then we're setting those environment variables here, same with our gcloud configurations. This next block of code, what we're doing is building up the feature columns for the model. So, to start, we're just going load the lists for the categories, the authors, the article IDs that we created previously in the create datasets notebook. So, we're creating a variable called categories list, authors lists, content IDs list, and then we're setting the mean_months_since _epoch to be 523. So, in this cell, we come to our first batch of TODOs. What we want to do here is create all the feature columns that we're going to be using for our model. Let's look at the first one. So, the first feature column we're asked to create is an embedding column for the title, and what we want to do is use the TensorFlow Hub module to create a text embedding for the article title. So, to see which hub module is most useful, we can go to the website here. We're doing text embedding. So, we'll choose embedding, and these articles are in German. So, we'll specify the German language, and we're going to use this TensorFlow Hub here. So, this actually gives us the location of the TensorFlow Hub module that we'll use for embedding the titles. So, let's go back here. To actually create the feature column, we'll use a hub.text_embedding_column. We'll set the key to be the title, and we'll set the module_spec to be the module that we just grabbed. We'll set the trainable value to be false. This is the default and it just says that we won't relearn the way to the embedding as we train. So, I can get rid of this to do. Next, we'll create our content ID column. So, for this, we want to do is to create an embedded categorical feature column for the article ID, what we're calling the Content ID. Let's see how that might look. As we mentioned in the lecture, since we know the number of content IDs, but not necessarily their values, what we'll do is we'll use a categorical column with a hash bucket. So, in TensorFlow that looks like this. We'll use a TF feature column. It's going to be a categorical column with a hash bucket. The key will be the Content ID and hash bucket size. We'll use the length of the content IDs list plus one just to be on the safe side. Next, we'll use a feature column to embed these values. So, you can either do this by just wrapping the variable we just did in a TF feature column. Embedding column, but it might be more readable if we create a new variable that's our embedding column. So, to do that, we'll create a new variable called in embedded content column and that will be again a feature column that is embedding column and feed it this as my categorical column. We also specify the dimension. So, a good rule of thumb you will see is that, you take the fourth root of the number of elements in the feature. So, if you remember, we had about 250,000 total elements in the fourth root of that is about 12. So, we'll just use ten here. Okay. So, that should take care of this to do. Moving on, we'll do the same for the author. It's also a categorical column with a hash bucket. So, we use a feature column. That's a categorical column, the hash bucket. Now, the key will be author, and our hash bucket size will be the length of the author's list plus one. As before, will create an embedded column for the author which is just an embedding column where we specify the category column. As this author column which is created here. So, it looks like this, and again we set the dimension here using similar logic. We can set to be equal to three. Okay. So, for our next do, we want to create a category column for the category. So, we only had a few categories. If you remember we only had three. So, we can use a categorical column with vocabulary lists here. So, what we'll do is we'll use TF feature column again. Category column this time with a vocabulary list. The key will be the category and we pass it the vocabulary lists which is just our categories list that we created above. We can also specify the number of out of value buckets for any values that come along that may not be in our categories list. We only had three. You don't anticipate. There being new categories down the road, but there could be. So, we'll just use one here. To finish off this feature column, we'll then wrap that category column. That's categorical into an indicator column. So, finally will have category column which will be given by a feature column, that's an indicator column and we'll just pass it this variable that we just created. Okay. So, next, we want to create a bucketize column four months since epic feature. To do this, we can use the bucketize feature column. Will first indicated TensorFlow that this is a numeric feature column in the following way. We'll go here and we can say, this is going to be a feature column. Let's say a numeric column and the key is going to be months since epic. To create a bucketize feature column, we have to specify then the source column and the boundaries. We have a bit of a hint here because the boundaries are given to us. The code it would look like this. We can use months since epic bucketize. It's going to be a feature columns before. It's a bucketize column. The source column will be the months since epic column we just created. This numeric column, and for the boundaries between just specify, we can use the boundaries that were given to us here. Okay. Now, we're to our last to do in the cell. We want to create a cross column for the months and the category. We can do this using a cross column feature. So, for that, we'll pass the two features we want to cross, his keys and the number of cross buckets as our hash bucket size. So, we can use a feature column as before. For the keys, will pass a list of the category column and the bucketize months since epic. So, here, pass our categorical column and then also the months since epic bucketize. Next, we specify what the hash bucket size should be. So, what we'll do is we'll take the length of our months since epic boundaries and we'll multiply that by the length of our categories list plus one. We'll then wrap this in a indicator column to create our final crossed months since category column. Okay. So, with all of our feature columns set up, we can then create a list called feature columns which has each of the variables that we created above. We execute that now. So, continuing on. The next section of this code will be building on the input function. So, most of the input function here is actually already written for us. In fact, all of it it is. Let's just look through it quickly and see what's going on. So, it starts off by setting up some variables that we'll need later, then we come to the read dataset function. So, this takes three inputs: the file name, the mode, and the batch size. So, continuing on. What this will do is it will return this underscore input function, which is ultimately just a dataset. To do that, we'll have this helper function to decode the CSV files, and this probably looks familiar to you. So, we decode the CSV to create our columns, and here we use the record defaults. When we call a tf.decode_csv, we pull out the features as a dictionary by zipping together the column keys and the values. Here, the column key is above, and then we pop off the label. So, our label key is given above as the next content ID. So, moving along. The next part of the code, it creates a file list of file names that we've passed above, and then we create a dataset using the dataset API, and what this will do is it'll run through that file list that we created and use the decode_csv function above to decode each one of the files. When we're in train mode, we'll set the epochs to be equal to none and we'll control the training duration by setting the number of steps to a finite number later, and when we're not in train mode, we'll set the number of epochs equal to 1. This will be what happens during validation. So, we'll pass this information to the data set genre that we've created, and that's our input function. Next, we come to building the actual model. So, the model function is here. It takes features, labels, the mode and any additional parameters that we may need. So, we'll use all of these to build our custom model, and in the end, what it will return is an EstimatorSpec. So, what we'll return at the end is this EstimatorSpec. So, reading through our model function. On the first line, we set up our input layer. So here, we're using a feature called an input layer, where we're passing the features and the parameters that we need for our feature columns. This next for loop is just going to create the architecture around neural network. We can replace this net value with the dense layer for the number of elements in our hidden units. In the end, we pass everything through a fully dense layer and compute the logits. So, we determine the predicted class here by taking the largest value of the logits. To make our predictions dictionary here, we'll want to return the article name, not just the content ID. So, above here, we use tf.gather, again, to determine what the predicted class names will be, and we'll add these to our final predictions dictionary here. If we go down a bit further, we can see the loss that we're using to train the model. Here, it's a sparse_softmax_cross_entropy. We're passing it the labels and our logits, and for the performance metrics, we compute the accuracy for our model. So, that's computed here, and then we come to our TODO. So, what we're asked to do here is to compute a top 10 accuracy for the model. Again, we can do this using the libraries within TensorFlow, and there's actually already an in_top_k function. So, what this would look like is we can use tf.nn.in_top_k. We pass the predictions, which are our logits that we computed above, and we pass the targets, which are our labels. We also specify what k should be, and so here it would asking that k is equal to 10. So, what this means is that we're computing an accuracy, which says, "Looking at our top 10 articles that we would predict, does the actual target lie anywhere in those top 10?" To find the accuracy, we'll wrap this in_top_k with a metrics mean. Okay. So, this computes our top 10 accuracy. Looking a bit further down, we can see that we want to add this new metric to our metrics dictionary. So, to do that, we just add in a key name. So, we'll call it top 10 accuracy, and we'll pass it the value that we've computed here as the value. So, let's just get that to do. Going a bit further, we want to add this metric to our TensorBoard summary. So, to do that, we can just tack this on by using tf.summary.scalar. We're passing it the top 10 accuracy, and the value it's going to use to return that will be our top 10 accuracy that we computed above. So, the rest of the cell just sets up the training job. It sets up our optimizer, and the training operation, and then returns the estimator along with the mode, and the loss that we're using and the training opt that we defined above. So, let's evaluate that. Okay. So, this next cell carries out our train and evaluate steps. Here, we define our output directory, and next we clear out the output directory just in case it isn't our first time to do training, and then we define our estimator using the estimator API and the model function that we created above. Also, here, we can set the parameters of our model. Right now, it's set to have three layers with 200, 100 and 500 nodes each respectively. This can easily be changed to make the network larger or smaller as we wish. Also, the number of classes is set. So, the number of classes is gotten by taking the length of our content IDs list. Next, we set up the training spec using the train and input function and setting the finite number of steps for training here, and then also the eval_spec using the read dataset function for the input function as before, and here we can set the steps to be equal to none because above, if you remember, we saw the number of epochs to be just one. Lastly, we train and evaluate, passing the estimator and the train_spec and the eval_spec that we just defined above. So, that gets started. It takes a little bit of time to train. What we'll do is we'll go over to our solutions and see what the result looks like. Okay. So, this has been training for some time and we can see what the results look like. We asked for 2,000 steps, which is quite of a lot. But if we scroll to the end, we see that for our last step, our loss was about 4.9. Our accuracy wasn't very high, but our top 10 accuracy was much better, as you would expect. It's about 30 percent for the top 10 accuracy. So, to finish with this lab, let's go back to our previous lab, and we'll complete the rest of TODOs. Okay. So, lastly, what we want to do is to make predictions with our model. So, to do that, we'll pull out just five records from the training set and create a new file called first_5_content_ids, contain just the article IDs for those articles. So, you can see that being done here. This is what the first_5.csv looks like and the first_5_content_ids pulls out the content ID for each one of these entries. Continuing along, we see one final to do, and then here what we're asked to do is to use the predict method to make predictions on the model that we just trained. So, let's see how that would do. We want to make predictions for all of the examples contained in the first_5.csv file. To do that, we call the predict method on our estimator. So, we have our estimator.predict. For our input function, we'll pass our read dataset function, and we'll read just the first five values. So, we'll pass first_5.csv, and let me specify the mode. Here, we're going to be in predict mode. So, I set my mode keys to be predict. We want to make a list of these predictions. So, we'll wrap this in a call to list and it makes the predictions on the elements in first_5.csv. To compare predictions with the actual articles, we'll go back to BigQuery. So, what this cell does is we use our NumPy library to create our recommended content IDs, which will decode each of the elements in our output list that we created above, and then we'll also pull out the content IDs from the first_5_content_ids file that we made previously. This next cell has two queries; one using the recommended article ID and one using the current title ID. Looking through the query, what they do is they return either the current title or the recommended title from the courier database. So, our first query, does the queries before, only we're asking that the ID is the recommended content ID here, by passing that first value as argument here, and then on our current title SQL query, looks very similar, only now we're passing the first content ID into our argument of our [inaudible]. This will return the recommended title and the current title. So, when we execute that cell, what we'll get back is the current title the person's reading and what our recommended model would suggest as the next article to read. The last thing we can do is monitor TensorBoard to see how our accuracy and the top 10 accuracy performs through training. So, we'll execute this cell, and we can click here to look at TensorBoard, and what we see is the evaluation metric for the accuracy and our training, even though trains a little bit more wobbly, we'd expect that. They're both increasing over time, and down here we can see our top 10 accuracy. It also improves with more training. Okay. So, that wraps up the content-based filtering lab.