Okay. To get started with this lab, I'm going to clone the code that is used to train a convolutional neural network using MNIST data. To clone the code, I will simply create a new notebook and run the Git Clone command. All right. Once the command is done executing, I will see a training data analyst directory and will need to navigate into the training data analyst, courses, machine learning, deep dive and I will work with the labs notebook in the directory listed here on the screen. So, to get started, make sure you open the MNIST models notebook. Once you're there, here is a few set up steps that you need to perform. First, you need to make sure that the project in the bucket variables in the notebook are set to match the Google Cloud Project in the Cloud Storage bucket you have already set up earlier in this course. In my case, both the project and the bucket had been set up with the same ID. So, I will go ahead and replace the existing values and notebook with a corresponding value for the ID. Since my bucket was created in the US East1 region, that's what I'll use for the region variable. As this part of the lab focuses on the convolutional neural network model, I need to make sure that I have a CNN specified. Next, I hit run to set the environment variables and then use those environment variables to configure Google Cloud settings for the current project and the compute region. Alright. The next step is to run this lab as a Python module. You can take a look at the instructions, and in the code immediately below, you will notice that the code relies on the models located in the MNIST trained directory. The first step is to remove any of the previous models that may have been trained earlier and then train a new model using the MNIST dataset. Before you can run this code, you need to make sure you complete the to-dos in the model.py file. Let's go ahead and open that file. Once it comes up in the browser, you'll notice that in the beginning of the file, you have a variety of alternative model definitions. But the one that's used in this lab is in the CNN model method. There are few hyper-parameters that are provided for you as defaults for training the convolutional neural network on MNIST data. These are kernel sizes for the first second layer the CNN and also the number of filters, in other words, total number of distinct kernels per CNN layer. Here, the first layer uses 10 filters and the second layer uses 20 filters. The to-dos for this code snippet are to create a second convolutional layer that takes as input the output of the previous max-pooling layer and generates a result. So, let's go ahead and set up tf.layers.conv2d taking p1 as an input and also specifying the number of filters to be the hyper-parameter of value per nfil2. In addition, you have to specify the kernel size. In this case the kernel size is also based on the hyperparameter. So, I will use the size equal to ksize2 and the rest of the parameters are set as defaults. So, this rise should be one using the same padding. In the activation function, it's going to be tf activation relu. The next to-do is to take the output of the convolutional layer and to perform a max-pooling operation. So, the code should use the max-pooling method from tf layers package and take the output of c2 and apply the pool size equals two and strides equals two. Now, just as a sanity check. Think about the dimensions of the changes. Since padding is the same and stride is one for the second convolutional layer, the output of this layer is going to have the same batch size as indicated here with the question mark, 14 by 14 due to the same padding and the number of filters in the layer was 20. So, this means that the output will be batch size by 14, by 14, by 20. The output of the next layer, the pooling layer, is going to depend on the two-by-two. Pooling kernel was the stride of two, which means that the size of the output of this layer will be batch size by seven, by seven, by 20. To confirm, notice that the output of the max-pooling layer should generate 980 values and seven times seven is 49, and that times twenty 20 will give you 980. So, the expectation for the total number of outputs from this pooling layer is correct. At this point, the implementation is done. So, go ahead and save the file. Once the file is saved, you can go back to MNIST models notebook to run the training of the CNN model locally and validate that the code was implemented correctly. Since the training is going to take a few seconds, you will see the video fast forward to a point after the training is finished. Alright. You can be sure that the training is completed and you can see these messages confirming that the code downloaded the MNIST data. Although you see the messages in pink that look like errors, in reality, these are just warnings from TensorFlow due to some code applications. You can scroll down the output section with the training message and notice that the end of the training, a model is written to a local file system. This means that the training is completed. In the next notebook, take a look at another G Cloud command that will set up a job on Cloud ML Engine for training. Go ahead and run this job to get training on graphical processing units, GPUs, that will get you results faster. The output of this command should be an ID of a job for the Cloud ML Engine. As from the last time you use Cloud ML Engine, you will take a few moments to start the job and to return the job ID. So, the video will fast-forward to a point where the ID is ready. Okay. Now, you can see that this job has been queued. The message is about removed files, simply confirm that any old versions of previously trained models had been deleted. When the output reports the job is still active, it means that cloud ML Engine is doing the training. You can confirm that the training operation is running by opening the Cloud ML Engine user interface and check that the model exists. Let's take a closer look at how to do this. Return back to the Google Cloud Platform dashboard and navigate to Cloud Machine Learning Engine. From here, you can see that there's a job that's doing the training. If you go ahead and click on that job, you can monitor it in more detail. As you can see the training inputs show the same configuration parameters as you specified in the Jupiter notebook. The job takes almost 10 minutes to train. So, this video will jump forward to a point when the job is complete. Okay. So, now the job is done, and for me, it took just under eight minutes to finish training. At this point, you can return back to the MNIST model notebook and also launch TensorBoard to monitor how training went with this model. For that, I'll go ahead and execute this next step in the notebook that starts an instance of TensorBoard, and I'll click on the resulting hyperlink to open a new browser tab with TensorBoard and to review the details of training. From TensorBoard, you can find out the results of the loss function and get the accuracy details for this model. To make the metrics easier to understand, I like to smooth out the functions. So, here you can see that loss value was about 0.1 at the end of training. To stop TensorBoard, simply run the next step on the lab notebook. Okay. At this point, you're ready to deploy the CNN based model and use it to generate predictions. To deploy the model, you will run another G Cloud ML Engine command and this step will also take some time. To confirm that the model is being deployed, you can return back to Cloud Machine Learning Engine and navigate to the models section of the UI. Here you can see that there's already a model called MNIST and if you click on it, you can monitor the process of deploying this model. Alternatively, you can return back to the notebook and wait for the cell to finish running. So, you don't have to wait, this video is fast-forwarding until after the step is done. Now, the model deployment is complete and you can see that there's a CNN model that has been deployed as a service as an API on Cloud Machine Learning Engine. To test out the predictions from the API, you need to return back to the MNIST model notebook, confirm that the model was actually deployed and then in the next code cell, you'll find out how to submit an image from the MNIST dataset to test our predictions. Once you can see an image of the one digit at the bottom of the code block, go ahead and submit the image to the API. To do that, you call the G Cloud ML Engine predict. As you can see in this case, once you've submitted the image, you get back a prediction of class one which matches the digit on that image. All right. That's it for this lab. Thank you for watching.