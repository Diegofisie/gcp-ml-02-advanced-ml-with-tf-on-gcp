Hi. My name is Carl Osipov, and I'm a program manager here at Google. In my role, I work with customers to help them succeed with Google Cloud for machine learning training and certification. This module will introduce convolutional neural networks, or CNN's for short, and gets you started with implementing CNNs using TensorFlow. Since 2012, CNN-based systems achieved unparalleled performance on tasks like image recognition, and even at playing the ancient board game of Go against the best human champions. The plan is to start with just the highlights of history covering how CNNs came about, and why and they help detect visual patterns in images. Then you will learn about what makes CNNs different from traditional neural network architectures, and specifically how convolutions can extract features from images by imitating a filter sliding over image pixels. After that, you will dive deep into the key parameters you should consider when setting up your neural net including padding, stride length, activation functions, and the number of channels. In addition to convolutions, you will see that many CNN architectures use pooling layers to help detect patterns regardless of their location in an image, while reducing the computational requirements for training CNNs. You will explore sample TensorFlow code for creating convolutional layers in implementing all of the filters and pulling settings covered in the module. Lastly, you will review the complete CNN architecture end-to-end, and get started was a TensorFlow code example in the upcoming lab to implement your own convolutional neural network. So, what are convolutional neural networks? To prepare to answer this question, it's important to recap two key points introduced earlier in the course. First, neural networks for image recognition can get complex with hundreds of millions of weights, even for relatively small images. For example, many front-facing cameras on smart phones take pictures consisting of approximately eight megapixels. If each of the pixel values was connected to just 100 neurons, that would mean having to train billions of weights, and that's just for the first hidden layer of the network. This problem is caused by the density of the connections of the neural network layer. In this module, a dense layer is used to describe a neural network layer where every input has a weighted connection to every neuron of the next layer, as shown in the diagram on the screen. Second, in a dense layer, it does not matter which neuron is trained to process which input values. For example, with the deep neural network model for amnesty, which you saw earlier in this course. If you randomly reshuffle the order of pixels in the images, taking care to reshuffle the corresponding weights the same way, the classification performance stays the same. However, when human beings look at images where the pixels have been randomly shuffled, the image just looks like noise. So, for image recognition, how pixels are placed next to each other is critically important. Prior to 2012, many image recognition systems used complex feature engineering approaches to convert input images to features that can be used for machine learning. As you know from the earliest specialization, feature based approaches preprocess raw input data to extract various patterns or features that simplify the training on a machine-learning model for specific tasks like object classification. For example, in an image of a cat, the features could be eyes, ears, nose or whiskers. A feature of a cat side could in turn be defined in terms of lower level features like patterns of bright color next to black color of the pupil, when an elliptical shape feature for the contours of the eye. These used to be done using image pre-processing operations such as Gabor filters or collocation matrices. So, here are the pre-2012 models which relied heavily on the [inaudible] practitioner to engineer and define which features for the model to identify. Since 2012, designers of image recognition systems have started to rely on convolutional neural networks, made popular by systems like AlexNet and Google's own inception network. In addition to achieving high performance on image recognition tasks, CNNs have dramatically reduced the need for feature engineering as part of the image recognition system design. Instead of manually designing image processing filters, CNNs allow the weights of general purpose filters to be learned during training itself. In the remaining part of the module, you will learn about two new types of deep neural network layers; convolutional and pooling. You will explore in more detail what are convolutions and convolutional layers, what types of features they can extract, and later, you will see how sequencing these layers and combining them with a traditional neural network architecture helped improve the performance of image recognition systems.