Let's do a recap of the entire architecture before the next lab. Recall, this you are given an image as input, and then you have kernels acting as feature detector filters convolve over the image and create outputs for the first layer. If you use same padding, the size of the output will be the same size as your image. Otherwise, the output will be smaller for kernels larger than one by one. You can down sample between convolutional layers using max-pooling to reduce the data size. Recall that these pooling layers reduce computational load, memory usage and total number of weights. As you keep adding kernels, the key point point remember, that you are gaining channel depth while you're shrinking the height and width dimensionality of the image. This process continues until you have a very deep album that you can then pass into a dense deep neural network layer for classification and for output as a final step. Now, it's time for you to build a convolution neural your network on a MNIST dataset, to see if we can get better accuracy and performance than the linear and DNN models you created previously. The steps will be similar to what you did before, except was a CNN the sign. In this lab, you will import the training dataset of MNIST handwritten images, reshape and preprocess the image data, setup you CNN with 10 classes, one for each possible digit zero to nine, create your architecture whose convolutional and pooling layers as well as softmax function, define and create your EstimatorSpec in TensorFlow to create your custom estimator, define and run your train and evaluate function to train against the input dataset of 60 thousand images and evaluate your model's performance. Don't forget, you have multiple attempts to start and stop each lab, so don't worry if you don't make it on the first time. Also, as a tip, you can take a second attempt at a lab even if you just wanted to experiment with the code before watching the solution video later.