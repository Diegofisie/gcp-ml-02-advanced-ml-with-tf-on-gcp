So, what is a convolution? You can think of it as an operation that processes a collection of neighboring input elements and in case of image recognition, this means processing a group of pixels that are located close to each other. In the animated diagram on the screen, you can see an example of a convolution operation, shown as the yellow square window that overlays in slides across parts of the green input image. As you take a closer look at the animation, notice that there are also a set of nine weights shown as binary numbers in red on the bottom right of each of these smaller yellow squares. These weights arranged as grid, make up what's known as a convolutional kernel. At every frame of the animation you saw earlier as the convolutional kernel slides across the training dataset image, the values of the weights stay the same or in other words the convolutional weights are shared across different parts of the input. You will learn more about weight-sharing later in this module. After nine pixel values from the training data set image are multiplied by corresponding nine weights of the convolution. The sum of the products is stored as the output of the convolution, which is shown as the numbers in the pink square on the right. You should also take note: that when you apply a convolution kernel to an input as is the output of the convolution, will have a smaller shape compared to the original input. The right side of the animation shows an example, the size of the input is five, the size of the kernel is three. So, the output shape is reduced by kernel size minus one, which means by two. In general, if you use square kernels you should expect that the output shape will be smaller by kernel size minus one along each dimension of the input. To confirm your understanding. Here's a more detailed walk-through of a single convolutional operation, that results in a value of three shown on the right. If you count just the waste that have the value one in the free by free convolutional kernel, you'll find that there are five of them and they're arranged in an X-like shape or other ways of the kernel, have a value of zero. The free ones when added together, they produce the three shown on the right. Most convolutional neural networks use multiple kernels, each having its own weights. For example; The early AlexNet use kernel sizes from three by three to 11 by 11. However, more recent CNN architectures rely primarily on three by three kernels. You've probably heard of processors like TPUs, TensorFlow processing units or GPUs, graphical processing units. These types of processors can be much faster than traditional CPUs at working with CNNs, because they can compute the values of the convolution operation for all the different positions of the convolution kernel in parallel. You will see examples of how that works later in the module. Now, it's time for a quick quiz. Suppose I tell you, that the first kernel, the tax horizontal edges because its maximum at locations where the top row is much higher in value than the bottom row and zero where the top row and the bottom row are about equal in value. The first kernel, finds areas of the image that exhibit sharp changes in intensity between nearby rows. What about the kernel in the bottom? What kind of patterns does it match? If you said it would find pixels that are brighter than their neighboring pixels, you're right. The filter on the bottom will match bright spots, that are exactly one pixel in size. So, at this point you have seen how convolutional kernels enable neural nets to process those parts of the input that are in close proximity to each other. But why are convolutions useful? The images here. Illustrate a practical example from the last quiz of the convolutional kernel designed to detect horizontal edges. This photograph is of Grace Murray Hopper, one of the most famous pioneers in the field of computing. She left the Lessing legacy for computer professionals worldwide, after she made popular the term "debugging" to describe effects to a problem caused by math in a computer relay. Anyway. Her photograph is on black and white. So, it consists of pixels that represent grayscale values. Zero is completely black, 255 is completely white and the larger the numbers in the range from zero to 255, the wider they are. Try thinking about what happens, when you apply the convolution kernel shown on the screen to the image matrix. The kernel will produce a high positive value only in the places of the image matrix, where they're bright pixels. In other words, pixel values that are closer to 255 than to zero. Below the white pixels, they're darker pixels. Well, this precisely describes the horizontal edge. Take a look at the top of the computer in the upper left of the original image and notice how the horizontal lines that run along the top, match the horizontal edges in the image processed by this convolutional kernel. If you rotate the convolutional kernel from the previous example by 90 degrees, you can create a vertical edge detector. Notice that this kernel outputs a high value whenever, they're brighter pixels stacked on top of one another and the values to the right of them are darker pixels. In this case, the horizontal edges in the upper left are missing entirely from the processed image. However, the vertical space in between the parts of the computer are clearly visible on the processed image. In practice CNNs use many kernels, each having a distinct set of weights. A filter is an application of a convolution kernel to the entire input. In the two example on the screen, there just a horizontal edge filter or a vertical edge filter. What happens if you add these two filter outputs? Well, you will get bright pixels, where either of these filters had to match. Of course you would get very bright pixels where both filters are activated. A CNN can be trained to learn a variety of different filters, by finding kernel weights that improve the CNN performance on tasks like image recognition. Later in this module, you will see how sequences of filters in a CNN, can be trained to combine lower-level filters like edge detectors into higher level filters that can detect meaningful features like cats, whiskers or ears. Here's a quiz. What does the process of sliding a kernel across an image called? Is it a convolution? A Confusion matrix? A contortion? A curve detection? The correct answer is convolution, which is where CNNs get their name. So, a convolutional layer in a neural network, is defined as a collection of filters, each having a distinct set of kernel weights that are adjusted by the training process. Okay. Up until this point, I have been making an important assumption about the input to the convolutional layer. The images you have seen previously in the module we're in grayscale, meaning you can easily represent them using a tensor of rank two, which is just the matrix. To represent color images, a single matrix is not enough. You need a tensor of rank three, that can represent multiple matrices stacked on top of one another. With color images, these matrices are usually the ones for the red, green, blue channels. The kernel depth, the cube you see convolving is always equal to the number of input channels, three in this case for RGB. Now as output from the convolutional layer, the output layer has depth equal to the number of kernels applied. Let's say two kernels were applied here on this RGB image of a flower, which gives us a new image of size 296 by 296 by 2. We'll talk about that reduce dimensionality from 300 down to 296 in height and width, in the next section on padding.