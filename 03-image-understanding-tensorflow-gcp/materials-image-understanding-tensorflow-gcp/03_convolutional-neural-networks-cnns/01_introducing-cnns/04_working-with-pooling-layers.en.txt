Another way to reduce the amount of data that needs to be processed by CNN, is to apply the maxpooling operation. Although maxpooling has fallen out of favor was designers of more recent CNN architectures like resonant, it's still often used in combination with a convolutional layer. The maxpooling operation is like a convolution that returns the maximum value out of all the input data values passed to a kernel. In the graphic, you can see that six is the maximum value when the kernel is in a position shown in pink. Eight is the maximum value for the kernel position shown in green and so on. When this type of maxpooling operation is applied to the grasshopper image, the image size is cut in half along both the horizontal and vertical dimensions. Also, keep in mind that a pooling layer that relies on maxpooling does not require any weights, since the operation only cares about the largest value out of the input values evaluated by the kernel. This means that during training none of the parameters of the pooling layer need to change. Maxpooling operations are just part of a pooling layer in a network. In most cases, the inputs to the pooling layer are the outputs of the convolutional layers as you can see on the screen. TensorFlow provides the TF layers max-pooling two D API, to add the max-pooling layer to a neural network. The values of the parameter shown in the code snippet, configure the maxpooling layer with a two-by-two kernel in a stride of two. These specific values are commonly used for pooling layers. The last thing to notice about this code snippet is that instead of using the parameter name kernel size as with a convolutional layer, you need to use the name pool size. Here's a quiz. What are the smaller red numbers represent in this image you've seen before? The weights of the particular kernel we're applying, in other words, what feature are we detecting. The intensity of the pixel for that area of the original image, the channel depth of the image. The answer is the weights of the particular kernel we're applying. In other words, what feature we're detecting. Given the above, what is the value of the output that the kernel will generate for this part of the image? The correct answer is four. Remember that you just multiply the kernel weights against the input image values and take the sum.