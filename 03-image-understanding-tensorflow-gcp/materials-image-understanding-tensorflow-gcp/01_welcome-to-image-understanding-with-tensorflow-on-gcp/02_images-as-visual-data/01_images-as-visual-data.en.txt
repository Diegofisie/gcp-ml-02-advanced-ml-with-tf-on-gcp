In this introductory module, we'll review the business applications for computer vision and modern image classification models. Then will break down images themselves as just a collection of pixel data which has a height, a width, and even a depth of color intensity. Lastly, we'll brainstorm and compare the limits of analyzing unstructured data with traditional methods. While capturing an image into his digital form has been around since the 1970s, getting computers to really understand the subject matter or the meaning behind all those pixels, has only recently made a huge strides in both research and application. It's only within the last few years with the advent of cloud computing and specialized processors, that we can now classify many high-resolution images quickly at scale and with high accuracy. By 2006, more than half of all phones had integrated cameras. Now, a little more than 10 years later, there're arguably more cameras and other sensors than the total number of people on the planet. These phones in your pockets can capture high resolution, millions of pixel images and automatically upload them to the cloud. Just think of how many pictures you've taken in the last year or have seen when browsing online. To give you a sense of the scale of visual data, it's both images and video, consider that as every second passes, five hours of videos are uploaded to YouTube. That's 300 hours of video every minute, and that's just data, video data to YouTube. For images, over 1.2 billion photos are uploaded every day to Google Photos, which is roughly 13,000 of high-resolution images every second. Let's do a quick walk-through of how some companies and organizations are applying intelligent computer vision and ML classification models for their businesses. First, you might be Airbus and you're using image classification and recognition techniques to differentiate between clouds and snow cover. Which is critical for planning safe flight routes and tracking weather patterns. If you're stumped like I am, the clouds are actually in the upper right-hand corner of the right image highlighted in red. As you can see, the models have gotten really advanced in the last decade. Let's take a look at another. You might be Kewpie, and use machine-learning to differentiate between bad potatoes and good potatoes while making baby food. What's really impressive to me is that the team uses Tensorflow model with pictures of diced potatoes like the ones that you see here as the image input, not even the whole potato. After training, the model was able to pick out the bad potatoes with near perfect accuracy, which reduces the overall volume of cases for the plant workers to review themselves. Again, it's with the review of the foundations of ML that the team provided a labeled image of good high-quality data and bad diced potatoes and not hard-coded rules. Like, if it's a green potato if you see that then it's bad. The model itself figures the rules between your training dataset that's labeled and ultimately the correct output. Next up, you might be an economic forecast firm looking at track the global fleet of container ships via satellite imagery. Knowing the amount of cargo being carried, might improve your economic forecast days or months ahead of the official numbers, you can track those ships. After this course, if you want your practice your own hand and image recognition on satellite imagery, I'll provide a link to the public datasets and a cool weather prediction tutorial that [inaudible]. What about classifying medical images to help out doctors? You can diagnose medical conditions like diabetic retinopathy earlier when it's easier to treat and prevent blindness. Diabetic retinopathy is one of the fastest growing causes of blindness. With nearly 415 million diabetic patients at risk worldwide. If caught early, the disease can be treated. If not, it can lead to irreversible blindness. The Google Teams model showed that the results that ML algorithms performance is on par with that of ophthalmologist who then use it in assisting and making diagnoses. I'll provide links for each of these different models in the resources for your additional reading. Now, the key takeaway here is that image classification can now automate tasks that may save or even assist human team, and recently, these models have been even outperforming humans and some particular image domains. Image classification as a field, is far more than just a binary classification tool. Later on in this course, we'll experiment with a pre-built vision API model, which allows you to pass through a JSON request and get back a ranked list of associated labels for your image. It's pretty good. Also, if you have more than one subject matter in one photo, it can draw bounding boxes and classifying pieces of the image as well. Modern image classification models can even generate captions that describe what's going on in an image like map of dependencies, two hockey players are fighting over a puck. Here's important to call up that even the best models can make mistakes with their predictions, like a road sign here captioned as a refrigerator filled with lots of food and drinks. As you saw, there's a lot of really impressive use cases for machine learning, like detecting objects and images, helping you to detect diseases, even building real-time road data to enable cars to drive themselves. But ML can also be used in more playful ways too. Through a pose estimation model, a Google AI experiment called Move Mirror matches your real-time movements to hundreds of images of people doing similar poses from around the world. Feel free to try it out yourself and have some fun and get inspired. Then tune back into this course and learn how image classification models extract features from those images and how you can make the magic happen.