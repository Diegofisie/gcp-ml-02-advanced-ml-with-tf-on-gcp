Now, before we build our own image classification models in this course, we firstly do a dress a break from the traditional model inputs that we've been working with so far. Welcome to the battle of structured versus unstructured data. So here's what you've seen before. A single feature vector with a small number of components or elements in that array. Again, if you're already familiar with this structure because these vectors are the tensors that you've been feeding into your early models tensor flow. By way of contrast, image is going to look something like this, it's a collection of pixels, it has a height, a width, the images has color, it also has a depth. A common image encoding scheme for images is RGB, a red, blue, green layers which each have values ranging between zero and 255 for each and every pixel representing the color intensity of that pixel. Then digital world zero, zero, zero is black, 255, 255, 255 and all three RGB channel depths is white. So, what color do you think 255, zero, zero is in RGB? If you said it's super intense red for that one pixel, you're exactly right. Now, there's often a fourth component that represents the opacity or transparency of that pixel, but we'll leave that one alone for now. If you consider the total number of floating point values representing an image, it's simply the product of the number of channel layers, three for our RGB case, multiplied by the area, the height and width of each layer. With modern cameras, the pixel dimensions and area alone can easily be in the millions of pixels. For example, an eight megapixel camera creates images with eight million pixels each. Multiply that by the three channel layers, you've got over 23 million data points per image. While it makes for amazingly high resolution photos that look great and have really good color depth, it greatly increases the input size for an ML model. This can lead to performance problems like really long training times or even insufficient computing power to complete model training at all. But, thanks to Moore's Law and the advances of computing power, we are able to handle large image input sizes if we had to brute force it. The real challenge though, in which unstructured data like images is different from structured data has to actually do with locality. Now to demonstrate this, consider these pictures. The left hand side image shows a cat, and the right hand side shows a similar photograph. One of them is translated a bit by zooming in same output dimensions of the image. That's still the same image of a cat right? Well, not necessarily, to a computer you're comparing pixel for pixel, it's different. More the computer doesn't see images like the way we do, it just gets an array of numbers representing colors, their intensity and their position. Here for illustration purposes, I've overlaid the two images to show how different the location of the pixels are. So how is this any different from what we've used to compare two data points in our structured data examples? At a differentiating point that'll make this all clear is how we compare one vector of data with the other. One of the most common ways of comparing vectors is the simple euclidean distance. In euclidean distance, we consider the element wise difference between each of the vectors components and the distances between them grows as the sum increases. Lower the distance, the more similar those vectors are. With structured data, euclidean distances behave exactly as you might expect, euclidean distance often correlates with the semantic space. Assume we have two vectors representing a cat's height and weight, in the table that you see here. Say where to compare two similar house cats with the euclidean distance formula. We were relatively low 2.02, keep in mind an output of zero is the exact same vector. This tells us that these feature vectors are quite similar. What about comparing a house cat vector to that of a tiger? As you might expect comparing these feature vectors gives us a much greater distance of 313, which is kind of expected, I mean a tiger weighs over 300 pounds and a normal house cat is about 10. Why can't we just compare feature vectors with a simple straight line distance for images since image data is just numbers in a vector behind the scenes? Here's the critical point that we talked about briefly earlier, locality. To demonstrate this again, consider these two cat pictures. Top one shows a cat, bottom one shows an image that we zoomed in on. The cat on the bottom is just shifted simply left. Again, is it the same photo of a cat or not? Well, again not to a computer. If you were to apply the same procedure of doing the straight line distance between two of these vectors, you will get very large deltas. So, a very large overall distance, even though you as an observer know it's essentially the same cat. Now, this reminds you of a model memorizing or over-fitting for what a cat is, and getting confused where we even slightly vary the image, and then you've understood the whole challenge here. It turns out with the visual data, there are many operations that behave in a similar manner to translation. So, if you rotate it, gray-scale and skew it, zoom in, distorted, or change something like the hue and saturation, most people would still be able to recognize the primary subject matter as a cat, but why not computers? Why? This is the key reason, is because that we know that the feature information that's defined as the relationship between the pixels. Every vector of pixel information in a vacuum may not be the best way for the model to learn what ultimately the image subject matter is. So, onto our fundamental question, how do you create a model that's able to deal with his new visual data as an input and do so with accuracy? How do we get it to be tolerant of variations in an image? Since in the real world, the model's unlikely to see through the exact same or identical images, so set a different way. How do we create a model that through his understanding of the visual domain, is able to map the high dimensional image space to a lower dimensional subspace like a really long feature vector that behaves more like structured data? Well, to start, we've already seen two different types of models so far, linear models and DNN or deep neural networks. It's often best to start with the simplest model first and see if that meets our needs. So, in the next module, you'll build a linear model and a deep neural network to classify images using TensorFlow. Now, before we close our overview, I'd like you to brainstorm what types of image data sets do you think would be very easy for machine learning model to classify? Now, what about the hardest? Can you think of how a model could potentially learn an unintended bias based on the available data that we have? I look forward to seeing your responses in the forum.