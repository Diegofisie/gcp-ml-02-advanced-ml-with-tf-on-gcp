So, in this demo, what I am going to be doing is I'm going to show you the changes that we have to make to our flowers model to get it to work on the TPU. Recall that we built a flowers training model from scratch using convolutional networks in an earlier lesson, and we showed you how to do this and then you trained it using basic underscore GPU on ML engine. Let's say that you have some such custom model, so you don't want to run resonate 50, you don't want to run resonant 18, you want to run your own model. Let's see what are the steps to take your own model, and get it running on the TPU. So, let's say you want to take your code, your own model and get it running on the CPU, on the TPU rather. You want to get it running on the TPU. So, this code that I'm going to be demonstrating, its in the GitHub repository for training data analyst. There is the flowers_fromscratch_tpu.ipynb. So, this is different from the flowers from scratch iPython notebook. Essentially, if you look at the original one, the training is on a GPU. Where's our training? The scale tier in the original one is on the basic GPU. In the case of from scratch.TPU, the training is on basic TPU instead of GPU. But the code itself, I mean, we would like to quote to be identical and I think we will get there. But right now, TPUs are new and so there is actually a separate estimator that TPU estimator on which you have to write your training code. Over time, these will converge as we do. When we do the TPU estimator, we write the code in such a way that it runs on everything on CPUs, GPUs, and TPUs. Over time, we imagined that that code will become the real estimator. So that then, whatever estimator code you write, will actually run on all three platforms. The way currently, any estimator code that you write, runs on both CPUs and GPUs code unchanged. This will be the case for the TPU as well. It's not the case right now. So, right now, what are the changes that you have to make your code, so that rather than using the estimator, which only works on CPUs and GPUs, how do you use a TPU estimator, which works on all three platforms? So, in order to do that, so let me go ahead and start with Datalab and the first part of the lab is still the same, so I won't do it, which is to basically install apache-beam. Having installed apache-beam, you basically go ahead and run the dataflow pipeline to take your CSV files, convert them into TF records, and end up with TensorFlow records on which we are going to be doing our training. So, this is the same as the code that we had for the resonant 18 code, but it is different from what we had to do when we were doing the flowers data from scratch. When we did it from scratch, our model read the JPEG data directly. That was okay if you're training on a CPU or on a GPU, because those are relatively slower. On a TPU, reading directly JPEG files, individual files for every pattern in a batch is going to be way too slow. While then, essentially, a TPU is going to sit there and wait to read 128 images, 128 individual JPEG files. We don't want to do that, and that's why we are taking the JPEG images and converting them into TensorFlow records, where we can read all 128 all at once out of cloud storage. So, that is why this lab, unlike the from scratch lab that we did with flowers, this one starts with the dataflow pipeline, similar to what we did when we were training the resonant model on the TPU. We started the TF records. On TPUs, you want to start with TF records. So, we did that and now we have a file. We can now go ahead and try to run the code. So, I'll show you the code later but let's just see how to run the code. To run the code, I'm going to basically look at the package, flowersmodeltpu. So again, if you were to look at the course repository, there's our flowersmodel, this is the one that we used to use, and there is a flowersmodeltpu, which is the modification of the flowersmodel to run on the TPU. So, what I'm doing here is that I'm running the flowersmodel, the task.pie in it, and I'm passing in the number of images, and I'm saying run this right now without the TPUs. So, run it on a CPU, with a learning rate of 0.01 and with this project, and with this as the training data path, and this as the evaluation data path. So, let's go ahead and run this. At this point, because I'm now running without a TPU, I'm setting my training batch size to be relatively small, and the number of training steps to be relatively small. I want this thing to finish. Again, this will run or not run depending on how much memory you have on that VM that you're running Datalab in. So, don't be concerned if a process gets killed for using too much memory. But this should run, and you're now basically running it local to the VM on which you're running the Datalab thing. Then, so this goes and so this again. Mine got killed, that's okay. I know that the code all compiles and runs, and then I can basically go ahead and run this with TPU. Let's run it on ML engine with Bayes scale tier is basic CPU. As before, I will basically go ahead and change my output there with like an underscore delete in it, so that it doesn't affect the one that I already have deployed. So, we can basically go ahead and submit the training job passing in a bunch of parameters, and then we can run it. After that, once we run it, we can then go ahead, let me start that off. While it's running, we can go ahead and look at the model location, and we can basically do a saved model cli. So, this is now being distributed, it's being run on ML engine, and ML engine is going to take care of coordinating with the TPU, and running that job on it. Then, we can basically go ahead and now I'll go back to my original thing that I already have run, and I can look at the model location, saved_model_cli shows you all the signatures and everything that had been exported, and we can make sure that this thing has been exported correctly. We can ask before, so now, you basically see that there is a serving default, where it asks for an image byte which is the encoded jpeg image, and it will basically give us back the classid, and the probabilities, and the best class. Now, we can deploy and we can do all of the things that we normally do. So, I will step you through it. Those all permits are the same. Instead, I'll switch over to the code base, and I'll show you the changes that we did. So, in the model.py. So, let's go down. The model.py is essentially the same. The big difference is instead of reading from jpeg, our input function is now going to read from TensorFlow records. So, let's go ahead and let's look at the input function, make input function, given a pattern and notice that now we ask for the number of course of the TPU that you are going to be using. That's going to be important because it'll show why that isn't a little bit. So, in my input function, I basically go ahead and use tf.data.Dataset, find all the files that match a particular pattern, and if it's in training, I repeat this indefinitely. Each time I fetch about eight megabytes, and I basically, these are all TF records. So, my dataset is a TF record, and then I do something pretty interesting. I basically say take my fetch dataset, and do this in parallel. On all my course. Then, I go ahead and shuffle this into a buffer of about 1024 records which is larger than my batch size, and then, I say, go ahead and do the read and preprocess, having read the TensorFlow records, go ahead and decode the JPEG data, read batch size files at a time, and read them in parallel. Right? Because we're going to have a separate input function for every one of our hosts. So, read them in parallel, read my dataset. Having done that then we do something relatively interesting and we also set the shape to be static. Unlike GPUs, unlike CPUs, TPUs work best if we know the size of the vectors beforehand. So, what we're doing is we're setting the shape explicitly. We're saying that the first dimension is not going to be question mark, it's going to be batch size, and will then always work. Well, it will work if the number of records that you have is divisible by eight or whatever the batch are divisible by the batch whatever the batch size is. What if it's not? So, that's why we say, go ahead and drop the remainder. Now, to be sure, you're not actually dropping the images, you're dropping the remainder. What does that mean? In training, you're reading this indefinitely. You're reading the training data indefinitely. So, you're really not pre-adopting anything because if you have some leftover, let say you have eight images or 12 images left over, they're going to get added to the next time you read it. So, i'ts just going to roll over, and you're going to get your batch size of 128 anyway. So, you're going to always get your batch size in training, you're not really dropping any images. In evaluation though, we will drop the remainder. Right? So, if you have an evaluation dataset of say 3,000 images, and when you do a division, it's not 3,000. So, you basically have, let say I can do the division in my head. Let's say you have 12 images leftover, you're never going to evaluate on those 12 images. You're going to evaluate on the number of evaluation images, modulo the bat size. Right? So, that is a trade-off here. When you're doing evaluation, you're going not to evaluate in the last few images, if it's not exactly divisible. This is not a big deal because as long as you say that we're going to evaluate on the same set of images you're fine, and we will always be evaluating the same set of images. It's the last few images that don't fit in, that are going to get dropped. So, that's what we're doing. We're dropping the remainder, resetting this size to be static. The other thing is that one of the reasons that a TensorFlow processing unit, the TPU, the one of the reasons that gets its efficiency is by the fact that it's the matrix multiplies saturate the matrix unit. One of the ways that you can do that is that if you can transpose your matrix such that the last dimension is the batch. Right? Which is what we want, and so that's what we're doing here. If you're going to be running on a TPU, transpose the unit. So, this transpose unit flag, will be true on a TPU, will be false on a CPU or a GPU. No harm if you don't do it, but this is a way to get a little bit extra performance. Then, we also do one more thing. We prefetch the data. While the TPU is out there doing gradient descent on one batch, we fetch in the next batch into the pipeline. So, something that you are noticing is that this pipeline is actually written to be extremely efficient, and the extreme efficiency really matters, because a TPU compute cycle is fast. This sort of thing, we haven't talked about so far. All these optimizations are going to help even on a GPU. But they're not necessary. So, we didn't actually talk about them, we kept our input functions relatively simple because on the GPU it doesn't matter as much. On a TPU it does. So, that's why we're talking about it here. But these are good practices that you might if you want, do on all your codebase. That's what I meant by over time these best practices, et cetera, will just get folded in into the the typical way that we do things. So, one thing that we often hear is until the API catches up, you have best practices. So, that's what we have here. The next thing that we do is that we basically write a model function. My model functions is called an image classifier. It as usual, takes features labels more than params. This is pretty much the same model function as we had before, and we basically instead of returning an estimator spec, we now return a TPU estimator spec. The parameters are all pretty much the same. Right? So, this just works as this. The difference here is in this ops. The operations here, you're basically passing in a function rather than a dictionary. But that's esoteric, will it know, but it's a small change in the API. The next bit is, normally we said go ahead and use train and evaluate. So far in our specialization, all of our courses we've said, use estimators training and evaluate because it handles a training loop for you. Well, training and evaluate handles a training loop if you're doing distributed training on CPUs and GPUs. Right now, today, it doesn't do TPUs. So, for train and evaluate, we write our own loop. We do the distribution somewhat differently on a TPU. So, I'm writing the code myself here. So, I'm writing a train and evaluate loop, and I'm basically saying that if we are using a TPU, go ahead and find the TPU. So, add some boilerplate code to go find the TPU from the cluster, set up a TPU run config, where you specify every how many steps do you want to do the evaluation, and that you want to basically have a separate input function for every one of your hosts. So, this is essentially a boilerplate code. So, now you have to find the TPU. If you're not using a TPU, you're basically just using a regular run config. Then, we're creating a TPU estimator passing in the model function, passing in the config, passing in the training batch size, passing in the evaluation batch size, and passing in the flag on whether or not to use the TPU. Having done that, we've now set up the data. Let's set up the input functions. So, we create a make input function from the training input path with more to strain, that's my training input function. Similarly, mode is my evaluation input function. Then, we start our training. So, these are the kinds of things that train and evaluate would do for you. We're not doing it. First thing is you have to go ahead and look in the output directory to see if there is some training checkpoints already present because you have to start from that checkpoint and, that's what we're doing here saying Load the step from the checkpoint directory, and then figure out how many more steps you need to carry out by doing the subtraction. Right? Then, you basically start your training. So, as long as the current step is less than the total number of steps that you need to do, go ahead and figure out what the next checkpoint is, call estimator strain method until that checkpoint, and then having done that, go ahead and do the evaluation. Of course you can decide not to evaluate at every checkpoint, but to evaluate every other checkpoint for example, and that's fine. So, remember that checkpointing is about fault recovery. Because every time you start back, you will start back at the checkpoint that exists on disk, so that if you're training for whatever reason fails, you don't have to start from scratch, you can start from the middle, or another reason to do checkpointing, is to save the checkpointed model, so you can do continuous training. So you can restart your training from the model as of yesterday, for example. So, you do the checkpoints, but you don't have to always evaluate every time you checkpoint, but in this case we are everytime we checkpoint, we're doing our evaluation. So, we do a training for a little bit of time, we do a checkpoint, we do our evaluation, and having evaluated, we go to do the whole thing, and then when we're exiting, we go ahead and call Export Save model to basically export out. So, again the kinds of things that train and evaluate would do for you, here we're doing it from scratch. But at this point, this code will work on all three platforms. Whether you're using an accelerator like a GPU or TPU or you're using plain old CPUs, this code will just work. That's pretty much it. At this point, you have code, custom code. The custom code being whatever code you wrote in this model function, in the image classifier, and this will work on both GPUs and on TPUs. Thanks.