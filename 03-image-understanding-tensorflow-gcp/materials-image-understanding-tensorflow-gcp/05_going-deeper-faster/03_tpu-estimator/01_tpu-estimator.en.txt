In the previous lesson and the lab, we looked at how to use TPUs to run the ResNet model very fast. This is easy because, the ResNet source code for TensorFlow, it was open-source, it was available. All that we had to do was to take it and apply it to our own dataset. For the most part if you're doing standard image classification, you will use a state of the art open-source model. There are several of them in the same repository as a ResNet model we tried so you could do any of them on the TPU. Later in this course we'll talk about AmoebaNet that's in that same repository. But what if you want to run TPU on your own custom code? So you want to take your custom code, and you want them to execute it on the TPU. Well, first of all, I hope you're using the estimator API and the dataset API to read into your data. If you're not using the estimator and dataset, first rewrite your model using higher level TensorFlow abstractions instead of using the very low level. So let's say you're using a custom estimator, the TensorFlow TPU module gives you the equivalent of the regular classes. So to write code to run on a TPU we need to do four things; number one; replace the optimizer, number two; replace the EstimatorSpec, number three; replace our RunConfig, and number four; replace the Estimator. So let's look at each of these four steps. The first step is to wrap whatever optimizer you're using in a cross shard optimizer. What this does, what the cross shard optimizer does is that it takes advantage of the special architecture of the TPU and applies a novel Adam optimizer or Adagrad optimizer or whatever optimizer you're using on the TPU. Second step, instead of returning an estimator spec from your model function return a TPU estimator spec. The parameters are pretty much the same, so what you're doing is you're just changing the class name. Thirdly, add these lines to create a cluster resolver so that your training code can find the TPU, and also you will have to specify how often to checkpoint. Now once your code is running on a TPU, you'll want to minimize the input output overhead, so that the TPU is not waiting for the disk. So what you will typically do is that you will use much higher batch sizes than you do on a GPU. Another thing that you typically do is to check point less often, so here, am check pointing only once every thousand steps. Another thing that I'm doing is to do data parallelism, by specifying the per host input each TPU core gets its own input function. Finally, change your estimator from a plane estimator to a TPU estimator. Here too the parameters are pretty much the same, so what you're doing is you're simply changing the class name here. One best practice is to use a command line flag for the use_tpu property of the TPU estimator. Initially I would recommend setting the use_tpu to be false, make sure that your entire code works and finally when it works, flip the boolean flag to true. The reason is, that when you have use_tpu is true the TPU classes they change your graph and rewrite the graph significantly for the TPU architecture. So if you have error messages, those error messages on the TPU code will refer to functions and variables that are auto-generated, that you had no hand in writing. It's going to be really hard for you to understand what's going on if you're trying to debug your code after it's been converted and rewritten for TPUs. So, create your code, develop your code with use_tpu as false and then once the model code works flip the switch, that way your error messages if there's something wrong with the model comes from the regular TensorFlow graph and will reference your own functions, your own variables, and that's easier to fix. So with these four changes; cross shard optimizer, TPU estimator spec, run config, and TPU estimator your code should work on a CPU, GPU, or TPU. So, rather quickly, some points to keep in mind on TPUs because these help explain the code changes a little bit. Number one, there are four chips per TPU. There are two cores per chip, and each of those cores has eight gigs of memory. There are 512 cores or 64 TPUs within a pod. You might choose the number of cores that you want your training job to take within that number, of course you can even use more chips than the 64 chips on a TPU, but then you're using multiple pods and the distribution strategy has to change to accommodate that. TPUs provide high speed interconnect, so we tend not to worry about communication overhead within a TPU pod. So you have lots of memory on disk, remember by eight gigs of memory per core, so it means that you can have a lot more data and less frequent calls to the CPU. Scaling is pretty much linear. As you to increase a number of cores assigned to your job your training speed will increase linearly, and the interconnect within a TPU is much faster than the connection to the CPU. So we try to tend to do as much as we can inside the TPU without going back to the CPU. TPUs also offer very large matrix multiplication hardware, infact this is the primary speedup given by the TPU it's in the matrix multiplication. So points two and three mean that you should use much higher batch sizes in order to take full advantage of the TPU. Your peak performance is going to be possible when the batch size is divisible by 128 because this will allow you to fully saturate that matrix multiply unit. If you can't get a batch size divided by 128 at least go to the batch size that's a multiple of eight. Because TPUs are most efficient when operating on batches of 128 or multiples of 128, TPU estimators are currently not designed for single predictions, so they're not for online prediction, they're intended mainly for batch predictions. If your use case requires latency sensitive predictions, it is recommended to create a TPU estimator after training your model, and set the use TPU flag to be false so that you can do the serving on a GPU or a CPU. So these four characteristics that we just looked at are essentially what dictate how we interact with the TPUs through code. The fact that TPUs have a specialized instruction set, means that they have to share responsibility along with other hardware. The high-speed interconnect, large amount of memory, and the large matrix multiplication unit means that accommodating this constraint isn't so bad, because certain workloads can just stay on the TPU for very long periods and be very efficient. The fifth point to remember about a TPU is that TPUs use bfloat. Bfloat is a new 16-bit representation of floating point numbers, that trades precision for density. So we can pack more bfloats onto a chip. Use of a bfloat is optional, if you want you can use float32 as your floating point representation almost everywhere in the graph. Another thing that you could do is to do everything in bfloat but then finally when you're exporting convert the weights from bfloat to float32. Having said that, you will get the maximum benefit of a TPU, if you use bfloat throughout. Even if you use float32 the matrix multiply will be carried out in bfloat. So, on a TPU use cases that absolutely require double-precision arithmetic, they're not a good fit.