1
00:00:00,000 --> 00:00:02,190
So, in this demo, what I am going to be doing

2
00:00:02,190 --> 00:00:04,290
is I'm going to show you the changes that we

3
00:00:04,290 --> 00:00:08,770
have to make to our flowers model to get it to work on the TPU.

4
00:00:08,770 --> 00:00:10,555
Recall that we built

5
00:00:10,555 --> 00:00:17,820
a flowers training model from scratch using convolutional networks in an earlier lesson,

6
00:00:17,820 --> 00:00:20,640
and we showed you how to do this and then you trained it using

7
00:00:20,640 --> 00:00:24,025
basic underscore GPU on ML engine.

8
00:00:24,025 --> 00:00:27,640
Let's say that you have some such custom model,

9
00:00:27,640 --> 00:00:29,730
so you don't want to run resonate 50,

10
00:00:29,730 --> 00:00:31,350
you don't want to run resonant 18,

11
00:00:31,350 --> 00:00:33,180
you want to run your own model.

12
00:00:33,180 --> 00:00:37,710
Let's see what are the steps to take your own model,

13
00:00:37,710 --> 00:00:41,200
and get it running on the TPU.

14
00:00:41,200 --> 00:00:44,850
So, let's say you want to take your code,

15
00:00:44,850 --> 00:00:48,025
your own model and get it running on the CPU,

16
00:00:48,025 --> 00:00:49,460
on the TPU rather.

17
00:00:49,460 --> 00:00:51,005
You want to get it running on the TPU.

18
00:00:51,005 --> 00:00:53,750
So, this code that I'm going to be demonstrating,

19
00:00:53,750 --> 00:00:56,750
its in the GitHub repository for training data analyst.

20
00:00:56,750 --> 00:01:00,030
There is the flowers_fromscratch_tpu.ipynb.

21
00:01:00,370 --> 00:01:04,855
So, this is different from the flowers from scratch iPython notebook.

22
00:01:04,855 --> 00:01:08,420
Essentially, if you look at the original one,

23
00:01:08,420 --> 00:01:13,280
the training is on a GPU.

24
00:01:13,280 --> 00:01:16,170
Where's our training?

25
00:01:18,460 --> 00:01:24,065
The scale tier in the original one is on the basic GPU.

26
00:01:24,065 --> 00:01:27,655
In the case of from scratch.TPU,

27
00:01:27,655 --> 00:01:33,530
the training is on basic TPU instead of GPU.

28
00:01:33,530 --> 00:01:36,070
But the code itself, I mean,

29
00:01:36,070 --> 00:01:39,610
we would like to quote to be identical and I think we will get there.

30
00:01:39,610 --> 00:01:42,830
But right now, TPUs are new and so there is actually

31
00:01:42,830 --> 00:01:47,905
a separate estimator that TPU estimator on which you have to write your training code.

32
00:01:47,905 --> 00:01:50,160
Over time, these will converge as we do.

33
00:01:50,160 --> 00:01:51,685
When we do the TPU estimator,

34
00:01:51,685 --> 00:01:57,290
we write the code in such a way that it runs on everything on CPUs, GPUs, and TPUs.

35
00:01:57,290 --> 00:02:01,380
Over time, we imagined that that code will become the real estimator.

36
00:02:01,380 --> 00:02:03,980
So that then, whatever estimator code you write,

37
00:02:03,980 --> 00:02:06,050
will actually run on all three platforms.

38
00:02:06,050 --> 00:02:09,410
The way currently, any estimator code that you write,

39
00:02:09,410 --> 00:02:12,790
runs on both CPUs and GPUs code unchanged.

40
00:02:12,790 --> 00:02:14,850
This will be the case for the TPU as well.

41
00:02:14,850 --> 00:02:16,700
It's not the case right now.

42
00:02:16,700 --> 00:02:20,690
So, right now, what are the changes that you have to make your code,

43
00:02:20,690 --> 00:02:22,820
so that rather than using the estimator,

44
00:02:22,820 --> 00:02:25,545
which only works on CPUs and GPUs,

45
00:02:25,545 --> 00:02:27,825
how do you use a TPU estimator,

46
00:02:27,825 --> 00:02:30,415
which works on all three platforms?

47
00:02:30,415 --> 00:02:32,140
So, in order to do that,

48
00:02:32,140 --> 00:02:33,940
so let me go ahead and start with

49
00:02:33,940 --> 00:02:37,310
Datalab and the first part of the lab is still the same,

50
00:02:37,310 --> 00:02:38,540
so I won't do it,

51
00:02:38,540 --> 00:02:41,210
which is to basically install apache-beam.

52
00:02:41,210 --> 00:02:44,420
Having installed apache-beam, you basically go ahead

53
00:02:44,420 --> 00:02:47,885
and run the dataflow pipeline to take your CSV files,

54
00:02:47,885 --> 00:02:50,085
convert them into TF records,

55
00:02:50,085 --> 00:02:55,505
and end up with TensorFlow records on which we are going to be doing our training.

56
00:02:55,505 --> 00:03:00,710
So, this is the same as the code that we had for the resonant 18 code,

57
00:03:00,710 --> 00:03:04,220
but it is different from what we had to do when we

58
00:03:04,220 --> 00:03:08,570
were doing the flowers data from scratch.

59
00:03:08,570 --> 00:03:10,035
When we did it from scratch,

60
00:03:10,035 --> 00:03:13,255
our model read the JPEG data directly.

61
00:03:13,255 --> 00:03:17,220
That was okay if you're training on a CPU or on a GPU,

62
00:03:17,220 --> 00:03:19,410
because those are relatively slower.

63
00:03:19,410 --> 00:03:23,445
On a TPU, reading directly JPEG files,

64
00:03:23,445 --> 00:03:29,425
individual files for every pattern in a batch is going to be way too slow.

65
00:03:29,425 --> 00:03:36,845
While then, essentially, a TPU is going to sit there and wait to read 128 images,

66
00:03:36,845 --> 00:03:39,905
128 individual JPEG files.

67
00:03:39,905 --> 00:03:41,210
We don't want to do that,

68
00:03:41,210 --> 00:03:46,360
and that's why we are taking the JPEG images and converting them into TensorFlow records,

69
00:03:46,360 --> 00:03:51,320
where we can read all 128 all at once out of cloud storage.

70
00:03:51,320 --> 00:03:54,200
So, that is why this lab,

71
00:03:54,200 --> 00:03:58,450
unlike the from scratch lab that we did with flowers,

72
00:03:58,450 --> 00:04:01,040
this one starts with the dataflow pipeline,

73
00:04:01,040 --> 00:04:05,450
similar to what we did when we were training the resonant model on the TPU.

74
00:04:05,450 --> 00:04:07,295
We started the TF records.

75
00:04:07,295 --> 00:04:10,830
On TPUs, you want to start with TF records.

76
00:04:10,830 --> 00:04:13,330
So, we did that and now we have a file.

77
00:04:13,330 --> 00:04:15,980
We can now go ahead and try to run the code.

78
00:04:15,980 --> 00:04:19,380
So, I'll show you the code later but let's just see how to run the code.

79
00:04:19,380 --> 00:04:23,995
To run the code, I'm going to basically look at the package, flowersmodeltpu.

80
00:04:23,995 --> 00:04:31,505
So again, if you were to look at the course repository, there's our flowersmodel,

81
00:04:31,505 --> 00:04:33,350
this is the one that we used to use,

82
00:04:33,350 --> 00:04:35,240
and there is a flowersmodeltpu,

83
00:04:35,240 --> 00:04:39,925
which is the modification of the flowersmodel to run on the TPU.

84
00:04:39,925 --> 00:04:44,165
So, what I'm doing here is that I'm running the flowersmodel,

85
00:04:44,165 --> 00:04:45,880
the task.pie in it,

86
00:04:45,880 --> 00:04:48,650
and I'm passing in the number of images,

87
00:04:48,650 --> 00:04:50,900
and I'm saying run this right now without the TPUs.

88
00:04:50,900 --> 00:04:52,395
So, run it on a CPU,

89
00:04:52,395 --> 00:04:56,000
with a learning rate of 0.01 and with this project,

90
00:04:56,000 --> 00:04:58,700
and with this as the training data path,

91
00:04:58,700 --> 00:05:01,330
and this as the evaluation data path.

92
00:05:01,330 --> 00:05:03,985
So, let's go ahead and run this.

93
00:05:03,985 --> 00:05:07,460
At this point, because I'm now running without a TPU,

94
00:05:07,460 --> 00:05:10,880
I'm setting my training batch size to be relatively small,

95
00:05:10,880 --> 00:05:13,430
and the number of training steps to be relatively small.

96
00:05:13,430 --> 00:05:14,720
I want this thing to finish.

97
00:05:14,720 --> 00:05:18,020
Again, this will run or not run depending on how much memory

98
00:05:18,020 --> 00:05:21,625
you have on that VM that you're running Datalab in.

99
00:05:21,625 --> 00:05:25,420
So, don't be concerned if a process gets killed for using too much memory.

100
00:05:25,420 --> 00:05:26,740
But this should run,

101
00:05:26,740 --> 00:05:29,450
and you're now basically running it

102
00:05:29,450 --> 00:05:33,305
local to the VM on which you're running the Datalab thing.

103
00:05:33,305 --> 00:05:39,625
Then, so this goes and so this again.

104
00:05:39,625 --> 00:05:41,320
Mine got killed, that's okay.

105
00:05:41,320 --> 00:05:44,230
I know that the code all compiles and runs,

106
00:05:44,230 --> 00:05:48,980
and then I can basically go ahead and run this with TPU.

107
00:05:48,980 --> 00:05:55,240
Let's run it on ML engine with Bayes scale tier is basic CPU.

108
00:05:55,240 --> 00:05:58,370
As before, I will basically go ahead and change my output

109
00:05:58,370 --> 00:06:01,700
there with like an underscore delete in it,

110
00:06:01,700 --> 00:06:04,840
so that it doesn't affect the one that I already have deployed.

111
00:06:04,840 --> 00:06:07,160
So, we can basically go ahead and submit

112
00:06:07,160 --> 00:06:10,105
the training job passing in a bunch of parameters,

113
00:06:10,105 --> 00:06:12,020
and then we can run it.

114
00:06:12,020 --> 00:06:14,875
After that, once we run it,

115
00:06:14,875 --> 00:06:17,450
we can then go ahead,

116
00:06:17,450 --> 00:06:19,345
let me start that off.

117
00:06:19,345 --> 00:06:24,375
While it's running, we can go ahead and look at the model location,

118
00:06:24,375 --> 00:06:27,565
and we can basically do a saved model cli.

119
00:06:27,565 --> 00:06:30,030
So, this is now being distributed,

120
00:06:30,030 --> 00:06:32,035
it's being run on ML engine,

121
00:06:32,035 --> 00:06:35,400
and ML engine is going to take care of coordinating with the TPU,

122
00:06:35,400 --> 00:06:36,850
and running that job on it.

123
00:06:36,850 --> 00:06:39,830
Then, we can basically go ahead and now I'll go back to

124
00:06:39,830 --> 00:06:44,010
my original thing that I already have run,

125
00:06:44,010 --> 00:06:46,190
and I can look at the model location,

126
00:06:46,190 --> 00:06:51,260
saved_model_cli shows you all the signatures and everything that had been exported,

127
00:06:51,260 --> 00:06:54,760
and we can make sure that this thing has been exported correctly.

128
00:06:54,760 --> 00:06:56,960
We can ask before, so now,

129
00:06:56,960 --> 00:07:00,435
you basically see that there is a serving default,

130
00:07:00,435 --> 00:07:05,030
where it asks for an image byte which is the encoded jpeg image,

131
00:07:05,030 --> 00:07:07,590
and it will basically give us back the classid,

132
00:07:07,590 --> 00:07:11,100
and the probabilities, and the best class.

133
00:07:12,230 --> 00:07:16,690
Now, we can deploy and we can do all of the things that we normally do.

134
00:07:16,690 --> 00:07:17,760
So, I will step you through it.

135
00:07:17,760 --> 00:07:19,030
Those all permits are the same.

136
00:07:19,030 --> 00:07:22,570
Instead, I'll switch over to the code base,

137
00:07:22,570 --> 00:07:25,410
and I'll show you the changes that we did.

138
00:07:25,410 --> 00:07:29,070
So, in the model.py.

139
00:07:29,070 --> 00:07:31,960
So, let's go down. The model.py is essentially the same.

140
00:07:31,960 --> 00:07:35,610
The big difference is instead of reading from jpeg,

141
00:07:35,610 --> 00:07:39,230
our input function is now going to read from TensorFlow records.

142
00:07:39,230 --> 00:07:46,280
So, let's go ahead and let's look at the input function, make input function,

143
00:07:46,280 --> 00:07:50,330
given a pattern and notice that now we ask

144
00:07:50,330 --> 00:07:54,600
for the number of course of the TPU that you are going to be using.

145
00:07:54,600 --> 00:07:58,190
That's going to be important because it'll show why that isn't a little bit.

146
00:07:58,190 --> 00:08:00,555
So, in my input function,

147
00:08:00,555 --> 00:08:04,625
I basically go ahead and use tf.data.Dataset,

148
00:08:04,625 --> 00:08:08,630
find all the files that match a particular pattern,

149
00:08:08,630 --> 00:08:10,060
and if it's in training,

150
00:08:10,060 --> 00:08:12,615
I repeat this indefinitely.

151
00:08:12,615 --> 00:08:17,635
Each time I fetch about eight megabytes,

152
00:08:17,635 --> 00:08:21,060
and I basically, these are all TF records.

153
00:08:21,060 --> 00:08:23,095
So, my dataset is a TF record,

154
00:08:23,095 --> 00:08:26,435
and then I do something pretty interesting.

155
00:08:26,435 --> 00:08:29,655
I basically say take my fetch dataset,

156
00:08:29,655 --> 00:08:32,375
and do this in parallel.

157
00:08:32,375 --> 00:08:34,875
On all my course.

158
00:08:34,875 --> 00:08:38,910
Then, I go ahead and shuffle this into a buffer of

159
00:08:38,910 --> 00:08:42,900
about 1024 records which is larger than my batch size,

160
00:08:42,900 --> 00:08:44,735
and then, I say,

161
00:08:44,735 --> 00:08:47,500
go ahead and do the read and preprocess,

162
00:08:47,500 --> 00:08:50,105
having read the TensorFlow records,

163
00:08:50,105 --> 00:08:53,375
go ahead and decode the JPEG data,

164
00:08:53,375 --> 00:08:56,060
read batch size files at a time,

165
00:08:56,060 --> 00:08:59,125
and read them in parallel.

166
00:08:59,125 --> 00:09:03,990
Right? Because we're going to have a separate input function for every one of our hosts.

167
00:09:03,990 --> 00:09:06,675
So, read them in parallel, read my dataset.

168
00:09:06,675 --> 00:09:11,050
Having done that then we do something relatively interesting

169
00:09:11,050 --> 00:09:15,590
and we also set the shape to be static.

170
00:09:15,590 --> 00:09:18,525
Unlike GPUs, unlike CPUs,

171
00:09:18,525 --> 00:09:24,850
TPUs work best if we know the size of the vectors beforehand.

172
00:09:24,850 --> 00:09:29,165
So, what we're doing is we're setting the shape explicitly.

173
00:09:29,165 --> 00:09:33,615
We're saying that the first dimension is not going to be question mark,

174
00:09:33,615 --> 00:09:36,280
it's going to be batch size,

175
00:09:36,280 --> 00:09:39,395
and will then always work.

176
00:09:39,395 --> 00:09:44,070
Well, it will work if the number of records that you have is divisible by

177
00:09:44,070 --> 00:09:49,735
eight or whatever the batch are divisible by the batch whatever the batch size is.

178
00:09:49,735 --> 00:09:53,495
What if it's not? So, that's why we say,

179
00:09:53,495 --> 00:09:55,760
go ahead and drop the remainder.

180
00:09:55,760 --> 00:10:00,315
Now, to be sure, you're not actually dropping the images,

181
00:10:00,315 --> 00:10:01,700
you're dropping the remainder.

182
00:10:01,700 --> 00:10:03,575
What does that mean? In training,

183
00:10:03,575 --> 00:10:05,340
you're reading this indefinitely.

184
00:10:05,340 --> 00:10:07,395
You're reading the training data indefinitely.

185
00:10:07,395 --> 00:10:12,190
So, you're really not pre-adopting anything because if you have some leftover,

186
00:10:12,190 --> 00:10:15,400
let say you have eight images or 12 images left over,

187
00:10:15,400 --> 00:10:18,595
they're going to get added to the next time you read it.

188
00:10:18,595 --> 00:10:20,040
So, i'ts just going to roll over,

189
00:10:20,040 --> 00:10:22,990
and you're going to get your batch size of 128 anyway.

190
00:10:22,990 --> 00:10:25,995
So, you're going to always get your batch size in training,

191
00:10:25,995 --> 00:10:28,150
you're not really dropping any images.

192
00:10:28,150 --> 00:10:31,900
In evaluation though, we will drop the remainder.

193
00:10:31,900 --> 00:10:37,150
Right? So, if you have an evaluation dataset of say 3,000 images,

194
00:10:37,150 --> 00:10:39,730
and when you do a division, it's not 3,000.

195
00:10:39,730 --> 00:10:41,060
So, you basically have,

196
00:10:41,060 --> 00:10:43,535
let say I can do the division in my head.

197
00:10:43,535 --> 00:10:45,710
Let's say you have 12 images leftover,

198
00:10:45,710 --> 00:10:48,320
you're never going to evaluate on those 12 images.

199
00:10:48,320 --> 00:10:52,070
You're going to evaluate on the number of evaluation images,

200
00:10:52,070 --> 00:10:54,130
modulo the bat size.

201
00:10:54,130 --> 00:10:56,545
Right? So, that is a trade-off here.

202
00:10:56,545 --> 00:10:58,090
When you're doing evaluation,

203
00:10:58,090 --> 00:11:01,145
you're going not to evaluate in the last few images,

204
00:11:01,145 --> 00:11:03,525
if it's not exactly divisible.

205
00:11:03,525 --> 00:11:06,390
This is not a big deal because as long as you

206
00:11:06,390 --> 00:11:09,615
say that we're going to evaluate on the same set of images you're fine,

207
00:11:09,615 --> 00:11:12,365
and we will always be evaluating the same set of images.

208
00:11:12,365 --> 00:11:16,750
It's the last few images that don't fit in, that are going to get dropped.

209
00:11:16,750 --> 00:11:18,500
So, that's what we're doing.

210
00:11:18,500 --> 00:11:20,075
We're dropping the remainder,

211
00:11:20,075 --> 00:11:23,580
resetting this size to be static.

212
00:11:23,580 --> 00:11:30,375
The other thing is that one of the reasons that a TensorFlow processing unit, the TPU,

213
00:11:30,375 --> 00:11:33,950
the one of the reasons that gets its efficiency is by the fact

214
00:11:33,950 --> 00:11:37,850
that it's the matrix multiplies saturate the matrix unit.

215
00:11:37,850 --> 00:11:41,375
One of the ways that you can do that is that if you can transpose

216
00:11:41,375 --> 00:11:45,035
your matrix such that the last dimension is the batch.

217
00:11:45,035 --> 00:11:47,990
Right? Which is what we want,

218
00:11:47,990 --> 00:11:49,805
and so that's what we're doing here.

219
00:11:49,805 --> 00:11:53,000
If you're going to be running on a TPU, transpose the unit.

220
00:11:53,000 --> 00:11:54,840
So, this transpose unit flag,

221
00:11:54,840 --> 00:11:56,710
will be true on a TPU,

222
00:11:56,710 --> 00:11:59,035
will be false on a CPU or a GPU.

223
00:11:59,035 --> 00:12:00,410
No harm if you don't do it,

224
00:12:00,410 --> 00:12:03,685
but this is a way to get a little bit extra performance.

225
00:12:03,685 --> 00:12:06,610
Then, we also do one more thing.

226
00:12:06,610 --> 00:12:08,760
We prefetch the data.

227
00:12:08,760 --> 00:12:13,405
While the TPU is out there doing gradient descent on one batch,

228
00:12:13,405 --> 00:12:17,600
we fetch in the next batch into the pipeline.

229
00:12:17,600 --> 00:12:20,510
So, something that you are noticing is that

230
00:12:20,510 --> 00:12:25,965
this pipeline is actually written to be extremely efficient,

231
00:12:25,965 --> 00:12:29,410
and the extreme efficiency really matters,

232
00:12:29,410 --> 00:12:32,145
because a TPU compute cycle is fast.

233
00:12:32,145 --> 00:12:33,440
This sort of thing,

234
00:12:33,440 --> 00:12:35,440
we haven't talked about so far.

235
00:12:35,440 --> 00:12:39,150
All these optimizations are going to help even on a GPU.

236
00:12:39,150 --> 00:12:40,710
But they're not necessary.

237
00:12:40,710 --> 00:12:42,760
So, we didn't actually talk about them,

238
00:12:42,760 --> 00:12:45,170
we kept our input functions relatively

239
00:12:45,170 --> 00:12:48,095
simple because on the GPU it doesn't matter as much.

240
00:12:48,095 --> 00:12:49,590
On a TPU it does.

241
00:12:49,590 --> 00:12:51,560
So, that's why we're talking about it here.

242
00:12:51,560 --> 00:12:55,165
But these are good practices that you might if you want,

243
00:12:55,165 --> 00:12:56,770
do on all your codebase.

244
00:12:56,770 --> 00:13:00,780
That's what I meant by over time these best practices, et cetera,

245
00:13:00,780 --> 00:13:05,240
will just get folded in into the the typical way that we do things.

246
00:13:05,240 --> 00:13:08,375
So, one thing that we often hear is until the API catches up,

247
00:13:08,375 --> 00:13:09,600
you have best practices.

248
00:13:09,600 --> 00:13:11,455
So, that's what we have here.

249
00:13:11,455 --> 00:13:15,065
The next thing that we do is that we basically write a model function.

250
00:13:15,065 --> 00:13:17,385
My model functions is called an image classifier.

251
00:13:17,385 --> 00:13:20,990
It as usual, takes features labels more than params.

252
00:13:20,990 --> 00:13:24,750
This is pretty much the same model function as we had before,

253
00:13:24,750 --> 00:13:28,705
and we basically instead of returning an estimator spec,

254
00:13:28,705 --> 00:13:32,400
we now return a TPU estimator spec.

255
00:13:32,400 --> 00:13:35,100
The parameters are all pretty much the same.

256
00:13:35,100 --> 00:13:39,160
Right? So, this just works as this.

257
00:13:39,160 --> 00:13:43,955
The difference here is in this ops.

258
00:13:43,955 --> 00:13:50,275
The operations here, you're basically passing in a function rather than a dictionary.

259
00:13:50,275 --> 00:13:52,475
But that's esoteric, will it know,

260
00:13:52,475 --> 00:13:56,100
but it's a small change in the API.

261
00:13:56,100 --> 00:13:58,210
The next bit is,

262
00:13:58,210 --> 00:14:01,575
normally we said go ahead and use train and evaluate.

263
00:14:01,575 --> 00:14:03,700
So far in our specialization,

264
00:14:03,700 --> 00:14:05,340
all of our courses we've said,

265
00:14:05,340 --> 00:14:09,640
use estimators training and evaluate because it handles a training loop for you.

266
00:14:09,640 --> 00:14:12,245
Well, training and evaluate handles

267
00:14:12,245 --> 00:14:17,105
a training loop if you're doing distributed training on CPUs and GPUs.

268
00:14:17,105 --> 00:14:19,880
Right now, today, it doesn't do TPUs.

269
00:14:19,880 --> 00:14:21,840
So, for train and evaluate,

270
00:14:21,840 --> 00:14:23,410
we write our own loop.

271
00:14:23,410 --> 00:14:26,860
We do the distribution somewhat differently on a TPU.

272
00:14:26,860 --> 00:14:28,870
So, I'm writing the code myself here.

273
00:14:28,870 --> 00:14:31,105
So, I'm writing a train and evaluate loop,

274
00:14:31,105 --> 00:14:35,575
and I'm basically saying that if we are using a TPU,

275
00:14:35,575 --> 00:14:37,660
go ahead and find the TPU.

276
00:14:37,660 --> 00:14:42,564
So, add some boilerplate code to go find the TPU from the cluster,

277
00:14:42,564 --> 00:14:45,430
set up a TPU run config,

278
00:14:45,430 --> 00:14:51,219
where you specify every how many steps do you want to do the evaluation,

279
00:14:51,219 --> 00:14:53,440
and that you want to basically have

280
00:14:53,440 --> 00:14:56,535
a separate input function for every one of your hosts.

281
00:14:56,535 --> 00:14:58,860
So, this is essentially a boilerplate code.

282
00:14:58,860 --> 00:15:01,635
So, now you have to find the TPU.

283
00:15:01,635 --> 00:15:03,855
If you're not using a TPU,

284
00:15:03,855 --> 00:15:07,065
you're basically just using a regular run config.

285
00:15:07,065 --> 00:15:12,580
Then, we're creating a TPU estimator passing in the model function,

286
00:15:12,580 --> 00:15:14,005
passing in the config,

287
00:15:14,005 --> 00:15:16,010
passing in the training batch size,

288
00:15:16,010 --> 00:15:18,130
passing in the evaluation batch size,

289
00:15:18,130 --> 00:15:22,740
and passing in the flag on whether or not to use the TPU.

290
00:15:22,740 --> 00:15:28,435
Having done that, we've now set up the data.

291
00:15:28,435 --> 00:15:30,430
Let's set up the input functions.

292
00:15:30,430 --> 00:15:35,690
So, we create a make input function from the training input path with more to strain,

293
00:15:35,690 --> 00:15:37,295
that's my training input function.

294
00:15:37,295 --> 00:15:41,055
Similarly, mode is my evaluation input function.

295
00:15:41,055 --> 00:15:42,760
Then, we start our training.

296
00:15:42,760 --> 00:15:44,495
So, these are the kinds of things that train and

297
00:15:44,495 --> 00:15:46,790
evaluate would do for you. We're not doing it.

298
00:15:46,790 --> 00:15:52,260
First thing is you have to go ahead and look in the output directory to see if there is

299
00:15:52,260 --> 00:15:54,970
some training checkpoints already present because you have to

300
00:15:54,970 --> 00:15:58,305
start from that checkpoint and,

301
00:15:58,305 --> 00:16:03,185
that's what we're doing here saying Load the step from the checkpoint directory,

302
00:16:03,185 --> 00:16:08,985
and then figure out how many more steps you need to carry out by doing the subtraction.

303
00:16:08,985 --> 00:16:13,115
Right? Then, you basically start your training.

304
00:16:13,115 --> 00:16:15,075
So, as long as the current step is

305
00:16:15,075 --> 00:16:17,675
less than the total number of steps that you need to do,

306
00:16:17,675 --> 00:16:22,300
go ahead and figure out what the next checkpoint is,

307
00:16:22,300 --> 00:16:26,870
call estimator strain method until that checkpoint,

308
00:16:26,870 --> 00:16:29,435
and then having done that,

309
00:16:29,435 --> 00:16:30,850
go ahead and do the evaluation.

310
00:16:30,850 --> 00:16:33,865
Of course you can decide not to evaluate at every checkpoint,

311
00:16:33,865 --> 00:16:37,180
but to evaluate every other checkpoint for example, and that's fine.

312
00:16:37,180 --> 00:16:40,680
So, remember that checkpointing is about fault recovery.

313
00:16:40,680 --> 00:16:43,030
Because every time you start back,

314
00:16:43,030 --> 00:16:46,320
you will start back at the checkpoint that exists on disk,

315
00:16:46,320 --> 00:16:49,425
so that if you're training for whatever reason fails,

316
00:16:49,425 --> 00:16:50,870
you don't have to start from scratch,

317
00:16:50,870 --> 00:16:52,920
you can start from the middle,

318
00:16:52,920 --> 00:16:55,235
or another reason to do checkpointing,

319
00:16:55,235 --> 00:16:57,315
is to save the checkpointed model,

320
00:16:57,315 --> 00:16:58,860
so you can do continuous training.

321
00:16:58,860 --> 00:17:05,635
So you can restart your training from the model as of yesterday, for example.

322
00:17:05,635 --> 00:17:07,795
So, you do the checkpoints,

323
00:17:07,795 --> 00:17:10,734
but you don't have to always evaluate every time you checkpoint,

324
00:17:10,734 --> 00:17:13,735
but in this case we are everytime we checkpoint,

325
00:17:13,735 --> 00:17:15,115
we're doing our evaluation.

326
00:17:15,115 --> 00:17:17,165
So, we do a training for a little bit of time,

327
00:17:17,165 --> 00:17:18,465
we do a checkpoint,

328
00:17:18,465 --> 00:17:21,515
we do our evaluation, and having evaluated,

329
00:17:21,515 --> 00:17:24,270
we go to do the whole thing,

330
00:17:24,270 --> 00:17:26,155
and then when we're exiting,

331
00:17:26,155 --> 00:17:30,395
we go ahead and call Export Save model to basically export out.

332
00:17:30,395 --> 00:17:33,410
So, again the kinds of things that train and evaluate would do for you,

333
00:17:33,410 --> 00:17:35,645
here we're doing it from scratch.

334
00:17:35,645 --> 00:17:37,405
But at this point,

335
00:17:37,405 --> 00:17:41,700
this code will work on all three platforms.

336
00:17:41,700 --> 00:17:46,780
Whether you're using an accelerator like a GPU or TPU or you're using plain old CPUs,

337
00:17:46,780 --> 00:17:48,505
this code will just work.

338
00:17:48,505 --> 00:17:50,650
That's pretty much it.

339
00:17:50,650 --> 00:17:54,475
At this point, you have code, custom code.

340
00:17:54,475 --> 00:17:58,725
The custom code being whatever code you wrote in this model function,

341
00:17:58,725 --> 00:18:00,365
in the image classifier,

342
00:18:00,365 --> 00:18:06,180
and this will work on both GPUs and on TPUs. Thanks.