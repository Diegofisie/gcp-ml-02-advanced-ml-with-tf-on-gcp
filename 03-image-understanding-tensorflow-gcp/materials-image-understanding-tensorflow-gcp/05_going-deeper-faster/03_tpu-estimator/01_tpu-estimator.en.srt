1
00:00:00,000 --> 00:00:02,955
In the previous lesson and the lab,

2
00:00:02,955 --> 00:00:07,650
we looked at how to use TPUs to run the ResNet model very fast.

3
00:00:07,650 --> 00:00:10,325
This is easy because,

4
00:00:10,325 --> 00:00:13,325
the ResNet source code for TensorFlow,

5
00:00:13,325 --> 00:00:15,295
it was open-source, it was available.

6
00:00:15,295 --> 00:00:19,685
All that we had to do was to take it and apply it to our own dataset.

7
00:00:19,685 --> 00:00:24,645
For the most part if you're doing standard image classification,

8
00:00:24,645 --> 00:00:28,190
you will use a state of the art open-source model.

9
00:00:28,190 --> 00:00:31,830
There are several of them in the same repository as a ResNet model

10
00:00:31,830 --> 00:00:35,250
we tried so you could do any of them on the TPU.

11
00:00:35,250 --> 00:00:39,555
Later in this course we'll talk about AmoebaNet that's in that same repository.

12
00:00:39,555 --> 00:00:45,130
But what if you want to run TPU on your own custom code?

13
00:00:45,130 --> 00:00:46,855
So you want to take your custom code,

14
00:00:46,855 --> 00:00:50,670
and you want them to execute it on the TPU.

15
00:00:50,670 --> 00:00:52,665
Well, first of all,

16
00:00:52,665 --> 00:00:58,330
I hope you're using the estimator API and the dataset API to read into your data.

17
00:00:58,330 --> 00:01:01,065
If you're not using the estimator and dataset,

18
00:01:01,065 --> 00:01:03,890
first rewrite your model using

19
00:01:03,890 --> 00:01:08,870
higher level TensorFlow abstractions instead of using the very low level.

20
00:01:08,870 --> 00:01:11,995
So let's say you're using a custom estimator,

21
00:01:11,995 --> 00:01:18,440
the TensorFlow TPU module gives you the equivalent of the regular classes.

22
00:01:18,440 --> 00:01:24,130
So to write code to run on a TPU we need to do four things;

23
00:01:24,130 --> 00:01:27,069
number one; replace the optimizer,

24
00:01:27,069 --> 00:01:31,185
number two; replace the EstimatorSpec, number three;

25
00:01:31,185 --> 00:01:36,040
replace our RunConfig, and number four; replace the Estimator.

26
00:01:36,040 --> 00:01:38,920
So let's look at each of these four steps.

27
00:01:38,920 --> 00:01:47,750
The first step is to wrap whatever optimizer you're using in a cross shard optimizer.

28
00:01:47,750 --> 00:01:50,690
What this does, what the cross shard optimizer

29
00:01:50,690 --> 00:01:54,590
does is that it takes advantage of the special architecture of

30
00:01:54,590 --> 00:01:59,029
the TPU and applies a novel Adam optimizer

31
00:01:59,029 --> 00:02:04,325
or Adagrad optimizer or whatever optimizer you're using on the TPU.

32
00:02:04,325 --> 00:02:09,020
Second step, instead of returning an estimator spec

33
00:02:09,020 --> 00:02:13,700
from your model function return a TPU estimator spec.

34
00:02:13,700 --> 00:02:15,800
The parameters are pretty much the same,

35
00:02:15,800 --> 00:02:19,190
so what you're doing is you're just changing the class name.

36
00:02:19,190 --> 00:02:24,350
Thirdly, add these lines to create

37
00:02:24,350 --> 00:02:29,735
a cluster resolver so that your training code can find the TPU,

38
00:02:29,735 --> 00:02:34,610
and also you will have to specify how often to checkpoint.

39
00:02:34,610 --> 00:02:37,770
Now once your code is running on a TPU,

40
00:02:37,770 --> 00:02:41,110
you'll want to minimize the input output overhead,

41
00:02:41,110 --> 00:02:44,270
so that the TPU is not waiting for the disk.

42
00:02:44,270 --> 00:02:47,210
So what you will typically do is that you will use

43
00:02:47,210 --> 00:02:52,305
much higher batch sizes than you do on a GPU.

44
00:02:52,305 --> 00:02:57,475
Another thing that you typically do is to check point less often,

45
00:02:57,475 --> 00:03:02,615
so here, am check pointing only once every thousand steps.

46
00:03:02,615 --> 00:03:06,890
Another thing that I'm doing is to do data parallelism,

47
00:03:06,890 --> 00:03:16,575
by specifying the per host input each TPU core gets its own input function.

48
00:03:16,575 --> 00:03:24,740
Finally, change your estimator from a plane estimator to a TPU estimator.

49
00:03:24,740 --> 00:03:28,170
Here too the parameters are pretty much the same,

50
00:03:28,170 --> 00:03:32,285
so what you're doing is you're simply changing the class name here.

51
00:03:32,285 --> 00:03:36,470
One best practice is to use

52
00:03:36,470 --> 00:03:43,625
a command line flag for the use_tpu property of the TPU estimator.

53
00:03:43,625 --> 00:03:49,940
Initially I would recommend setting the use_tpu to be false,

54
00:03:49,940 --> 00:03:55,245
make sure that your entire code works and finally when it works,

55
00:03:55,245 --> 00:03:57,665
flip the boolean flag to true.

56
00:03:57,665 --> 00:04:03,275
The reason is, that when you have use_tpu is true the TPU

57
00:04:03,275 --> 00:04:06,230
classes they change your graph and rewrite

58
00:04:06,230 --> 00:04:09,890
the graph significantly for the TPU architecture.

59
00:04:09,890 --> 00:04:12,085
So if you have error messages,

60
00:04:12,085 --> 00:04:15,680
those error messages on the TPU code will

61
00:04:15,680 --> 00:04:19,345
refer to functions and variables that are auto-generated,

62
00:04:19,345 --> 00:04:21,010
that you had no hand in writing.

63
00:04:21,010 --> 00:04:22,940
It's going to be really hard for you to understand

64
00:04:22,940 --> 00:04:25,100
what's going on if you're trying to debug

65
00:04:25,100 --> 00:04:29,690
your code after it's been converted and rewritten for TPUs.

66
00:04:29,690 --> 00:04:31,590
So, create your code,

67
00:04:31,590 --> 00:04:35,735
develop your code with use_tpu as false

68
00:04:35,735 --> 00:04:40,585
and then once the model code works flip the switch,

69
00:04:40,585 --> 00:04:45,065
that way your error messages if there's something wrong with the model

70
00:04:45,065 --> 00:04:49,960
comes from the regular TensorFlow graph and will reference your own functions,

71
00:04:49,960 --> 00:04:53,570
your own variables, and that's easier to fix.

72
00:04:53,570 --> 00:04:56,345
So with these four changes;

73
00:04:56,345 --> 00:05:01,230
cross shard optimizer, TPU estimator spec,

74
00:05:01,230 --> 00:05:10,710
run config, and TPU estimator your code should work on a CPU, GPU, or TPU.

75
00:05:10,710 --> 00:05:15,180
So, rather quickly, some points to keep in mind on

76
00:05:15,180 --> 00:05:20,940
TPUs because these help explain the code changes a little bit.

77
00:05:20,940 --> 00:05:25,715
Number one, there are four chips per TPU.

78
00:05:25,715 --> 00:05:28,480
There are two cores per chip,

79
00:05:28,480 --> 00:05:33,270
and each of those cores has eight gigs of memory.

80
00:05:33,270 --> 00:05:39,875
There are 512 cores or 64 TPUs within a pod.

81
00:05:39,875 --> 00:05:42,950
You might choose the number of cores that you want

82
00:05:42,950 --> 00:05:46,060
your training job to take within that number,

83
00:05:46,060 --> 00:05:52,305
of course you can even use more chips than the 64 chips on a TPU,

84
00:05:52,305 --> 00:05:55,220
but then you're using multiple pods and

85
00:05:55,220 --> 00:05:59,185
the distribution strategy has to change to accommodate that.

86
00:05:59,185 --> 00:06:03,130
TPUs provide high speed interconnect,

87
00:06:03,130 --> 00:06:10,315
so we tend not to worry about communication overhead within a TPU pod.

88
00:06:10,315 --> 00:06:13,635
So you have lots of memory on disk,

89
00:06:13,635 --> 00:06:16,700
remember by eight gigs of memory per core,

90
00:06:16,700 --> 00:06:24,760
so it means that you can have a lot more data and less frequent calls to the CPU.

91
00:06:24,760 --> 00:06:28,550
Scaling is pretty much linear.

92
00:06:28,550 --> 00:06:31,230
As you to increase a number of cores assigned to

93
00:06:31,230 --> 00:06:35,205
your job your training speed will increase linearly,

94
00:06:35,205 --> 00:06:42,025
and the interconnect within a TPU is much faster than the connection to the CPU.

95
00:06:42,025 --> 00:06:50,350
So we try to tend to do as much as we can inside the TPU without going back to the CPU.

96
00:06:50,620 --> 00:06:57,250
TPUs also offer very large matrix multiplication hardware,

97
00:06:57,250 --> 00:07:03,335
infact this is the primary speedup given by the TPU it's in the matrix multiplication.

98
00:07:03,335 --> 00:07:06,770
So points two and three mean that you should use

99
00:07:06,770 --> 00:07:12,415
much higher batch sizes in order to take full advantage of the TPU.

100
00:07:12,415 --> 00:07:18,590
Your peak performance is going to be possible when the batch size is divisible by

101
00:07:18,590 --> 00:07:25,170
128 because this will allow you to fully saturate that matrix multiply unit.

102
00:07:25,170 --> 00:07:28,940
If you can't get a batch size divided by 128 at

103
00:07:28,940 --> 00:07:33,655
least go to the batch size that's a multiple of eight.

104
00:07:33,655 --> 00:07:42,535
Because TPUs are most efficient when operating on batches of 128 or multiples of 128,

105
00:07:42,535 --> 00:07:48,915
TPU estimators are currently not designed for single predictions,

106
00:07:48,915 --> 00:07:50,895
so they're not for online prediction,

107
00:07:50,895 --> 00:07:54,900
they're intended mainly for batch predictions.

108
00:07:54,900 --> 00:07:59,950
If your use case requires latency sensitive predictions,

109
00:07:59,950 --> 00:08:05,915
it is recommended to create a TPU estimator after training your model,

110
00:08:05,915 --> 00:08:14,050
and set the use TPU flag to be false so that you can do the serving on a GPU or a CPU.

111
00:08:14,050 --> 00:08:17,600
So these four characteristics that we just looked at are

112
00:08:17,600 --> 00:08:22,990
essentially what dictate how we interact with the TPUs through code.

113
00:08:22,990 --> 00:08:27,110
The fact that TPUs have a specialized instruction set,

114
00:08:27,110 --> 00:08:31,095
means that they have to share responsibility along with other hardware.

115
00:08:31,095 --> 00:08:35,185
The high-speed interconnect, large amount of memory,

116
00:08:35,185 --> 00:08:37,280
and the large matrix multiplication

117
00:08:37,280 --> 00:08:41,090
unit means that accommodating this constraint isn't so bad,

118
00:08:41,090 --> 00:08:44,270
because certain workloads can just stay on the TPU

119
00:08:44,270 --> 00:08:48,070
for very long periods and be very efficient.

120
00:08:48,070 --> 00:08:54,690
The fifth point to remember about a TPU is that TPUs use bfloat.

121
00:08:54,690 --> 00:08:59,555
Bfloat is a new 16-bit representation of floating point numbers,

122
00:08:59,555 --> 00:09:02,300
that trades precision for density.

123
00:09:02,300 --> 00:09:06,010
So we can pack more bfloats onto a chip.

124
00:09:06,010 --> 00:09:08,465
Use of a bfloat is optional,

125
00:09:08,465 --> 00:09:11,300
if you want you can use float32 as

126
00:09:11,300 --> 00:09:16,180
your floating point representation almost everywhere in the graph.

127
00:09:16,180 --> 00:09:21,460
Another thing that you could do is to do everything in bfloat but then

128
00:09:21,460 --> 00:09:28,110
finally when you're exporting convert the weights from bfloat to float32.

129
00:09:28,150 --> 00:09:33,050
Having said that, you will get the maximum benefit of a TPU,

130
00:09:33,050 --> 00:09:35,495
if you use bfloat throughout.

131
00:09:35,495 --> 00:09:41,660
Even if you use float32 the matrix multiply will be carried out in bfloat.

132
00:09:41,660 --> 00:09:49,905
So, on a TPU use cases that absolutely require double-precision arithmetic,

133
00:09:49,905 --> 00:09:51,620
they're not a good fit.