1
00:00:00,000 --> 00:00:03,510
Hi there, I'm Lak and I lead the team that is

2
00:00:03,510 --> 00:00:07,035
putting together this course and this specialization.

3
00:00:07,035 --> 00:00:10,200
Welcome to Going Deeper and Faster.

4
00:00:10,200 --> 00:00:12,990
This is the fifth module in the third course

5
00:00:12,990 --> 00:00:16,280
of the advanced machine learning on GCP specialization,

6
00:00:16,280 --> 00:00:18,460
and we're talking about images.

7
00:00:18,460 --> 00:00:23,955
Neural networks aren't new and deep neural networks aren't new either.

8
00:00:23,955 --> 00:00:27,750
As we discussed in the courses on launching to machine learning

9
00:00:27,750 --> 00:00:31,915
and on the art and science of ML in the first specialization,

10
00:00:31,915 --> 00:00:37,320
a variety of small innovations make deep neural networks possible.

11
00:00:37,320 --> 00:00:41,145
The discovery of convolutional neural networks in particular,

12
00:00:41,145 --> 00:00:46,020
allowed for practical high accuracy models on images.

13
00:00:46,020 --> 00:00:51,080
People quickly discovered that the more layers an image model has,

14
00:00:51,080 --> 00:00:55,485
the better it performed but only up to a point.

15
00:00:55,485 --> 00:00:59,150
In this module, we'll focus on the problems that

16
00:00:59,150 --> 00:01:02,690
deep learning researchers encountered that prevented

17
00:01:02,690 --> 00:01:04,625
them from training deeper

18
00:01:04,625 --> 00:01:09,765
better-performing neural networks and how those problems are mitigated,

19
00:01:09,765 --> 00:01:12,000
how their effect is reduced.

20
00:01:12,000 --> 00:01:16,000
In this module, you will learn how to train deeper,

21
00:01:16,000 --> 00:01:20,640
more accurate networks and to do such training faster.

22
00:01:20,640 --> 00:01:24,350
You will learn about common problems that arise when training

23
00:01:24,350 --> 00:01:28,780
deeper networks and how researchers have been able to address these issues.

24
00:01:28,780 --> 00:01:33,785
The first of these problems is called internal covariate shift.

25
00:01:33,785 --> 00:01:38,145
A technique called batch normalization helps address it.

26
00:01:38,145 --> 00:01:43,415
You will learn how to implement batch normalization in deep neural networks.

27
00:01:43,415 --> 00:01:46,625
The next big advance was in adding

28
00:01:46,625 --> 00:01:52,134
shortcut connections and repeated structures to neural networks.

29
00:01:52,134 --> 00:01:54,085
So, you will learn how to do this.

30
00:01:54,085 --> 00:01:57,090
As networks get deeper,

31
00:01:57,090 --> 00:02:00,185
training them takes longer and longer.

32
00:02:00,185 --> 00:02:04,590
You will learn how to train deep networks on tensor processing units,

33
00:02:04,590 --> 00:02:07,100
on TPUs to do this faster.

34
00:02:07,100 --> 00:02:12,080
You will learn how to write a custom estimator for TPUs.

35
00:02:12,080 --> 00:02:20,130
Finally, you will learn how to automate network design using neural architecture search.

36
00:02:20,130 --> 00:02:23,380
CNNs were introduced in the early 1900s

37
00:02:23,380 --> 00:02:27,665
and they proved quite effective on handwriting recognition.

38
00:02:27,665 --> 00:02:36,080
But what really jump-started the deep neural network revolution was AlexNet, in 2012.

39
00:02:36,080 --> 00:02:39,870
AlexNet was a neural network with eight layers;

40
00:02:39,870 --> 00:02:46,185
three convolutional, two max pool and two fully connected and one softmax.

41
00:02:46,185 --> 00:02:53,750
It proved to be very effective even on more complex image classification tasks.

42
00:02:53,750 --> 00:02:56,330
When you take the top five error rate,

43
00:02:56,330 --> 00:03:00,510
which is a common benchmark for object recognition and computer vision,

44
00:03:00,510 --> 00:03:05,840
AlexNet reduced the top five error rate on a dataset

45
00:03:05,840 --> 00:03:11,745
called ImageNet in a competition from 26 percent to 15 percent.

46
00:03:11,745 --> 00:03:15,650
AlexNet was fundamentally different.

47
00:03:15,650 --> 00:03:19,490
Whereas previous contestants in this competition

48
00:03:19,490 --> 00:03:23,730
they had used traditional image processing and machine learning techniques,

49
00:03:23,730 --> 00:03:27,065
AlexNet proved that deep neural networks

50
00:03:27,065 --> 00:03:30,245
could not only compete but they could actually win.

51
00:03:30,245 --> 00:03:33,350
So, several factors were responsible for

52
00:03:33,350 --> 00:03:37,690
the revival in CNNs and we've talked about this earlier.

53
00:03:37,690 --> 00:03:42,200
First, the availability of a much larger training set,

54
00:03:42,200 --> 00:03:46,760
and this is where ImageNet comes in with millions of labeled examples.

55
00:03:46,760 --> 00:03:50,185
Number two, hardware accelerators in the form of

56
00:03:50,185 --> 00:03:55,400
GPUs that made the training of larger models practical.

57
00:03:55,400 --> 00:04:02,665
Three, tricks such as dropout to add better model regularization.

58
00:04:02,665 --> 00:04:05,860
So, why do CNNs work so well?

59
00:04:05,860 --> 00:04:11,720
One idea to understand why they work is to project

60
00:04:11,720 --> 00:04:17,610
the layer activations back to the input pixel space and show the top activations,

61
00:04:17,610 --> 00:04:20,635
which pixels get activated by which neurons.

62
00:04:20,635 --> 00:04:24,340
So, this graphic by my colleague Chris Olah

63
00:04:24,340 --> 00:04:28,745
illustrates what a network might learn in each layer.

64
00:04:28,745 --> 00:04:34,370
The initial layers, they consist of primitives like colors and edges.

65
00:04:34,370 --> 00:04:37,450
So, the neuron has a small receptive field.

66
00:04:37,450 --> 00:04:41,265
So, what you're seeing here in the visualization it's actually a tiling.

67
00:04:41,265 --> 00:04:48,685
So, what the neuron sees as one black dot surrounded by a light yellow or a lightweight.

68
00:04:48,685 --> 00:04:55,060
The next layers combine these black dots with yellows and whites in

69
00:04:55,060 --> 00:05:00,970
a hierarchical manner to start to identify corners and curves and textures.

70
00:05:00,970 --> 00:05:04,610
So, these textures now are more complex.

71
00:05:04,610 --> 00:05:07,210
They're not as local as they were in

72
00:05:07,210 --> 00:05:08,800
the previous layer because

73
00:05:08,800 --> 00:05:12,325
their receptive fields from the previous layer are being combined,

74
00:05:12,325 --> 00:05:15,415
but ultimately these things are still

75
00:05:15,415 --> 00:05:18,905
tied to which part of the image we are talking about.

76
00:05:18,905 --> 00:05:24,295
Later layers put these building blocks together

77
00:05:24,295 --> 00:05:29,715
to start to identify a recognizable aspects of the category being classified.

78
00:05:29,715 --> 00:05:33,730
For example, when you look at the features for the dog images,

79
00:05:33,730 --> 00:05:40,150
you see that discriminative features like eyes and fur are being emphasized.

80
00:05:40,150 --> 00:05:45,780
So, the eyes started off the first layer as being small dots surrounded by yellows,

81
00:05:45,780 --> 00:05:48,320
they got combined into a larger textures,

82
00:05:48,320 --> 00:05:49,770
and now in the final layer,

83
00:05:49,770 --> 00:05:52,250
you're basically seeing eyes and fur,

84
00:05:52,250 --> 00:05:54,470
things that are very discriminative of a dog.

85
00:05:54,470 --> 00:05:58,130
So, what this visualization technique shows is what

86
00:05:58,130 --> 00:06:03,030
parts of images individual filters responded to.

87
00:06:03,030 --> 00:06:07,790
This visualization proves in some way that

88
00:06:07,790 --> 00:06:12,105
CNNs what they learn is a hierarchy of features.

89
00:06:12,105 --> 00:06:17,090
So, see the link that's shown in this video for more analysis of

90
00:06:17,090 --> 00:06:22,700
what the later neural network layers do and what things they pick up on.

91
00:06:22,700 --> 00:06:25,770
Based on the visualization,

92
00:06:25,770 --> 00:06:30,400
the New York University of researchers were able to improve upon AlexNet,

93
00:06:30,400 --> 00:06:34,370
specifically by using a smaller receptive window and

94
00:06:34,370 --> 00:06:37,820
a smaller stride and they created a network called

95
00:06:37,820 --> 00:06:42,850
ZF Net which won the ImageNet competition in 2013.

96
00:06:43,340 --> 00:06:49,040
2014 rolled around VGGNet from a pair of

97
00:06:49,040 --> 00:06:54,375
researchers at Oxford and GoogLeNet from an outfit based in California,

98
00:06:54,375 --> 00:06:58,915
showed that deeper models yielded higher accuracy.

99
00:06:58,915 --> 00:07:04,580
VGGNet had 19 layers and GoogLeNet had 22 layers,

100
00:07:04,580 --> 00:07:08,345
almost three times as many layers as AlexNet.

101
00:07:08,345 --> 00:07:12,055
But why limited to 19 layers at 22 layers?

102
00:07:12,055 --> 00:07:15,440
Why not 150 layers?

103
00:07:15,440 --> 00:07:18,510
More is better, right?

104
00:07:18,510 --> 00:07:24,145
Unfortunately, when researchers tried to train deeper networks,

105
00:07:24,145 --> 00:07:28,160
they found that deeper networks take a very long time to

106
00:07:28,160 --> 00:07:33,115
train and they are very sensitive to hyperparameters,

107
00:07:33,115 --> 00:07:37,985
so research shifted to finding methods

108
00:07:37,985 --> 00:07:43,270
that would allow for robust training of really deep neural networks.

109
00:07:43,270 --> 00:07:46,450
Of course, as the number of layers goes up,

110
00:07:46,450 --> 00:07:52,100
the number of weights that need to be optimized also increases quite dramatically.

111
00:07:52,100 --> 00:07:57,410
The size of the dataset also increases dramatically.

112
00:07:57,410 --> 00:08:00,830
So, more layers, more weights, bigger datasets.

113
00:08:00,830 --> 00:08:04,505
We need to find methods that allow us to train

114
00:08:04,505 --> 00:08:11,135
really deep networks and really large datasets much much faster.

115
00:08:11,135 --> 00:08:15,890
The problems that occur in deep networks and the ways to get

116
00:08:15,890 --> 00:08:22,900
robust fast training despite these problems is what this module is about.