In this module, we learned why deep networks work. The earlier layers identify primitives in the images, that are then put together into matching filters by later layers. The deeper the network the more relationships the network can learn. However, we discovered that there were practical issues to simply training deeper and deeper networks. We learned about the problem of internal covariate shift, and how by normalizing the weights in each layer, you can train deeper models to be more accurate and to do this training faster. We learned that parallel paths and shortcuts can help us get more accurate models, and we discussed the ResNet architecture. We then looked at trends or processing units or TPUs, which are hardware accelerators that greatly speed up the training of deep learning models. We looked at how to train a ResNet model on your own data on the Cloud TPU. We also looked at how to implement custom estimators on the TPU. Finally, we discussed neural architecture search to automatically design high performing networks for a specific dataset and the specific hardware device. We started this module by saying that we were looking at ways to train deeper more accurate neural networks, and ways to train them faster. We followed the trajectory of research advances that steadily gave us better and better results, and ended up with a way to successfully automate model design. So, does this mean that all of our knowledge of image models is wasted? Not at all. AmoebaDB was trained for one task, image classification and one dataset, CIFAR 10 to train fast on one architecture TPS. For other tasks, other datasets, and other architectures, you may have to rerun this regimen, discover appropriate architectures, and train that neural network on your problem. Having said that though, much of this is automated. In the next module, we will look at AutoML, the Google product that does this for you with one press of a button. As a practitioner then, it's time to let the machines take care of design. The real action in machine learning is going to be in two areas. First, discovering what data to collect, designing the collection and curation of that data, and processing the data into a form that's amenable for machine learning. The second area that it's going to stay highly relevant is knowing what questions to ask and what models to build from that data. Once the model has produced a prediction, you will also have to figure out what decisions are suggested by those predictions. This is an inherently data-driven undertaking, and the availability of high quality predictions will enable better and more fine-grained decisions. So, on the one hand, the data engineer bringing in the data, and on the other hand the decision-makers in the product designers taking advantage of the machine learning models. In-between, simply taking up a cleaned up dataset and training a standard machine learning model to do some already specified set of predictions. That part in between is likely to get automated away. In other words, both ends of the data pipeline will remain highly important. The imagination and the entrepreneurship that's required to collect, curate, and process data, that entrepreneurship and imagination will remain highly valued. As with the boldness and the judgment required in making really fine grain decisions and machine learning enabled products. You will be a better data engineer, a better data analyst, and a better product manager, if you understand what the machines in the middle are doing. I hope that this course gave you that understanding.