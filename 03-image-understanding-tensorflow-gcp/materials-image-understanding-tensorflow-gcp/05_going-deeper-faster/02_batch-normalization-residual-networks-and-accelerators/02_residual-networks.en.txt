Let's talk about the next obstacle, that deep learning researchers faced in their quest to train deeper networks more quickly, the problem of gradient preservation. Even with batch norm, there were obstacles to making very deep networks. Although theoretically deeper networks should always perform better, when they did experiments, researchers discovered that adding depth seemed to actually worsen performance. For example, in a very influential paper, four researchers from Microsoft., showed a 56 layer network lagging behind the 20 layer network in terms of both training and test performance. Because there's a gap in their training error as well as the test error, this means that the tester or a gap is not just a consequence of overfitting. So, why would a 56 layer network perform so much worse than a 20 layer one? The problem when they investigated, seemed to be that gradients were not being preserved. Recall that the error from the output nodes gets propagated backwards and then each of the weights are updated based on the gradient of the error with respect to the weight, and this happens at every layer. So, as you add more and more layers, the signal that is used to make updates which is the error basically gets lost in transmission, because as you go backward in the layers, the size of the error that is being propagated is smaller and smaller. Some of the error has been used up in these last layers of the network. As a result, the gradients towards the early layers, they become vanishingly small. To some extent, batch normalization helps here, because what it does is that it normalizes all the weights within each layer, and even though all of those gradients are small, the weight updates get amplified in those earlier layers because they are normalized only within those layers. But even this works only to some extent. To get to more than about 20 layers, we need something more than just batch norm, we need something else. The GoogleNet paper from 2014, tried to tackle this problem of vanishing gradient descents using two novel ideas. The first idea was instead of having just one output node, have auxiliary outputs at intermediate layers in the network, to make sure that there is a stronger loss signal available to the early layers. The second idea was to have alternate routes through the network that are shorter, because shallower networks mean that the gradient gets preserved as it goes backward. In the picture, note the yellow rectangles. Those yellow rectangles are output layers and notice that in GoogleNet, you have output layers in the middle of the network. So, by computing the loss at these layers, the loss signal gets boosted. So, if this is a zoomed in picture of one small part of the network, note the connection on the left side, information travelling down this path has one fewer step to go than information traveling around other paths. Note also that GoogleNet use a repeating structure. Each of the blue boxes is a convolutional layer and each of the red boxes is a max pool layer. But instead of designing each layer individually, the Google Net designers used a repeating structure that consists of these convolutional max pool layers organized in a specific manner, and they distributed them throughout the network. So, expanding on these ideas of shortcuts and repeating structures, that yielded a network that was both significantly better performing and also about five times deeper. Instead of using a convolutional layer as a shortcut, the Microsoft researchers used an identity function. Note the identity function on the right hand edge. They call this entire structure residual network because once you take out the identity, what's left, the rest of the network is a residual. The intuition for why this, the identity function should help and definitely shouldn't hurt, stems from the fact that a network composed of many blocks like this is functionally equivalent to a shallower network where the weights in the longer path, they're zero, and information completely flows through the identity shortcut. A more complex rationale, stems from the fact that this network is changing the nature of the task. Instead of learning the mapping from inputs to outputs, which as you've seen becomes harder as networks become deeper, this network is trying to learn the difference between the desired output and the original inputs. This difference, that's almost like normalization. So, if learning that desired output relative to the original input is easier, then this should help substantially. So, this ResNet is residual network with 152 layers won the ImageNet Competition in 2015. Remember that the ImageNet dataset though, has one million images. So, ResNet 152 was okay. For smaller datasets, it's more common to use ResNet 18 or ResNet 50, same principle but fewer weights. Residual connections and network design remain extremely popular research areas. So, for more recent work on this topic, checkout papers on ResNet, DenseNet, FractalNet, SqueezeNet and on Stochastic Depth.