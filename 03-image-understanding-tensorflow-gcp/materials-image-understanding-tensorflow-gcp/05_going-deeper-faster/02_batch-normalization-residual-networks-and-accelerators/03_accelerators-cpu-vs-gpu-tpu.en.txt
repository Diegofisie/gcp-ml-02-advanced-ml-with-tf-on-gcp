The innovation of AlexNet was not just in the structure of the network, it was also in the choice of the researchers to use GPUs to do the network training. AlexNet was written with CUDA, a parallel computing API that was created by NVDA to run on special hardware called Graphics Processing Units or GPUs. One reason that deep neural networks were not used until then on the ImageNet competition was that training deep neural networks on a CPU would take way too long. Being able to train them on GPUs addressed that speed issue, because it greatly sped up the training if you did it using CUDA. Now, GPUs are typically used for graphics rendering such as in games. However, the availability of CUDA as a more general purpose software layer led to the use of GPUs to accelerate non-graphical applications also, in fields such as genomics, cryptography and deep learning. CUDA presents a unified memory, allowing for reads from arbitrary addresses and supports a wide variety of mathematical operations. This makes GPUs quite conducive to machine learning. This graphic shows the relative performance of different types of hardware devices, taking into account state of the art devices in each case. The first number, three, refers to the relative performance of a GPU to a CPU. Training on a Graphics Processing Unit is about three times faster than training on a plane Villainlar CPU. The remaining bars refer to a tensor processing unit or TPU, which is a custom ASIC developed by Google. We introduced you to TPUs in the previous course of this specialization. Now, let's explore TPUs some more. Tensor Processing Units or TPUs are hardware accelerators that greatly speed up the training of deep learning models. Unlike GPUs which are general purpose chips, TPUs are application-specific chips or ASICs. TPUs are custom built for machine learning. An independent test conducted by Stanford University, the ResNet 50 model trained on a TPU was the fastest in about 30 minutes to reach the desired accuracy on the ImageNet dataset. From AlexNet two AlphaGo Zero, there has been a 300,000 times increase in the compute power necessary to train the models. Note that the y-axis is logarithmic, and each tick is 10 times that of the previous tick. This graph is another way to look at the progress of deep learning, by plotting the increase in accuracy with the compute. Indeed, it's widely recognized that as much as 80% of recent AI advances can be attributed to more available compute power. So, where's this increase compute power going to come from? At Google, we did the back of the envelope calculation and realized that if everyone spoke into their phone for three minutes a day, and we needed to do speech to text, the compute power necessary to understand that speech would it be more than all the currently available compute power in the world today. So, this back of the envelope calculation was the impetus behind developing a custom ASICs from machine learning to greatly speed things up. As such, our first concern in TPU Version 1 was inference because we're talking about people speaking into phones and being able to understand what's being spoken to their Android phone. Only later in TPU Version 2 would we also focus on neural network training. We've continued to improve the TPU making it more and more powerful. Starting with Version 2, we've offered the TPU to everyone else via Google Cloud. Stanford University publishes a benchmark and a competition on different hardware accelerators. They tried ResNet 50 on a TPU version 2.5 pod on the ImageNet dataset, and found that it reached 93% accuracy in less than 30 minutes. The training speed worked out to processing 77,000 images per second.