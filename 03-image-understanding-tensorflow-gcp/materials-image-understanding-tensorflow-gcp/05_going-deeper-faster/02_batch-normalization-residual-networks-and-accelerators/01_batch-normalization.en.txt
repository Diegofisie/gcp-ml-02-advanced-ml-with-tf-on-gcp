In this module, we are discussing the obstacles that researchers faced in their attempts to train deeper and better performing neural networks, and what ideas have allowed them to overcome those initial obstacles. Specifically, we'll focus on internal covariate shift, which has addressed by batch normalization, gradient preservation, which led to the development of residual networks and AI accelerators. Then, we'll finish by talking about state of the art network designs. So let's start by talking about internal covariate shift. One of the big primary culprits for those long training times was internal covariate shift. It's a fancy term for pretty intuitive concept. Imagine that you're training a deep neural network. So based on a batch of training examples, you change the weights of layer number two. This change is based on the current values for the weights and layer number one. Now, in the next cycle with the next batch of inputs, suppose you update the weights for layer number one. Are the weights for layer number two, are they still optimal? No. The weight for layer number two were set based on the old values for layer one. So the bigger the changes to the weights, the more problematic this becomes, and the deeper the network, the more problematic this becomes because there are many more layers all of whose values are changing all over the place. So this effect is made worse by the shape of your common activation functions. So put yourself in the shoes of a neuron in layer number 50. Every time something changes in any of the layers before you, all the tuning that's happened to your weight, worthless. You don't want to think of neurons as people, but the neuron does exactly what a person would do in that circumstance. It retreats into the zone where the activation function is saturated so that all the changes to its inputs don't affect its outputs. The longer you train, the more of your neurons will go into the zone where they're completely impervious, where they don't listen to any of the outside effects. This is what internal covariate shift is. So how do you fix it? How do you let the neurons continue learning? One approach is to lower the learning rate. If each weight changes only a very small amount, then the changes are in that dramatic and perhaps the weights that preset and the other layers they aren't affected too dramatically. So the smaller the learning rate, the less chance you have of neurons stopping to learn. However, the smaller the learning rate, the slower the network is to train. So, this is what people said neural networks take a long time to train the deeper they are. So how do you fix this? One approach is to use dropout. When you put a dropout layers in between layers two and three, then you make the weights in the hidden layer number three somewhat robust to things that happen in hidden layer number two. However, dropout is a form of regularization. Regularization, by its very nature, is a way of limiting a neurous network's ability to learn, to overfit, but it's an ability to learn. So bottom line, internal covariates shift would typically limit how deep neural networks could be. The deeper the neural network, the harder it gets to train. So a better solution to the internal covariate shift problem was to basically transform the weights in each layer in such a way that subsequent layers could make assumptions about the weights in previous levels. Not about the individual weights, but about the weights of all the neurons in the previous layers. Specifically, a technique called batch normalization takes the weights of each layer and convert them into a z-score. In other words, the weights are transformed such that the average weight of all the neurons in a layer is zero and the variance is one. This is done even as the network is getting trained so that each layer can count on the fact that all of its inputs have been normalized. Even if an individual weight changes, overall, all the weights have the same statistical nature. Because batch normalization has this effect of enhancing the gradients at later iterations, you end up training faster as well. You see this in this graph of the number of training iterations that are required with and without batch normalization. If anything, this understates the effect of using batch norm. Because you don't have to worry too much about internal covariate shift, you can also use higher learning rates as well. Implementing batch normalization is quite straightforward in pre-made or canned estimator models. Just set the batch norm property to be true and you will get batch normalization after every one of the three hidden layers in this particular network. If you're using Keras to implement the model function in a custom estimator, simply add a batch norm layer to your Keras model and you are done. The mean and the variance are then computed during training using an exponential moving average. So the momentum term essentially determines the extent to which recent values of those weights are emphasized, compared to the values from, say, 10 iterations ago. Now, batch norm is a little trickier if you're using custom estimator and your model function is written using core TensorFlow. This code snippet here, shows you how to implement batch normalizations of the outputs of layer number two. The key thing to remember is that like dropout, batch normalization is an operation you do only during training. However, it's also necessary to change the training operation to compute the mean and variance of the weights. So this is done with the second part of the code snippet, where updating of the mean and variance are added as dependencies to the training operations. So they are always done when training happens. So since batch normalization came out, there has been much work to improve upon normalization within neural networks, and this remains an active area of research. If you're curious, checkout these works on weight normalization, layer normalization, self normalizing networks, et cetera.