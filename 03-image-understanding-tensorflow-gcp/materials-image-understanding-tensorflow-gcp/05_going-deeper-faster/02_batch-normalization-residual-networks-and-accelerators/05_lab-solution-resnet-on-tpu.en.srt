1
00:00:00,000 --> 00:00:02,530
Hi. In this lab,

2
00:00:02,530 --> 00:00:05,520
the idea is to train a state of the art image model,

3
00:00:05,520 --> 00:00:07,390
in this case on ResNet 50,

4
00:00:07,390 --> 00:00:10,580
on the Tensor Processing Unit or TPU.

5
00:00:10,580 --> 00:00:14,420
To do this, we don't actually have to write any code because,

6
00:00:14,420 --> 00:00:19,455
the code for ResNet 50 that runs on the TPU,

7
00:00:19,455 --> 00:00:21,035
it's already open source,

8
00:00:21,035 --> 00:00:22,695
it's in a GitHub repository,

9
00:00:22,695 --> 00:00:24,190
we just have to get the code.

10
00:00:24,190 --> 00:00:26,580
The other thing that we need to do, of course,

11
00:00:26,580 --> 00:00:32,340
is to basically take our data and put it into a form that that code reads.

12
00:00:32,340 --> 00:00:39,155
The code that runs on the TPU wants your image data in Tensor Processing records.

13
00:00:39,155 --> 00:00:41,765
So, first thing that we will do will be to take

14
00:00:41,765 --> 00:00:45,565
our JPEG data and convert them into TensorFlow records.

15
00:00:45,565 --> 00:00:51,950
Fortunately, the same repository also includes an Apache Beam Pipeline,

16
00:00:51,950 --> 00:00:54,460
that we can run to do this conversion.

17
00:00:54,460 --> 00:00:57,500
So, our lab is going to consist of two steps.

18
00:00:57,500 --> 00:00:59,815
One to run the pipeline,

19
00:00:59,815 --> 00:01:01,180
take our image data,

20
00:01:01,180 --> 00:01:05,035
converted into TensorFlow records.

21
00:01:05,035 --> 00:01:07,280
Then, and once we have the TensorFlow records,

22
00:01:07,280 --> 00:01:12,420
we then go ahead and run the image model code on the TPU.

23
00:01:12,420 --> 00:01:13,575
In order to do that,

24
00:01:13,575 --> 00:01:16,490
we can use Cloud ML Engine pointed at

25
00:01:16,490 --> 00:01:22,365
a Python package that essentially consists of the code that we downloaded from the Repo.

26
00:01:22,365 --> 00:01:24,200
So, let's get started.

27
00:01:24,200 --> 00:01:30,335
So, this is the code lab instructions and the basic steps are number one,

28
00:01:30,335 --> 00:01:33,170
to convert the JPEG files in TensorFlow records.

29
00:01:33,170 --> 00:01:35,540
Number two, to train the image classifier.

30
00:01:35,540 --> 00:01:39,220
But at that point, you have a trained model, that's not enough.

31
00:01:39,220 --> 00:01:40,980
We want to take that trained model,

32
00:01:40,980 --> 00:01:43,280
we want to deploy the trained model as

33
00:01:43,280 --> 00:01:46,500
a web service and we can do that also in ML Engine,

34
00:01:46,500 --> 00:01:51,240
and then we can invoke the web service by sending it a JPEG image.

35
00:01:51,240 --> 00:01:53,985
When you want to send a JPEG image to the web service,

36
00:01:53,985 --> 00:01:57,680
we have to base 64 encode the JPEG image contents,

37
00:01:57,680 --> 00:01:59,290
and we'll see how to do that as well.

38
00:01:59,290 --> 00:02:05,185
So, the the code lab starts with a brief primer on resonated TPUs.

39
00:02:05,185 --> 00:02:07,360
We looked at this already in the course,

40
00:02:07,360 --> 00:02:14,480
which is essentially the key idea behind the ResNet 50 or residual connection,

41
00:02:14,480 --> 00:02:17,209
is that whatever input you get basically

42
00:02:17,209 --> 00:02:20,480
passes through a shortcut connection directly to the output.

43
00:02:20,480 --> 00:02:24,440
So, that's your shallow connection and then the residual,

44
00:02:24,440 --> 00:02:27,310
that difference between the output and the input,

45
00:02:27,310 --> 00:02:31,530
that's the left hand side and that's not so shallow,

46
00:02:31,530 --> 00:02:33,965
that's a deeper side of the network.

47
00:02:33,965 --> 00:02:39,140
So, the idea is that if you build a neural network consisting of these blocks,

48
00:02:39,140 --> 00:02:44,855
they turn out to be much easier to train and that's the basic idea behind a ResNet model.

49
00:02:44,855 --> 00:02:51,280
But the idea also is that you still have the deeper layers available,

50
00:02:51,280 --> 00:02:55,790
and if you have a deep network and you have a large dataset,

51
00:02:55,790 --> 00:02:57,425
it takes a while to train.

52
00:02:57,425 --> 00:03:00,400
So, it's good to basically do the training on the TPU.

53
00:03:00,400 --> 00:03:02,990
In our case because it's just a lab,

54
00:03:02,990 --> 00:03:04,790
we're not going to take a humongous dataset,

55
00:03:04,790 --> 00:03:06,910
we're going to take a relatively small dataset,

56
00:03:06,910 --> 00:03:10,955
the Flowers dataset and we're going to see how to essentially train

57
00:03:10,955 --> 00:03:15,370
a state of the art model on the Flowers dataset, the ResNet model.

58
00:03:15,370 --> 00:03:17,230
But to do this on a TPU.

59
00:03:17,230 --> 00:03:21,040
First step then is to essentially set up our environment.

60
00:03:21,040 --> 00:03:22,755
The reason we're setting up the environment,

61
00:03:22,755 --> 00:03:25,460
is so that we can convert our Flowers data which is

62
00:03:25,460 --> 00:03:28,360
that JPEG data into TensorFlow records.

63
00:03:28,360 --> 00:03:32,430
In order to do that, we're asked to basically go ahead and start Cloud Shell.

64
00:03:32,430 --> 00:03:34,000
So, let me go ahead and do that.

65
00:03:34,000 --> 00:03:37,795
I will start Cloud Shell from the GCP Console.

66
00:03:37,795 --> 00:03:43,130
Having started CloudShell, we're basically asked to go ahead and make sure that

67
00:03:43,130 --> 00:03:49,429
the project is set appropriately and that we have the ML Engine API enabled,

68
00:03:49,429 --> 00:03:55,915
which we can do by going to the library page and making sure that the ML Engine.

69
00:03:55,915 --> 00:03:58,370
So, we can do ML Engine,

70
00:03:58,720 --> 00:04:04,430
Cloud Machine Learning Engine and there it is,

71
00:04:04,430 --> 00:04:06,740
and let's make sure that it is enable.

72
00:04:06,740 --> 00:04:09,360
So, this is already enabled for me,

73
00:04:09,360 --> 00:04:11,730
that's why it shows me the "Manage" button.

74
00:04:11,730 --> 00:04:13,555
If it shows you the "Enable" button,

75
00:04:13,555 --> 00:04:15,955
you would basically go ahead and enable it.

76
00:04:15,955 --> 00:04:20,270
Then, we also need to enable the Dataflow API.

77
00:04:20,270 --> 00:04:21,440
So, we can do the same thing,

78
00:04:21,440 --> 00:04:27,320
we can go into Dataflow and there is

79
00:04:27,320 --> 00:04:34,560
the Cloud Dataflow API and that too is enabled.

80
00:04:34,560 --> 00:04:36,205
That's why I see the "Manage" button.

81
00:04:36,205 --> 00:04:40,520
So, that's also enabled and now we can basically go into

82
00:04:40,520 --> 00:04:45,465
the Cloud Shell repository and git clone these Repo.

83
00:04:45,465 --> 00:04:47,045
But let's just make sure,

84
00:04:47,045 --> 00:04:52,665
that the project it is correctly set as cloud training demos,

85
00:04:52,665 --> 00:04:55,520
so I'm fine there as far as the project is concerned.

86
00:04:55,520 --> 00:04:58,600
The UI will be different regardless of now,

87
00:04:58,600 --> 00:04:59,980
the UI keep changing.

88
00:04:59,980 --> 00:05:03,800
So, it's going to be looks something like this with the same set of steps.

89
00:05:03,800 --> 00:05:06,440
If the UI looks slightly different for you,

90
00:05:06,440 --> 00:05:08,480
don't be worried, it's all okay.

91
00:05:08,480 --> 00:05:10,890
So, let's go ahead and get clone the repository.

92
00:05:10,890 --> 00:05:14,120
So, we can do this and that also,

93
00:05:14,120 --> 00:05:16,070
I may have done already.

94
00:05:16,070 --> 00:05:21,410
So, git clone that repository.

95
00:05:21,410 --> 00:05:29,340
At this point, the code repository is cloned into my Cloud Shell account.

96
00:05:29,340 --> 00:05:32,240
The next thing that we're asked to do,

97
00:05:32,240 --> 00:05:35,130
is to basically go ahead and explore the data.

98
00:05:35,130 --> 00:05:39,874
So, this data is a set of professional photographs of flowers,

99
00:05:39,874 --> 00:05:43,700
which basically has your training dataset and evaluation dataset.

100
00:05:43,700 --> 00:05:46,515
So, let's go look at what the training dataset looks like.

101
00:05:46,515 --> 00:05:48,735
So, the training dataset,

102
00:05:48,735 --> 00:05:51,160
which got downloaded here,

103
00:05:51,160 --> 00:05:53,930
this is basically what it looks like.

104
00:05:53,930 --> 00:06:01,580
It basically has a URL of the image and the category,

105
00:06:01,580 --> 00:06:03,860
the classification or the label of the image.

106
00:06:03,860 --> 00:06:06,460
The first image is that of a daisy.

107
00:06:06,460 --> 00:06:09,440
The second image is that of a dandelion and the third,

108
00:06:09,440 --> 00:06:11,090
this image is that of roses.

109
00:06:11,090 --> 00:06:14,555
So, we could basically say let's go ahead and look at this image.

110
00:06:14,555 --> 00:06:17,330
Now, because this is a public data set,

111
00:06:17,330 --> 00:06:20,390
I can actually look at this image by changing

112
00:06:20,390 --> 00:06:30,190
the gs URL to API storage,

113
00:06:30,190 --> 00:06:39,975
let me just go ahead and try that or actually the code lab probably tells us to do this.

114
00:06:39,975 --> 00:06:44,840
Okay. So, let's go ahead and view the image.

115
00:06:44,840 --> 00:06:48,695
So, you see the storage.cloud.google.com with,

116
00:06:48,695 --> 00:06:51,510
this is the bucket name and the rest of the image.

117
00:06:51,510 --> 00:06:55,535
So, we can go ahead and click on this image and we basically get

118
00:06:55,535 --> 00:07:00,535
the photograph of what this is and this is,

119
00:07:00,535 --> 00:07:08,045
if you go ahead and look at this image,

120
00:07:08,045 --> 00:07:09,845
the image that just came up,

121
00:07:09,845 --> 00:07:15,200
that image was a very first one and so, it is a daisy.

122
00:07:15,200 --> 00:07:17,625
I wouldn't know the names of the flowers,

123
00:07:17,625 --> 00:07:20,540
but I'll trust that the labels are correct..

124
00:07:20,540 --> 00:07:26,485
So, let's go back here and look at this one.

125
00:07:26,485 --> 00:07:29,830
So, what this does is basically go to GS,

126
00:07:29,830 --> 00:07:32,780
the Google Cloud Storage and look at

127
00:07:32,780 --> 00:07:38,655
the first five lines and create a file called temp input dot CSV.

128
00:07:38,655 --> 00:07:41,230
So, let's go ahead and do that.

129
00:07:41,740 --> 00:07:45,325
So I can turn off this, let's go here.

130
00:07:45,325 --> 00:07:52,485
So here, if I were to go down here and do this, at this point,

131
00:07:52,485 --> 00:07:59,125
the first five lines will go into /tmp/input.csv and you

132
00:07:59,125 --> 00:08:08,300
can see that those

133
00:08:08,300 --> 00:08:10,005
are what-if lines look like.

134
00:08:10,005 --> 00:08:12,375
That's essentially what we looked at earlier.

135
00:08:12,375 --> 00:08:14,215
The format of the file,

136
00:08:14,215 --> 00:08:18,050
it looks like it's a comma separated file.

137
00:08:18,180 --> 00:08:23,235
How would you find all of the types of flowers in the dataset?

138
00:08:23,235 --> 00:08:25,785
Well, what we could do,

139
00:08:25,785 --> 00:08:31,495
is to basically go ahead and look at this tmp.csv.

140
00:08:31,495 --> 00:08:40,390
I can do cat/tmp/input.csv and replace all of the commas by

141
00:08:40,390 --> 00:08:50,085
spaces and I could print dollar one and that would print the URL,

142
00:08:50,085 --> 00:08:52,055
and if I print dollar two,

143
00:08:52,055 --> 00:08:53,875
that would print the labels.

144
00:08:53,875 --> 00:08:57,180
So, if I were to take those labels and I were to sort them.

145
00:08:57,180 --> 00:09:01,000
Now, those are now sorted and it can do unique

146
00:09:01,000 --> 00:09:05,345
and that'll give me just a unique numbers and that's my dictionary.

147
00:09:05,345 --> 00:09:06,740
My set of labels.

148
00:09:06,740 --> 00:09:09,950
Of course instead of doing the tmp/input.csv,

149
00:09:09,950 --> 00:09:14,750
I could basically do gsutil cat and that's what this thing is doing.

150
00:09:14,750 --> 00:09:21,945
So, it's doing a gsutil cat and doing the comma to spaces,

151
00:09:21,945 --> 00:09:24,390
printing out the second field, sorting it,

152
00:09:24,390 --> 00:09:28,225
find the unique values and finding all of the labels.

153
00:09:28,225 --> 00:09:32,290
So, let's go ahead and do that and once we do that,

154
00:09:32,290 --> 00:09:37,700
we should be able to do cat/tmp/labels.txt,

155
00:09:37,700 --> 00:09:45,900
and we find that there are 2,3,4,5 labels in the dataset,

156
00:09:45,900 --> 00:09:49,235
and we've already viewed the image.

157
00:09:49,235 --> 00:09:53,820
So, now we are ready to go ahead and convert

158
00:09:53,820 --> 00:09:58,620
all the JPEG images to TensorFlow records and as I mentioned in the intro,

159
00:09:58,620 --> 00:10:05,540
this is already part of the open source repository and we basically

160
00:10:05,540 --> 00:10:13,400
have some code that is written all ready to go and grab the data in that repository.

161
00:10:15,660 --> 00:10:21,890
Let's go ahead and let me copy this again into my buffer.

162
00:10:25,590 --> 00:10:29,050
There we go, that's now copied.

163
00:10:29,050 --> 00:10:37,335
So in here, there is a script to copy all the ResNet files and what this does,

164
00:10:37,335 --> 00:10:45,450
is that it basically goes ahead and starts with a git checkout of a particular version.

165
00:10:45,450 --> 00:10:51,660
So, in this case I'll be checking out a version say 1.8 of the TensorFlow code from

166
00:10:51,660 --> 00:10:54,460
that repository and having done that

167
00:10:54,460 --> 00:10:58,755
basically creating it and making a Python package out of it.

168
00:10:58,755 --> 00:11:00,305
So, let's go ahead and do this.

169
00:11:00,305 --> 00:11:07,110
So, bash./ copy_resnet_files for TensorFlow version 1.8.

170
00:11:08,200 --> 00:11:11,180
When you are doing it we might have updated

171
00:11:11,180 --> 00:11:13,820
the version but the steps are going to remain the same.

172
00:11:13,820 --> 00:11:15,600
So at this point then,

173
00:11:15,600 --> 00:11:19,430
we have my model which basically has

174
00:11:19,430 --> 00:11:26,395
the familiar Cloud ML Engine packaging instructions with a trainer setup.py et cetera,

175
00:11:26,395 --> 00:11:30,265
but it also includes the resnet_

176
00:11:30,265 --> 00:11:34,815
main.py which is basically the code that we're going to run our training on,

177
00:11:34,815 --> 00:11:38,565
and a preprocess.py which is an Apache Beam pipeline.

178
00:11:38,565 --> 00:11:42,160
So, let's go ahead and

179
00:11:43,030 --> 00:11:50,585
specify environment variables for the bucket, the project name.

180
00:11:50,585 --> 00:11:52,355
So, in my case,

181
00:11:52,355 --> 00:11:59,645
my bucket is called Cloud Training Demos ML,

182
00:11:59,645 --> 00:12:03,530
and my project name is,

183
00:12:03,530 --> 00:12:08,055
I can just get the project name from my Cloud Shell which is what we're doing here.

184
00:12:08,055 --> 00:12:11,350
So, let's go ahead and get the project name from the Cloud Shell,

185
00:12:11,350 --> 00:12:16,880
and we can echo the bucket and the project to make sure

186
00:12:16,880 --> 00:12:22,260
they look reasonable and yes, that is a bucket.

187
00:12:22,260 --> 00:12:24,035
Actually no, the bucket is wrong,

188
00:12:24,035 --> 00:12:28,580
it should be Cloud Training Demos ML.

189
00:12:28,580 --> 00:12:30,645
Good thing I checked.

190
00:12:30,645 --> 00:12:36,830
Now, if I do that, yes the bucket name is correct and the project name is correct.

191
00:12:36,830 --> 00:12:41,530
So now let's go ahead and install Apache Beam.

192
00:12:41,530 --> 00:12:45,765
Apache Beam for those of you who've been here in our previous courses,

193
00:12:45,765 --> 00:12:51,235
this is a way to do Batch processing and Stream processing using the same code.

194
00:12:51,235 --> 00:12:55,415
It essentially distributes all of the processing onto multiple machines,

195
00:12:55,415 --> 00:12:59,525
does these things in parallel and we'll get our output.

196
00:12:59,525 --> 00:13:03,010
So, we are now installing the package for

197
00:13:03,010 --> 00:13:10,265
Apache Beam and then we can then run the conversion program.

198
00:13:10,265 --> 00:13:14,160
So, that's conversion program is a pre-process program that came

199
00:13:14,160 --> 00:13:18,260
from that open source repository consisting of TPU models,

200
00:13:18,260 --> 00:13:22,495
and we're saying that our training CSV file is

201
00:13:22,495 --> 00:13:29,135
in that particular file and our validation CSV is that file,

202
00:13:29,135 --> 00:13:32,450
and our labels consist of the tmp/labels.txt that I

203
00:13:32,450 --> 00:13:36,100
just created by doing the sort and the unique et cetera,

204
00:13:36,100 --> 00:13:37,880
specifying my project ID.

205
00:13:37,880 --> 00:13:41,220
So, we know where to send the bill for doing

206
00:13:41,220 --> 00:13:46,980
this distributed conversion of JPEG files to TensorFlow records where to send that bill.

207
00:13:46,980 --> 00:13:50,075
Then specifying an output directory.

208
00:13:50,075 --> 00:13:53,470
So, we can go ahead and do this,

209
00:13:57,620 --> 00:14:02,200
clear that, so here.

210
00:14:03,380 --> 00:14:05,680
Okay.

211
00:14:13,290 --> 00:14:16,700
I still made a mistake.

212
00:14:24,890 --> 00:14:29,930
It's a good thing I made a mistake because what I'll do just to save time,

213
00:14:29,930 --> 00:14:33,060
is that I will not remove the previous bucket.

214
00:14:33,060 --> 00:14:36,840
So, what I'll do is simply is now to remove the bucket.

215
00:14:36,840 --> 00:14:42,330
So, I'll keep that original data as it is and instead just run the pre-processing.

216
00:14:42,330 --> 00:14:47,030
But I'll do the pre-processing into a different directory just to

217
00:14:47,030 --> 00:14:52,700
see it running but it will be data two or data delete.

218
00:14:52,700 --> 00:14:55,205
Let me just call it data delete.

219
00:14:55,205 --> 00:14:58,770
So it runs and then I'll delete that later.

220
00:14:58,770 --> 00:15:03,860
The reason I'm doing that is that if I go into my cloud storage bucket,

221
00:15:03,860 --> 00:15:08,540
storage in my browser

222
00:15:09,710 --> 00:15:16,330
and go into Cloud Training Demos ML and in there,

223
00:15:16,330 --> 00:15:19,880
there is a thing called TPU.

224
00:15:26,160 --> 00:15:29,320
Whole bunch of other stuff,

225
00:15:29,320 --> 00:15:35,220
I have to run these things but there should be a TPU version somewhere.

226
00:15:37,330 --> 00:15:40,294
There it is TPU,

227
00:15:40,294 --> 00:15:45,040
and ResNet and I already had a data,

228
00:15:45,040 --> 00:15:49,270
I'm just creating now a data delete that the new things are going to go in.

229
00:15:49,270 --> 00:15:51,170
So, inside my data,

230
00:15:51,170 --> 00:15:53,330
when the whole thing has been run,

231
00:15:53,330 --> 00:15:57,740
I will have my training files and I'll have my validation files.

232
00:15:57,740 --> 00:16:01,570
Meanwhile, if I go into dataflow.

233
00:16:01,570 --> 00:16:08,470
So, scroll down into dataflow which is where I submitted the job,

234
00:16:08,470 --> 00:16:12,335
you should see a job running for pre-processing of

235
00:16:12,335 --> 00:16:16,570
images and this particular job currently

236
00:16:16,570 --> 00:16:19,960
right's basically reading from the CSV files during

237
00:16:19,960 --> 00:16:25,260
the conversion from JPEG to TensorFlow records and writing out the TF records.

238
00:16:25,260 --> 00:16:29,240
Over time this will auto-scale to multiple robe workers,

239
00:16:29,240 --> 00:16:31,700
and about 15-20 minutes later I will have it all

240
00:16:31,700 --> 00:16:35,960
done and the pipeline options in this case,

241
00:16:36,660 --> 00:16:42,170
let's see the project name is that and that's the job name et cetera.

242
00:16:42,170 --> 00:16:46,555
So we can wait for this to run but fortunately we already have it run.

243
00:16:46,555 --> 00:16:50,440
So, at this point we have started the dataflow job.

244
00:16:50,440 --> 00:16:54,140
It's going to take about 15 minutes and at the end of 15 minutes,

245
00:16:54,140 --> 00:16:55,610
you will see all of those files,

246
00:16:55,610 --> 00:16:59,135
a training files, a validation files et cetera in your bucket,

247
00:16:59,135 --> 00:17:01,220
and that's where you would basically continue.

248
00:17:01,220 --> 00:17:03,950
So, you're going to be waiting for the dataflow job to finish.

249
00:17:03,950 --> 00:17:06,770
I on the other hand have already run this before.

250
00:17:06,770 --> 00:17:09,440
So what I'll do is I'll continue with

251
00:17:09,440 --> 00:17:12,825
the results of my dataflow job from the previous run.

252
00:17:12,825 --> 00:17:15,640
But if you watch over time,

253
00:17:15,640 --> 00:17:18,290
this will basically turn to running and

254
00:17:18,290 --> 00:17:22,370
then the number of workers here will increase beyond one,

255
00:17:22,370 --> 00:17:25,175
it'll start getting parallelized, getting autoscaled,

256
00:17:25,175 --> 00:17:26,870
it'll start getting processed and

257
00:17:26,870 --> 00:17:30,880
this entire dataset will all get converted into TF records.

258
00:17:30,880 --> 00:17:32,980
So, the CSV files would get read,

259
00:17:32,980 --> 00:17:35,355
they would get converted and they will get written out.

260
00:17:35,355 --> 00:17:36,800
So, wait for to finish,

261
00:17:36,800 --> 00:17:38,475
me on the other hand,

262
00:17:38,475 --> 00:17:42,300
I'm going to say my data is already there so let me go ahead and move on.

263
00:17:42,300 --> 00:17:43,655
So, me on the other hand,

264
00:17:43,655 --> 00:17:47,360
I'm basically going to essentially go with

265
00:17:47,360 --> 00:17:51,930
the files that I already have and move on to the next step which is to train the model.

266
00:17:51,930 --> 00:17:53,265
To train the model,

267
00:17:53,265 --> 00:17:57,065
let's just go ahead and make sure that the data actually exists.

268
00:17:57,065 --> 00:18:00,950
So, we can go in here because that job is still running.

269
00:18:00,950 --> 00:18:03,470
I'll open up a new Cloud Shell tab.

270
00:18:03,470 --> 00:18:06,890
Remember that output is going into data too,

271
00:18:06,890 --> 00:18:09,170
so, it's not really going to affect anything here.

272
00:18:09,170 --> 00:18:16,060
Let me go ahead and say that this may not work because export bucket hasn't been set.

273
00:18:16,060 --> 00:18:18,345
No, bucket hasn't been set.

274
00:18:18,345 --> 00:18:27,490
So, export BUCKET=cloud-training-demos-ml and export.

275
00:18:27,490 --> 00:18:31,660
Project equals cloud training demos.

276
00:18:31,660 --> 00:18:36,575
So now, let's make sure that the data exists,

277
00:18:36,575 --> 00:18:38,290
and the data does exist,

278
00:18:38,290 --> 00:18:41,225
I have my training files and I have my validation files.

279
00:18:41,225 --> 00:18:50,190
So, let's go ahead and enable ML engine to be able to access these things.

280
00:18:50,190 --> 00:18:54,830
So, I need to be in the training demos directory.

281
00:18:54,830 --> 00:18:56,875
What was the directory there?

282
00:18:56,875 --> 00:18:59,485
Training analyst quest steep,

283
00:18:59,485 --> 00:19:02,535
actually what I can do I'll do Ctrl+Z,

284
00:19:02,535 --> 00:19:07,755
background it and so now I'm actually in the original thing,

285
00:19:07,755 --> 00:19:14,360
original window itself and I can enable the TPU.

286
00:19:15,160 --> 00:19:24,420
So, let's go ahead and and enable the TPU and having done that,

287
00:19:24,420 --> 00:19:27,360
I can then submit my training job.

288
00:19:27,360 --> 00:19:29,310
So, let's walk through what this does.

289
00:19:29,310 --> 00:19:31,490
It says that this is the top directory,

290
00:19:31,490 --> 00:19:34,365
the directory where I have the data.

291
00:19:34,365 --> 00:19:38,140
In my case, I would like to run this in central one because that's where I

292
00:19:38,140 --> 00:19:41,670
have TPU quarter and I want my output to go into

293
00:19:41,670 --> 00:19:45,410
the trained directory and I will remove

294
00:19:45,410 --> 00:19:48,970
the prior outputs that way

295
00:19:48,970 --> 00:19:53,100
because remember that the way TensorFlow works is that if it sees a checkpoint,

296
00:19:53,100 --> 00:19:54,880
it starts from that checkpoint.

297
00:19:54,880 --> 00:19:56,170
So, if you've already trained,

298
00:19:56,170 --> 00:19:58,645
you want to remove that old one and restart it.

299
00:19:58,645 --> 00:20:02,100
So, we do that and then we will basically

300
00:20:02,100 --> 00:20:05,640
go ahead and submit your training job for a particular region,

301
00:20:05,640 --> 00:20:10,160
the module that we want to start as trained resnet main, resnet_main.

302
00:20:10,160 --> 00:20:16,640
Again, the file that we downloaded from the official TPU repo and specifying

303
00:20:16,640 --> 00:20:20,380
the package path and specifying that the scale tier is

304
00:20:20,380 --> 00:20:24,630
BASIC_TPU and that the runtime version is 1.8.

305
00:20:24,630 --> 00:20:29,480
Remember that we downloaded TensorFlow version 1.8 version of the files.

306
00:20:29,480 --> 00:20:33,935
So, in this case, we're also training with TensorFlow 1.8.

307
00:20:33,935 --> 00:20:36,390
We pointed at the data directory,

308
00:20:36,390 --> 00:20:39,600
the directory that contains a TensorFlow records and we

309
00:20:39,600 --> 00:20:43,340
want the output to go into the output directory that we specify.

310
00:20:43,340 --> 00:20:46,840
We want to use ResNet 18.

311
00:20:46,840 --> 00:20:49,575
Why 18? Because my dataset isn't very big.

312
00:20:49,575 --> 00:20:51,290
If I use ResNet 152,

313
00:20:51,290 --> 00:20:52,400
I just don't have enough data,

314
00:20:52,400 --> 00:20:53,720
I need a million images,

315
00:20:53,720 --> 00:20:55,020
I have a few thousands.

316
00:20:55,020 --> 00:20:57,770
So, ResNet 18 is small enough that maybe okay.

317
00:20:57,770 --> 00:21:04,600
So, I'll use ResNet 18 on this data and I want to specify my batch size.

318
00:21:04,600 --> 00:21:06,955
We talked about this, right?

319
00:21:06,955 --> 00:21:14,375
For ideal training, your TensorFlow batch size for training should be a multiple of 128.

320
00:21:14,375 --> 00:21:15,600
So, that's what I'm doing here,

321
00:21:15,600 --> 00:21:19,400
I'm passing it a multiple of 128 to basically do my training,

322
00:21:19,400 --> 00:21:22,225
that's the smallest batch size that I can sort of get away with.

323
00:21:22,225 --> 00:21:24,310
My dataset again is in very large.

324
00:21:24,310 --> 00:21:28,860
In reality, you want this to be something like 1,024 or something larger.

325
00:21:28,860 --> 00:21:32,090
Number of steps per eval, 250 steps.

326
00:21:32,090 --> 00:21:34,205
So, evaluate every 250 steps,

327
00:21:34,205 --> 00:21:39,570
train for a 1,000 steps and the number of training images is 3,300,

328
00:21:39,570 --> 00:21:42,530
number of evaluation images are 370,

329
00:21:42,530 --> 00:21:47,850
this is what is actually in my dataset and the number of label classes is five.

330
00:21:47,850 --> 00:21:53,905
Then, I ask for the model to get exported into this output directory.

331
00:21:53,905 --> 00:21:55,570
So, let's go ahead and run this,

332
00:21:55,570 --> 00:21:57,360
but as before with the dataflow,

333
00:21:57,360 --> 00:21:58,800
I've already run this before,

334
00:21:58,800 --> 00:22:01,395
that way I can move on to the next step in my demo.

335
00:22:01,395 --> 00:22:06,190
So, I will change my output directory to be not that directory,

336
00:22:06,190 --> 00:22:08,455
but something slightly different.

337
00:22:08,455 --> 00:22:10,030
So, I will do this,

338
00:22:10,030 --> 00:22:13,880
the output directory I will say is trained delete.

339
00:22:13,880 --> 00:22:20,165
So, I know how to delete it later and then I can go ahead and run the rest of the stuff.

340
00:22:20,165 --> 00:22:24,740
All right, go ahead and do that.

341
00:22:29,220 --> 00:22:39,605
At this point, the training job will be submitted and on ML engine if we go,

342
00:22:39,605 --> 00:22:42,225
we should find that there is a job running.

343
00:22:42,225 --> 00:22:44,130
So, I can go into ML Engine,

344
00:22:44,130 --> 00:22:48,000
I can go into jobs and I should see

345
00:22:48,000 --> 00:22:52,595
a training job for image classification that has just started,

346
00:22:52,595 --> 00:22:54,635
it's now under preparation.

347
00:22:54,635 --> 00:22:56,730
So again, this thing is going to run,

348
00:22:56,730 --> 00:22:57,910
it's going to train on the TPU,

349
00:22:57,910 --> 00:23:01,125
it's going to take about 20 minutes at the end of it,

350
00:23:01,125 --> 00:23:05,035
we will basically have a trained model.

351
00:23:05,035 --> 00:23:06,315
So, where would that be?

352
00:23:06,315 --> 00:23:08,285
That would be in our output directory.

353
00:23:08,285 --> 00:23:11,600
So, if we could also go ahead and look at TensorBoard and we

354
00:23:11,600 --> 00:23:15,040
could look at the model training.

355
00:23:15,040 --> 00:23:16,730
So, let's go ahead and do that.

356
00:23:16,730 --> 00:23:22,060
Around here, model training

357
00:23:30,180 --> 00:23:34,905
and set preview on 8080.

358
00:23:34,905 --> 00:23:37,480
So, this is the web preview,

359
00:23:37,480 --> 00:23:45,045
preview on 8080 and we should see TensorBoard come up and once TensorBoard has loaded,

360
00:23:45,045 --> 00:23:47,510
we can go ahead and look at the scalar graphs,

361
00:23:47,510 --> 00:23:49,950
we can look at the accuracy plots et cetera.

362
00:23:49,950 --> 00:23:55,180
So, there they are. So, this tells you the speed at which things got loaded,

363
00:23:55,180 --> 00:23:58,860
but we can go look at the top one accuracy which is like

364
00:23:58,860 --> 00:24:03,060
the probability that it got the label correctly and you will

365
00:24:03,060 --> 00:24:05,500
see that it basically started out at

366
00:24:05,500 --> 00:24:11,740
about 62 percent and then overtime right it basically ended up at 75 percent.

367
00:24:11,740 --> 00:24:15,240
It looks like the accuracy has kept increasing.

368
00:24:15,240 --> 00:24:18,005
So in reality, we trained for more than 1,000 steps.

369
00:24:18,005 --> 00:24:21,270
We want to basically see this thing saturate and it hasn't,

370
00:24:21,270 --> 00:24:22,940
but again, this is a lab,

371
00:24:22,940 --> 00:24:24,960
we wanted it to finish relatively quickly,

372
00:24:24,960 --> 00:24:29,050
so we cut this training relatively short is a trained for just a 1,000 steps.

373
00:24:29,050 --> 00:24:33,765
In reality, you train until this thing saturates wherever it does.

374
00:24:33,765 --> 00:24:39,020
So, we got about 75 percent accuracy on ResNet.

375
00:24:39,020 --> 00:24:41,580
Again, out of the box model,

376
00:24:41,580 --> 00:24:45,750
we just ran it on our data and we were able to do this very quickly,

377
00:24:45,750 --> 00:24:49,355
right about 20 to 25 minutes for training and we're done,

378
00:24:49,355 --> 00:24:53,860
which is actually quite cool.

379
00:24:53,860 --> 00:24:57,080
We can also look at the loss curves.

380
00:24:57,080 --> 00:24:59,470
So, that's the loss curve and you basically

381
00:24:59,470 --> 00:25:03,880
see the result and you see that it isn't actually very overfit,

382
00:25:03,880 --> 00:25:05,590
they're pretty close to each other.

383
00:25:05,590 --> 00:25:07,310
So, at any point in time, right?

384
00:25:07,310 --> 00:25:11,520
They are relatively close. Let's see.

385
00:25:12,600 --> 00:25:14,810
Again, if you say, "Well,

386
00:25:14,810 --> 00:25:16,340
my sand if I press it isn't good enough,

387
00:25:16,340 --> 00:25:18,930
how do I get better?" Well, you need more data.

388
00:25:18,930 --> 00:25:23,025
That's your typical, for such a small dataset,

389
00:25:23,025 --> 00:25:26,315
this is pretty reasonable on a deep learning model.

390
00:25:26,315 --> 00:25:27,940
So, once we have our model,

391
00:25:27,940 --> 00:25:30,565
the last step is to deploy the model.

392
00:25:30,565 --> 00:25:33,550
So, once a model training is done,

393
00:25:33,550 --> 00:25:35,505
we should be able to go down here.

394
00:25:35,505 --> 00:25:37,730
So notice that this thing that training has started.

395
00:25:37,730 --> 00:25:39,045
So, it's going to take a while.

396
00:25:39,045 --> 00:25:40,955
So again, as before,

397
00:25:40,955 --> 00:25:42,665
I will do Ctrl+Z,

398
00:25:42,665 --> 00:25:44,280
put that into the background,

399
00:25:44,280 --> 00:25:45,880
it's going to this other directory,

400
00:25:45,880 --> 00:25:49,280
but I know that my actual output directory,

401
00:25:49,280 --> 00:25:52,360
the export already exists because I ran it before and

402
00:25:52,360 --> 00:25:55,160
I can make sure that in my export directory,

403
00:25:55,160 --> 00:25:57,980
I do have an exported model.

404
00:25:57,980 --> 00:26:00,320
So, let's wait, make sure there it is.

405
00:26:00,320 --> 00:26:03,760
There's an exported model in that directory and we

406
00:26:03,760 --> 00:26:07,555
should be able to go ahead and deploy the model.

407
00:26:07,555 --> 00:26:09,205
So, to deploy the model,

408
00:26:09,205 --> 00:26:12,000
we use gcloud ml-engine models,

409
00:26:12,000 --> 00:26:16,590
create a model called "flowers" with the version ResNet and

410
00:26:16,590 --> 00:26:22,890
then go ahead and create a version of the model,

411
00:26:22,890 --> 00:26:25,470
let's call it resnet and that version should exist.

412
00:26:25,470 --> 00:26:30,305
So again, let me just make sure that if I go into ML Engine,

413
00:26:30,305 --> 00:26:34,420
models, does "flowers" ResNet exist?

414
00:26:34,420 --> 00:26:36,975
There's not, flowers ResNet doesn't exist,

415
00:26:36,975 --> 00:26:39,025
so I should be able to create it.

416
00:26:39,025 --> 00:26:40,640
Since my model already exists,

417
00:26:40,640 --> 00:26:41,980
I'll skip that part.

418
00:26:41,980 --> 00:26:43,900
So, let's just do this,

419
00:26:43,900 --> 00:26:51,970
I'll go ahead and there.

420
00:26:51,970 --> 00:26:54,860
So, that's one model location and I don't

421
00:26:54,860 --> 00:26:58,549
need to create the model because a model already exists,

422
00:26:58,549 --> 00:27:02,375
I just need to create a ResNet version of the model.

423
00:27:02,375 --> 00:27:10,210
So, I can do that by specifying that and once you do this,

424
00:27:17,660 --> 00:27:23,330
then we should be able to invoke the model.

425
00:27:23,330 --> 00:27:27,660
So, we can invoke the model by doing this,

426
00:27:32,930 --> 00:27:39,960
change mod plus invoke model.py model.

427
00:27:39,960 --> 00:27:43,520
So, this is the first time the model is being invoked in a while,

428
00:27:43,520 --> 00:27:47,720
so it's going to take a little bit to warm up and then I should get the results.

429
00:27:47,720 --> 00:27:49,135
At the end of it,

430
00:27:49,135 --> 00:27:53,460
go ahead and you can remove everything because this all serverless,

431
00:27:53,460 --> 00:27:56,570
there's actually no VMs to delete or anything,

432
00:27:56,570 --> 00:27:59,540
but you can delete the directories from

433
00:27:59,540 --> 00:28:04,500
the storage or the model versions and you can delete the storage bucket as well.

434
00:28:04,500 --> 00:28:09,450
So, there it is,

435
00:28:09,450 --> 00:28:15,630
we just got back the response for the model and we said we sent in and obviously we

436
00:28:15,630 --> 00:28:21,810
sent in a sunflower and what we got back were predictions, five predictions.

437
00:28:21,810 --> 00:28:24,690
Those are the probabilities,

438
00:28:24,690 --> 00:28:27,260
the actual class number is three,

439
00:28:27,260 --> 00:28:29,725
that is 0, 1, 2,

440
00:28:29,725 --> 00:28:34,755
3 and we could look at cat/tmp/labels.txt,

441
00:28:34,755 --> 00:28:36,220
and 0, 1, 2,

442
00:28:36,220 --> 00:28:38,005
3, there are sunflowers.

443
00:28:38,005 --> 00:28:44,055
So, the model got sunflowers correct and so that's pretty much the process then.

444
00:28:44,055 --> 00:28:47,135
Take your data, take your image data,

445
00:28:47,135 --> 00:28:49,805
put it into that form, input that CSV,

446
00:28:49,805 --> 00:28:53,490
eval that CSV where you have the Cloud storage locations of

447
00:28:53,490 --> 00:28:58,725
all your images and the labels for each one of them and then run two scripts.

448
00:28:58,725 --> 00:29:02,670
One script to convert the images into TF records and

449
00:29:02,670 --> 00:29:08,155
a second script to run the model training and then if you want to deploy an ML Engine,

450
00:29:08,155 --> 00:29:12,680
run the deploy command to basically deploy it on ML engine and that's it,

451
00:29:12,680 --> 00:29:15,830
the model is now a microservice and it can be invoked,

452
00:29:15,830 --> 00:29:19,030
you can basically send in the image and

453
00:29:19,030 --> 00:29:22,905
you can get the response back and if you look at invoke_image.py,

454
00:29:22,905 --> 00:29:25,510
you will basically see that what it's doing,

455
00:29:25,510 --> 00:29:31,985
is that it's basically doing a base 64 encode of the entire content of the datafile,

456
00:29:31,985 --> 00:29:35,170
of the JPEG file and that's pretty much it.