As you saw in the previous lesson, our main goal here is to build a deep neural network, which has multiple hidden layers with a bunch of neurons, to ultimately enable us to classify those MNIST images in our dataset and see if we can't do it a little bit more accurately than with there linear dataset. To review as you saw within the lecture, this is the linear model it's just the features that you see here and it can't classify the swirl as you see. So what we're going to be doing is were going to be adding a few hidden layers, adding a few more neurons, and then ultimately getting the MNIST dataset images, hopefully with classifying with more accuracy with a more complex model type, the deep neural network as compared to a linear model. Let's take a look at how we can actually add these hidden layers as new lines of TensorFlow code in your model. So first and foremost what you want to do is clone in the GitHub repo that we have which is going to give you your started lab code. Once you do that you'll see something that looks like this. I'm in deep dive, images and then labs. Now there's two IPython notebooks I want to draw your attention to. The first one is MNIST models, when you actually open that one up it's going to look like this. So, we're going to replace the project name, the bucket, the region and the model type. If you realize, here it's actually parameterized. So, unlike the linear model code which we wrote before which is just a one time use for the linear model, from here on out starting with a deep neural network and I'm going to go into a different concept with deep neural networks after this and then eventually work our way up into the convolutional neural network or CNN, we're going to be using this same notebook the MNIST models in order to productionalize it and run it on the Cloud with Cloud Machine Learning Engine. So if you scroll down here you'll notice code that actually does the deploying and the prediction, but the actual model code itself is not in this notebook, and that's because it's in a separate notebook. So if you went into the MNIST model, into trainer, and then opened up model.py, this is what you will see. So here we have our standard import TensorFlow and we have the parameters just like you saw before with the linear model, our input is a 28 by 28 image pixel, our pixel image for the grayscale MNIST images that we're passing in as well as the output classes, this is the numbers zero through nine or ten total classes that we're actually trying to predict for. You have the linear model here available to you, this is what you wrote previously and that is one of the models that we could run as part of this parameterized model. Again going back to the MNIST models, you can just have this run as linear and then change these and then run it just fine and you can run that linear model at scale with cloud machine learning engine. But we want to go a little bit deeper and that's where the purposes of this lab, we need to complete the definition of our deep neural network model. So how do we actually add in these dense layers but do so inside of code? Well, it's not that bad. So the first thing that you want to do, first, is before passing in data into your deep neural network we need to transform it, flatten it from that 28 by 28 in the same way that we did for the linear models so we're going to bring that in there, and now at the first layer of our deep neural network, what we're actually going to do is setup a hidden layer called H1, and then we're going to invoke the TF layers API, and then you can create a dense layer for our deep neural network, and then you're going to say, hey what's the input to this layer? Well, we'll just call it the previous one which is going to be our input. That's the X that we just defined in the line beforehand, and you can specify the total number of neurons that's going to be your input. This technically input equals but you can just write just a number for short. Again for our architecture we're going to specify just save 300, and we'll give it an activation function. What's the activation function commonly used for neural networks? If you said Relu, that's exactly correct. Boom, just like that we already have one layer. Say we want to add in a few more to our architecture. Let's add in, I'll simply copy and paste. Say we want to add two more layers. Now what you'll have to change here is, this is going to be our second and our third hidden layers and you're going to have to change the inputs. So for the second hidden layer, its input is the first hidden layer and we'll reduce the number of neurons and same thing with our third hidden layer, it takes the input from the second hidden layer and it will reduce the neurons again here. Then the number of neurons and the number of layers overall is just called your neural network architecture. Now we're not done yet, what do we have up here the logits that we don't have down here and that's exactly what we're going to have to copy over. Now, for the logits which is just going to be our layer that returns the role values for our predictions, this is also a dense layer and we're going to pass it in an input but its X for the linear model because that's the only input that we have here. What's the last layer that we just wrote previously? It's actually, H3, so I'm going to actually update that, the third hidden layer. We're going to pass in ten classes, again, this is just the placeholder that we have N classes is equal to ten we define that up here. That is actually creating those raw values for our prediction and then it's just going to be a activation of nano which is default for linear for this last layer. Then I'm gonna return that. Perfect. You've just set up your deep neural network model code with three different layers. Essentially, you just did the code version of what you saw here except with many more neurons than just four, four and four. So, let's take a look at what else is in the notebook. You can see if you're scrolling ahead and looking ahead, by the way pass means just ignore. All right we're going to do something with dropout, which is a regularization technique that we're going to explore after this. Then later on, if you're looking ahead, you'll see that you're actually going to be creating or working with a convolutional neural network. You're going to see some of the unique layers that occur here but it also uses some of the same layers for deep neural network all the way at the end, and you're going to see that in your next few lessons and then the lab there, but that's looking way ahead first we want to see the performance of using just a deep neural network. So we're going to save that, and then once we have our deep neural network all the way at the bottom don't forget you have your train and evaluate function just like you were previously with the batch size, and your estimators, and everything that you've learned previously and that you worked with in linear lab is still in play here. Okay. Saving that notebook and then hopping over into how we're actually going to test this, evaluate it and then productionalize it, is back in the MNIST models notebook. So the first thing that we need to do is we need to change the placeholder project names to our own. So for us, I'm going to go to the home, and I'm just going to grab my project ID, paste that in, and specify a bucket if you haven't created one already, just opening up the navigation menu let's find where our storage is. I scrolled past it. Let's find the storage. Storage here. Now I already created a bucket. Generally for big data and analytics and machine learning use cases it'll be a regional bucket that you'll create. I created a placeholder one called, my cloud bucket 99, and you'll notice that the location is specific, it's a region, US Central1. So, let's go back into our notebook, specify that bucket for a generic name for my bucket and then the region name is the same as it was here. Really, really important, you could evaluate and test it against that linear model but since you've already done that, let's see if we can beat the linear model with a deep neural network. Go ahead and run this, environmental variables, this is going to set our project and our compute region, and the note in here just says, "complete what you just did," which is the to-dos to make sure that your model actually works, you've added those layers to your neural network architecture, and then here is where we are actually going to be getting ready to train. So we're going to be running it locally with the running rate you see here and running it for just 100 steps. So we're actually going to see what the accuracy is here. So, scrolling down. Here we're calling on the model function, we're getting checkpoints saved, and then we're actually going to start the training right here. So step number one we had a gigantic loss of 2.36 and that's fine. We've finalized the graph and here we see what that actually looks like inside of a tensor board once we actually get to deploy this and get it to work. Alright, so at step 100 the accuracy for the deep neural network is okay. We're about 91 at least from when I ran this, maybe yours is a little bit similar, a little bit higher, a little bit lower, 91 for the deep neural network. Okay. So how do we actually deploy this and use Google Cloud resources like a GPU? Eventually later on in this course, you're going to be using TPUs which are even faster for training machine learning models. So, the first thing that we need to do is basically specify. All right, well we want to create a Cloud Machine Learning Engine job and we have a bunch of parameters basically saying, MNIST and it's going to be a DNN model type and then the date of the job and the output directory we actually want to store it in our Google Cloud Storage bucket in the MNIST folder under trained and then again prepend it with a suffix for what type of model that we're actually training. Here's the actual code that calls the job. If you remember from your earlier course we looked at the end-to-end labs. This is very, very similar. We're submitting a training job, specifying the region that we're in, and then ultimately deploying the package path to the train directory. Right now we're just using basic GPU for this scale tier, you can check out some of the latest scale tiers, you just explore that option and see what the available enumerated values are. We're not just training for a 100 steps are actually going to train for 10,000 now. All right. So we have that ready to go, let's go ahead and submit that job to Cloud Machine Learning Engine. Once you have that submitted what you can do is immediately it says it's queued before. Previously I had trained this, so if you actually have model code in there already, if you're rerunning your model it'll actually remove that for you. Likely you might get a warning or an error that says, 'hey, I can't remove this' and that's because you didn't have a model code model in there already, and that's completely fine. So it's removing all of those files for me, and then it actually is queuing up the job. How can you check the progress of the job? Well, back in the navigation menu all the way at the bottom, under artificial intelligence you can click on ML engine. Then for our jobs once it's queued, it'll actually show up inside of your ML Engine jobs. Let's go back in the notebook and inspect. Our job, MNIST DNN and then we have the date has been submitted successfully. We can actually view the status of the job through CloudShell or we can wait for the UI to refresh. It's going to pull up, Cloud Shell, get that to and get ready. Let's see a few of the job and the console. Let's see if it's made it into the queue. All right. Training has begun. At least this particular job is just for training. It's running. It's about a minute and 23 seconds right now. On average, when I've run this before, it takes about eight or nine minutes for that deep neural network to finish training. That's 10,000 steps that we specified in training. Then, ultimately when you get ready to actually create the model on given the to predict with it, we can actually deploy that as a model that we can predict with. But, once that's going, we can actually monitor and take a look at the architecture through TensorBoard. So, this is pretty cool. So, we're going to launch this, and this will actually give us a link. It's looking at the output directory. Look at that, it's starting TensorBoard on the MNIST trained directory where you're passing in the bucket type as a parameter there, and then the model type again, it just matches the directory that we specified earlier. We can open this up, and then once that model gets to a place where it starts actually outputting results in the train there, you're going to see the graph of results. For previously ran model, same model code, this is what the graph of the model looks like. So, you can see, if we go down and zoom in just a little bit, that input data comes in and passes through the deep neural network layers again, that's the dense layers that we have there. You can see our friend Softmax there helping with the probabilities, and then ultimately it's very hard to read, but at the top you can see that the the result are through after the rest of the transformations that happened here, that their matrix multiplication, that we ultimately get an accuracy number, and that's where we get that point nine two from. So, this is essentially what we did with the simple web browser, UI with the TensorFlow Playground, now represented in a graph. Again, keep in mind that unless using TF eager, TensorFlow is going to build out this graph first before actually initializing doing any execution as well. So, that's the first step is build the graph and the second step is execute on the graph and start training. All right. Well, we'll tune back in after the model training is complete, and about eight minutes is generally what you can expect, and then we can evaluate some predictions against this once we deploy out the job as an official model on ML Engine, and see what a prediction looks like. After your model finishes training, the deep neural network here again, I'm in the jobs for ML Engine. We finally have one version of it, I just trained three just experimenting with different architectures. For each one of them, what you can do is to test the evaluate, the accuracy of it and you can click into "view logs" and what you can do is look at the logs for each of the different steps in training that you have. So, you had steps here, and one of the things that you can do just to search for that last step. I think there was 10,000 steps to going through training is just searched for their performance metric. We're searching for accuracy and let's see what we ended up with. Accuracy at step 10,000 when the model stop, we have 97 percent accurate for the deep neural network. Great. So, now lets us simple evaluation. Let's see if we can't do some predictions. So, we're happy with our Job, we are getting ready to actually deploy it to be an actual model. So, this is what we're going to be creating. We're going to give the model a name, specified the default version, optionally give it a description and a label. Then, once it's a model, we can then invoke the model through simple APIs and pass in new data that it hasn't seen before. In this particular case, just some images and see the predictions that we get back. So, that's the really fun part. So, heading back into our notebook. If you haven't already you can actually go ahead and stop TensorBoard from running. Now, it's actually time, how you actually create one of those models am back in ML Engine here. In order to get this, we need to create a new Cloud ML Engine Job, and this is going to be a deployment. So, we're going to give it a model name, we'll just call this MNIST2 since I already have one in there. Specify the model version. In this particular case, we're just saying the version is equal to the type which is that parameter that we had previously. Then, where the model actually is. Again, that's just the results of the previous step. If there's an existing one in there, we remove it. So, let's see. We're are going and create another Cloud Machine Learning Engine Job, we're actually going to deploy this model, and as soon as it's deployed, then we can predict from it. Now, we don't have to actually wait. I've already pretrained one of those models and deployed it out as you can see, I had a version of MNIST already here, both MNIST and MNIST2 is here already. So, what we can do is, we can actually pass in some images for prediction. So, we're getting the height and the width. Specifying that, it's reading it from a particular dataset, the MNIST dataset. It's grabbing, not a random but we're hard-coding into say, hey predict on image number five. Again, that's just the index of what it appears in that dataset. So, let's see what number five actually is. Some execute this block, and this is the image that's returned, that we could do a prediction on. Let's see if we actually call a prediction service. Again, you may both know that this is number one. Hopefully, the model gets that right, now 97 percent accurate neural network, let's see. It's going to output as a class, a predicted class, and then it's going to output other probabilities for each of those different classes. Here, you see it. It's predicted to that, it was a one by a landslide, you can see that this to the negative 24 is an extremely small number, that's zero. So, zero, one, two, three, reader just like an array. So, predicting for the number one pretty much got a 100 percent because all the numbers are super miniscule here. Now, if you actually went through the dataset, I'm going to pull up some more challenging one. I just happen to know that number 87, let's take a look at. Maybe this could get it, we'll see what looks like a three to me, but it also you've got a loop at the bottom which confused a little bit of that linear network the last time. So, it could be interpreted as a six or something strange. Let's see what happens if we pass that intuitively if recognizes that it's a three. That was immediate. Three with a very small number. So, zero, one, two and three. Yeah. No trouble. All right. Let's see if we can grab another. I think 95 is another one that's a little bit on the border. Let's see. This is a fat for here, but also maybe could be a strange six or seven. There's a lot of ink that's going on there. Let's see if we can't get in some trouble. Four now, we got that was a four. Again, with very high degree of certainty. All right. Last one and then honestly you can experiment with these to your heart's content. So, forward to us could also possibly be a nine, just based on that location. Let's see if there's any hesitation from the model. No got that it was a four. Again, with not 100 percent, but very, from point nine nine nine eight probability again. All right. So, that is how our neural network did again 97 percent accurate here. One of the last topics that we want to cover before moving on to Convolutional neural networks, which are great at processing, variations in images. If you're going to rotate this M, this dataset, if you're going to add things like noise or something like that, the neural networks couldn't perform as well. But one of the things that we can do to help address one of the limitations of training on a very large neural network was the likelihood of the neural network to overfit itself on the training dataset. So, one of the common layer is that you add onto the end of it, is a layer that basically looks at your neural network architecture, and then randomly selects neurons to drop them out of the architecture. It's called a dropout layer, and that's what we're going to look at next. But give this Lab another try, feel free to edit the architecture, experiment with the different predictions that you have here, and then move on to the next lesson in Dropout. We'll see you there.