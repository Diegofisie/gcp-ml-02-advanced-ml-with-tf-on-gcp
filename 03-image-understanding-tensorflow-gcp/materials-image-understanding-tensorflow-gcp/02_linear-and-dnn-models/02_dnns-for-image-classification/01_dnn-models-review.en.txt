How did our linear model do? What are the limitations of linear models? If you remember back to the first specialization of this course, you were solving linear classification models in the TensorFlow Playground like this one that you've seen here before. The two classes were easily separated by a decision boundary and a line could very easily divide the two. For more complex input features, like the data swirl that you see here, modeling that might not be so easy. What you've figured out by now is using the feature option available to us in this type of model, this dataset is not linearly solvable. The best model that I could train had loss of about 0.6. However, this qualifier of the feature options available to us is crucial, because in fact, there is a feature that can make learning this relationship trivial. You get a magical feature that can unswirl the data and create one through feature engineering but, in absence of that, what we can do in our attempts to engineer new features for linear models fail, what do we do? Well, we could use more complex or complicated models. There are many types of models that are able to learn non-linear decision boundaries. In this course, we'll be focusing on neural networks, and specifically later on, a special type called a convolutional neural network. Because the source of business problems are becoming increasingly popular, seemed bias towards these, well, that's where neural network shine. In this classification is exactly one of those problems. We've already seen how a linear model can't classify this complex dataset, let's see how a neural network does. Before we do though, we need to review some of the additional features I've enabled in the TensorFlow Playground. The first I've enabled is the activation function. This choice of the activation function is what separates linear models from neural networks. In the linear models we've constructed previously, the activation function was unbeknownst to you, set to be linear. The second additional feature I've enabled is the hidden layers feature. The hidden layers feature allows you to change the number of hidden layers and the number of neurons within each of those hidden layers. You can think of this as changing the number of transformations that the network performs in your input data. Each neuron in every hidden layer receives all the output from the layer that precedes it. It then transforms that input and passes the output to all the neurons in this subsequent layer. The shorthand way for describing the number of neurons and how they pass information to each other is called the neural networks architecture. Following the link that you'll see shortly, try and train a model that can classify that's swirl dataset. However, instead of introducing non-linear features, try to improve the performance only by changing the network's architecture, so number of layers and number of neurons. Now, your model should have finished training and it should look something like this. Now, the first thing to color is relationship between the first hidden layer and those that come after it. What should be apparent is that although the outputs from the neurons in the first hidden layer we're basically lines, subsequent hidden layers had far more complicated outputs. These subsequent layers built upon those that came before it, much in the same way that we did when we stacked up the outputs of the hidden layer to understand the output. Consequently, you can think of a neural network as a hierarchy of features. This will be a particularly important concept as you explore the convolutional neural networks in the next module. This idea of taking the inputs, transforming them in more complex ways many times before ultimately classifying them is very typical of neural networks. It represents a significant departure from the approach used classically in machine learning. Before neural networks, data scientists spent much of their time doing feature engineering. Now, the model itself is taking some of that responsibility and you can think of these layers as being a form of feature engineering onto themselves.