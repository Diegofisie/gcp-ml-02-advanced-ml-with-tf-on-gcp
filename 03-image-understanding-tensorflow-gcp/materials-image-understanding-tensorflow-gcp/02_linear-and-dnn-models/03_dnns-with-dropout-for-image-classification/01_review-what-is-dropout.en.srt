1
00:00:00,110 --> 00:00:03,600
You might be wondering why we simply don't continue increasing

2
00:00:03,600 --> 00:00:06,885
the size of our deep neural network until it last drops to zero.

3
00:00:06,885 --> 00:00:08,310
In fact, you don't even need to use

4
00:00:08,310 --> 00:00:11,225
a deep neural network if you want to infinite machine-learning power.

5
00:00:11,225 --> 00:00:14,460
The universal approximation theorem states that neural networks with

6
00:00:14,460 --> 00:00:17,925
only a single hidden layer can approximate any function.

7
00:00:17,925 --> 00:00:19,530
That means that theoretically, there's no

8
00:00:19,530 --> 00:00:22,070
a multitask a neural network like this can't solve.

9
00:00:22,070 --> 00:00:25,605
The caveat though is in order to become an infinitely powerful neural network,

10
00:00:25,605 --> 00:00:28,995
this hypothetical network has to be infinitely large.

11
00:00:28,995 --> 00:00:32,940
The proof essentially states that the use of non-linearities a neural network can

12
00:00:32,940 --> 00:00:37,010
memorize the value an arbitrarily small position in feature space.

13
00:00:37,010 --> 00:00:42,175
In the same way that pin art toys like the ones that you see here approximates a hand.

14
00:00:42,175 --> 00:00:44,420
There are plenty of reasons why we don't want to create

15
00:00:44,420 --> 00:00:48,065
an infinitely large neural network, deep or otherwise.

16
00:00:48,065 --> 00:00:50,240
Large means that it would take up more memory,

17
00:00:50,240 --> 00:00:53,250
meaning a smaller batch sizes which will slow down optimization.

18
00:00:53,250 --> 00:00:55,755
Large also means slower to decide.

19
00:00:55,755 --> 00:00:58,175
One of the worst things about really large neural networks

20
00:00:58,175 --> 00:00:59,965
and really complex models in general,

21
00:00:59,965 --> 00:01:04,160
they're extremely good at overfitting on your training data,

22
00:01:04,160 --> 00:01:05,750
and that's something that we don't want.

23
00:01:05,750 --> 00:01:11,050
Do you remember one of the best ways to combat overfitting in a deep neural network?

24
00:01:11,050 --> 00:01:13,690
Well, one of the best ways that we have in mitigating

25
00:01:13,690 --> 00:01:16,430
overfitting is through the use of regularization.

26
00:01:16,430 --> 00:01:18,650
Recall back to the first specialization that we learned

27
00:01:18,650 --> 00:01:21,220
about when we talked about dropout, L1,

28
00:01:21,220 --> 00:01:23,510
and L2 regularization, which changed

29
00:01:23,510 --> 00:01:28,119
our loss function by introducing a penalty to model complexity.

30
00:01:28,119 --> 00:01:29,755
So, here's a question for you,

31
00:01:29,755 --> 00:01:33,410
which form of regularization is only used with neural networks,

32
00:01:33,410 --> 00:01:36,505
dropout, L1, or L2?

33
00:01:36,505 --> 00:01:38,425
Have you said dropout?

34
00:01:38,425 --> 00:01:41,575
That's exactly right. Let's review what dropout is.

35
00:01:41,575 --> 00:01:44,300
Dropout's a technique or parts of a neural network,

36
00:01:44,300 --> 00:01:47,090
a randomly dropped during training with

37
00:01:47,090 --> 00:01:50,180
a probability that's determined by a hyperparameter.

38
00:01:50,180 --> 00:01:51,995
At every training step,

39
00:01:51,995 --> 00:01:57,245
each neuron has a probability p of temporarily being dropped out.

40
00:01:57,245 --> 00:01:59,510
Thus for each training step,

41
00:01:59,510 --> 00:02:02,875
a unique neural network architecture is generated.

42
00:02:02,875 --> 00:02:06,950
Accordingly, the final neural network can be described as an average,

43
00:02:06,950 --> 00:02:10,100
an ensemble of many different networks.

44
00:02:10,100 --> 00:02:13,370
L1 and L2 are implemented by using terms

45
00:02:13,370 --> 00:02:16,685
edited a loss function so that they can be used with most models.

46
00:02:16,685 --> 00:02:22,335
L1 a.k.a, lasso regularization is our regularizer for sparsity.

47
00:02:22,335 --> 00:02:24,620
L2 regularization is likely to introduce

48
00:02:24,620 --> 00:02:27,889
weight values that are normally distributed about zero,

49
00:02:27,889 --> 00:02:32,080
which means that most model waste becomes small but not zero.

50
00:02:32,080 --> 00:02:34,860
Implementing dropout is easy in TensorFlow.

51
00:02:34,860 --> 00:02:36,715
Dropouts and just another type of layer.

52
00:02:36,715 --> 00:02:39,105
Like the fully connected layers that we've seen so far,

53
00:02:39,105 --> 00:02:43,130
it accepts a layer as input and then is consumed by a layer.

54
00:02:43,130 --> 00:02:45,135
When constructing a dropout layer,

55
00:02:45,135 --> 00:02:49,430
the input is simply the layer on which you want to drop out for it to apply to,

56
00:02:49,430 --> 00:02:53,210
and then you can set the dropout layer to be the input to whatever comes next.

57
00:02:53,210 --> 00:02:55,880
Another rate parameter tells the layer at

58
00:02:55,880 --> 00:02:59,315
which probability to drop the neurons in that layer.

59
00:02:59,315 --> 00:03:02,000
Because dropout is only used during training,

60
00:03:02,000 --> 00:03:06,620
we pass in a parameter that encodes whether or not the model is in training mode.