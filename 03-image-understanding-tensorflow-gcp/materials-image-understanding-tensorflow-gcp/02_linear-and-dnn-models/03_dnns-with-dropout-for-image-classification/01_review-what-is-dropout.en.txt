You might be wondering why we simply don't continue increasing the size of our deep neural network until it last drops to zero. In fact, you don't even need to use a deep neural network if you want to infinite machine-learning power. The universal approximation theorem states that neural networks with only a single hidden layer can approximate any function. That means that theoretically, there's no a multitask a neural network like this can't solve. The caveat though is in order to become an infinitely powerful neural network, this hypothetical network has to be infinitely large. The proof essentially states that the use of non-linearities a neural network can memorize the value an arbitrarily small position in feature space. In the same way that pin art toys like the ones that you see here approximates a hand. There are plenty of reasons why we don't want to create an infinitely large neural network, deep or otherwise. Large means that it would take up more memory, meaning a smaller batch sizes which will slow down optimization. Large also means slower to decide. One of the worst things about really large neural networks and really complex models in general, they're extremely good at overfitting on your training data, and that's something that we don't want. Do you remember one of the best ways to combat overfitting in a deep neural network? Well, one of the best ways that we have in mitigating overfitting is through the use of regularization. Recall back to the first specialization that we learned about when we talked about dropout, L1, and L2 regularization, which changed our loss function by introducing a penalty to model complexity. So, here's a question for you, which form of regularization is only used with neural networks, dropout, L1, or L2? Have you said dropout? That's exactly right. Let's review what dropout is. Dropout's a technique or parts of a neural network, a randomly dropped during training with a probability that's determined by a hyperparameter. At every training step, each neuron has a probability p of temporarily being dropped out. Thus for each training step, a unique neural network architecture is generated. Accordingly, the final neural network can be described as an average, an ensemble of many different networks. L1 and L2 are implemented by using terms edited a loss function so that they can be used with most models. L1 a.k.a, lasso regularization is our regularizer for sparsity. L2 regularization is likely to introduce weight values that are normally distributed about zero, which means that most model waste becomes small but not zero. Implementing dropout is easy in TensorFlow. Dropouts and just another type of layer. Like the fully connected layers that we've seen so far, it accepts a layer as input and then is consumed by a layer. When constructing a dropout layer, the input is simply the layer on which you want to drop out for it to apply to, and then you can set the dropout layer to be the input to whatever comes next. Another rate parameter tells the layer at which probability to drop the neurons in that layer. Because dropout is only used during training, we pass in a parameter that encodes whether or not the model is in training mode.