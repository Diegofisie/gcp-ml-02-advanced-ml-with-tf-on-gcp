As we talked about before, it's always best to start with a simpler model and gradually introduce complexity, until your performance criteria are met. That's because simple models are less likely to over-fit and are often easier to interpret and maintain. So let's start with the simplest model we've covered so far in this course, the linear model and see how it copes with this new domain of images. Let's take a high-level talks with the TensorFlow code that we're going to use. We'll start with a code that's responsible for learning. Then show how this ties together with a familiar TensorFlow object, the estimator. Then we'll review how the data are passed into the model during training and evaluation. Finally, we'll put all the pieces together by calling our old friend, train and evaluate. Here's the code that's responsible for making predictions. Notice the three constants outside the function, that set the height, the width of each image as 28 and the number of classes as 10. Again you got numbers zero through nine. Notice also that we've added the comments at the end of every line, describing the shape and the tensor at that point of execution. It's a really good practice to get into because it makes debugging your code later that much easier. In this very first line of code, tf.reshaped transforms the shape of the image tensor from a batch of 2D tensors to a batch 1D tensors an operation that you can think of as flattening. Next, we create a variable to represent our weights. If this was a binary classification task, we only have one weight for our component vector in our input feature tensor. However, because this is a multi-class classification task, we need connections between every input of which there are height times width and every output of which there are in classes which is 10 for handwritten digit example. We will initialize the values in the w tensor to be zero. A question for you, "Why doesn't the shape of the w tensor include a dimension that's equal to the batch size? Well, there are two reasons. First, and fundamentally, this would imply that we want to learn different models, for different images in our batch. When the whole point is that we want one model to learn, that works well for the entire input domain. Secondly, we've let the batch size dimension of our input to be negative one. That indicates that we expect this to vary at runtime. This would mean that the shape of our variables would change at runtime as well, which will likely throw an error. After defining a variable for our weights, we create a variable to represent our bias terms, because each output node gets its own bias. The shape of the vector in the tensor is nclasses. Finally, we compute our logits for our linear model, by simply multiplying our extensor by our w matrix, add in our bias term to get our logits. Once we have our code for learning, we can construct an estimator spec. Estimator spec is the only required parameter for constructing an estimator, which as you may recall, is the object that we tell to train and evaluate. An estimator spec needs to know the mode of what it is that we're actually doing. Is it training, evaluating or predicting? It also needs to know the predictions for a given input, which in this case will be our probabilities. Then, it needs to know the current loss, in which operation controls model training. Note that the predictions will be collected from our linear model function and that there was passed through a softmax function, that you learned about earlier to ultimately get our probabilities. Note too that our loss function is computed using a different more numerically stable function, which computes the entropy of the softmax vectors. Once we have an estimator spec, we're ready to construct our model. We can do this as simply as passing it to the estimator constructor. Note that we'll refer to it as a model function. Now, let's take a look at how data is passed into our model while it's training. Here we make use of a convenient function, the TensorFlow library, that loads data into memory on our machine. For production level performance, you should consider using the data sets API. However, for our purposes, what you are going to see here works really well and that's what you can be practicing inside of your labs. Our input function expects Numpy arrays, for both input and output. Note that these are defined in the MNIST variable, which you've set using some TensorFlow library code, that retrieves the images and labels. Note that we set the number of epochs to none here. The reason we're doing that is because in a distributed context, which is what we're preparing for, the only supported method of stopping training is through the train spec, an object that we're going to look at next. In essence, this code delegates the decision to stop to the train spec. The definition of the eval input function is identical except for three things. It uses different data, it doesn't shuffle, and the number of epochs is set to one. We don't need to shuffle, because we don't want to go through the entire evaluation set every time. We don't need more than one epoch, because during evaluation, we don't update our model, so one exposure per data point is sufficient. Once we defined our training and eval input functions, we can use them to build or train in eval specs. Train and eval specs are basically configuration for training and evaluation. They both require an input function. Now for train specs, it's also highly recommended that you define max steps. Max steps tells the model when to stop training. It is the only supported way of doing so during distributed training. If you set max steps to none and the input function as number of epoch set to none, the model's going to train forever, which is pretty bad. In addition to the input function, the eval spec also accepts a parameter that governs this steps. Although in this case, it's safe to align the input function itself to stop. That's why we set the steps parameter here to none. The eval spec is also responsible for exporting the model of the disk and so it requires an exporter. Now that the exporter references as serving input function, when it comes time to serve the model, it will need to know how to map what the client sends towards what the model expects. Lastly, throttle and seconds governs how frequently evaluation is done. Evaluation frequency is a trade-off between our desire to understand how well the model's performing and wanting to have a model trained as quickly as possible. The default value is for 600, which is five minutes in between evaluations. That might be fine for models that take a really long time to train, but in our case our model is maturing rather quickly, so I'm going set it to a much smaller number. Finally, we get to call the train and evaluate function, which expects our estimator as well as our train and evals pecs to configure training and evaluation. Train evaluate will initiate training, periodically evaluate the model as per the frequency that we've set in our eval spec.