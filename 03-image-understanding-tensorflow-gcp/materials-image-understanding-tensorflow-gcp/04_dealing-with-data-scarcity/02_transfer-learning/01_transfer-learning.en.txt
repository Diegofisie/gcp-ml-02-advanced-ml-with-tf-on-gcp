One of the other techniques for dealing with data scarcity when you have some labeled data is to use transfer learning. As mentioned before, transfer learning attacks the problem from a different angle. Rather than creating more data, transfer learning decreases our need for data. It does so by initializing our parameters with better values. You can think of optimisation as a journey through weight space, where we pay data in time to get from our random initialization point in weight space to some optimum. As we've said previously, unstructured data domains often require complex models and complex models require lots of data. So, often it's the case that the amount of data that we have is less than the amount of data that we need. So, using the normal optimization procedure, we'd be unable to reach an optimum. Transfer learning changes the problem of optimization. Instead of travelling from a random point in weight space, which is likely to be far from an optimum, we instead start travelling from a point much nearer to an optimum. The way we do this is by starting in an optimum for a related task. Here, the blue line represents the cost paid to train a related model to an optimum. The red line represents the cost we would pay if we wanted to train a model to perform well on our task. The orange line represents the cost to train a model that has already been trained on the related task to do well on our task. The power of transfer learning comes from the relative sizes of these lines. When the orange line is smaller than the red line, then transfer learning is much less expensive than training a model from scratch. The size of the orange line is determined by the similarity of the tasks. So, it's very important to choose as similar a task as possible. ImageNet classification is a popular choice for the related task, when trying to train a computer vision model because of its size and variety. ImageNet has 14 million input-output pairs and 20,000 categories, spanning many objects in the natural world. Because of the variety of its classes, ImageNet remains a common benchmark for computer vision models. So, how do we take a model trained on a related task and use it for our task? A naive form of transfer learning would simply take this model and then use it to predict on our task. But what if the classes in our task don't align with those used in ImageNet, which recall is a general-purpose data set? Not only would you have shape errors if your labels aren't the same as the shapes used to train the model, which is likely to be the case unless your task also has 20,000 categories, but in the event that there isn't a one-to-one one between each of your classes and those on ImageNet, the source models prior training might actually undermine performance. So instead, we take a subtler approach to transfering the source model to our task. Instead of using the model as is, we remove the parts that are closely aligned to the source task and replace them with newly initialized parts that are task appropriate. Which parts of the source image model are most closely aligned to the source task? If you say the layers near the output, you are precisely right. You can think of the parts in the model as existing on a spectrum of task-independence. At one extreme, is the final layer, which has a number of nodes equal to whatever was appropriate to the source task. At the other extreme is the input layer. Assuming you resize the image inputs, your inputs could theoretically handle any RGB image, which means they're completely a task-independent. The convolutional layers following your input are also nearly task-independent and as we progress through the network, they become increasingly task-dependent. Recall that, CNNs learn a hierarchy of features beginning with the most general and ending with the most specific. These early convolutional layers are maximally activated by simple patterns, that the network might observe over a span of a few pixels in the original image. As we progress from input to output in the network, these layers become semantically more specific and consequently more task dependent. I wish it were possible to say where the best place to cut the network is, but neural networks are not modular, like say the parts of a car. There is no layer that could become from the network in order to say, reduce its performance on a certain class and that's because neural networks learn distributed representations, where each neuron is responsible for many things. Because neural networks learn distributed representations, it's also hard to say which parts of the network have learned general functions and which have not. However, by convention, we cut the source network after the convolutional layers and append a number of fully connected layers of our own. This is consistent with the view that convolutional layers are excellent feature extractors for the image domain. At this point, you have an important decision to make. Do I make the source models weights trainable, as in allowing to change values during subsequent model training or do I make them constant? Leaving them constant, effectively treats the source model as a feature extractor. If you're new data set is small, this is the recommended approach, at the risk of over fitting your data. The larger your data set is, the more confident you can be that letting the source network continue to train will not result in overfitting