Let's illustrate this principle of "try the easiest solution first, add complexity as necessary" with an example. This part of the module uses an image classification problem in part because it makes for a cool demo, but this principle also works with other problem domains. Let's start with an easy version of a problem which can be solved out of the box with an ML API. Then as it gets increasingly harder, you'll need to use Auto ML and then eventually an ML framework. Seeing that Google Cloud is a cloud computing provider, let's choose detecting clouds as an image as our problem. Here's the problem statement; given an image, we simply want to identify if there's a cloud in it. As an ML practitioner, your first instinct should be to see how well a pre-trained model does in this problem, because if that works, then you've solved it was minimal additional engineering effort. You can experiment with each of the ML APIs in your browser and see what they return. When you are ready to build a production model, you simply will pass a Json object request to the API and parse wooded returns. Let's take a look at the pieces of the Vision API which will be the initial focus for image classification. There are three major components that all roll up into Cloud Vision ML API. Behind the scenes, each of these are powered by many ML models and years of research. The first is detecting what the image is and classifying it. For example, if you show the model a cat, it will classify it correctly as a cat. Next, what about images with text or scanned documents? Cloud vision will extract the text into a selectable searchable format. Lastly, it is a bit of intuition from the web. Does the image contain entities we know like the Eiffel Tower or a famous person? Identify them for us. Let's test limits of each of these in a web demo and then in the lab later. We'll access them via Cloud Shell in your lab.